{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset_functions import from_path_to_dataloader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#plot   \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataloaders\n",
    "path_train = './chaoyang-data/train'\n",
    "path_test = './chaoyang-data/test'\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = from_path_to_dataloader(path_train, batch_size, True, True)\n",
    "test_dataloader = from_path_to_dataloader(path_test, batch_size, False, False)\n",
    "\n",
    "#split the train dataset into train and validation\n",
    "train_size = int(0.9 * len(train_dataloader.dataset))\n",
    "val_size = len(train_dataloader.dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataloader.dataset, [train_size, val_size])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#use the GPU if available\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#use huggingface's pretrained model for image classification vit-base-patch16-224\n",
    "#model = transformers.ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=1000)\n",
    "# use huggingface's pretrained model for image classification swin-base-patch4-window7-224\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, AutoConfig\n",
    "path_swin=\"microsoft/swinv2-base-patch4-window16-256\"\n",
    "path_vit=\"google/vit-base-patch16-384\"\n",
    "config = AutoConfig.from_pretrained(path_vit )\n",
    "config.num_labels = 4\n",
    "model = AutoModelForImageClassification.from_pretrained(path_vit)\n",
    "#initialize the weights of models using kaiming normal\n",
    "#model.init_weights()\n",
    "#change the number of output classes\n",
    "model.classifier = nn.Linear(768, 4)\n",
    "model.config = config\n",
    "#model.classifier = nn.Linear(1024, 4)\n",
    "model.to(device)\n",
    "#use the AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "#use the cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#import classification_report from sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "def evaluate(model, val_dataloader, criterion):\n",
    "    #initialize the loss and the number of correct predictions\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    predictions=[]\n",
    "    labels=[]\n",
    "    #for each batch\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(val_dataloader):\n",
    "            #send the data and the target to the GPU\n",
    "            images, target = data.to(device), target.to(device)\n",
    "            images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "            s = images['pixel_values'].shape\n",
    "            images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "            #forward pass\n",
    "            output = model(**images)\n",
    "            #compute the loss\n",
    "            val_loss += criterion(output.logits, target).item()\n",
    "            #compute the number of correct predictions\n",
    "            pred = output.logits.argmax(dim=1, keepdim=True)\n",
    "            #append the predictions\n",
    "            predictions.extend(pred.cpu().numpy().tolist())\n",
    "            #append the labels\n",
    "            labels.extend(target.cpu().numpy().tolist())\n",
    "\n",
    "    #compute the average loss\n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "    #return the average loss and the number of correct predictions\n",
    "    #print the classification report\n",
    "    print(classification_report(labels, predictions))\n",
    "    return val_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train the model\n",
    "output_path = './models/vit_384_finetuning/'\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, criterion, epochs):\n",
    "    #set the model in training mode\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    #for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        train_losses_inner = []\n",
    "        loss=0\n",
    "        #for each batch\n",
    "        for (data, target) in tqdm(train_dataloader):\n",
    "            #send the data and the target to the GPU\n",
    "            images, target = data.to(device), target.to(device)\n",
    "            images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "            s = images['pixel_values'].shape\n",
    "            images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "            #forward pass\n",
    "            output = model(**images)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output.logits, target)\n",
    "            train_losses_inner.append(loss.cpu().detach().numpy())\n",
    "            loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(np.mean(train_losses_inner))\n",
    "        #evaluate the model on the validation set\n",
    "        print('Epoch: ', epoch)\n",
    "        print('Validation set:')\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        print('Test set:')\n",
    "        test_loss, test_accuracy = evaluate(model, test_dataloader, criterion)\n",
    "        #if the path doesn't exist, create it\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        #save the model\n",
    "        #torch.save(model.state_dict(), output_path + 'model_' + str(epoch) + '.pth')\n",
    "        model.save_pretrained(output_path + 'model_' + str(epoch) + '.pth')\n",
    "    #plot the train and validation losses after transferring to cpu\n",
    "    \n",
    "\n",
    "    print(train_losses)\n",
    "    plt.plot(train_losses, label='train loss')\n",
    "    plt.plot(val_losses, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 29/316 [00:35<05:16,  1.10s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "#train the model\n",
    "#train(model, val_dataloader, val_dataloader, optimizer, criterion, 20)\n",
    "\n",
    "train(model, train_dataloader, val_dataloader, optimizer, criterion, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
