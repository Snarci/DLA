{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# from einops import repeat\n",
    "# from einops.layers.torch import Rearrange\n",
    "# from vit_pytorch.vit import Transformer\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import transformers\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# from vit_pytorch.distill import DistillableViT, DistillWrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets_utils\n",
    "train_loader, val_loader, test_loader, classes , img_size = datasets_utils.get_Chaoyang_loaders(BATCH_SIZE=64, SEED=42)\n",
    "# train_loader, val_loader, test_loader, classes , img_size= datasets_utils.get_CUB_loaders(BATCH_SIZE=256, SEED=42, SPLITS=[0.50,0.25,0.25])\n",
    "# train_loader, val_loader, test_loader, classes , img_size = datasets_utils.get_vegetables_dataloader(BATCH_SIZE=32, SEED=42, SPLITS=[0.50,0.25,0.25])\n",
    "N_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resenetModel = resnet50(pretrained = True)\n",
    "resenetModel.fc = nn.Linear(resenetModel.fc.in_features, N_CLASSES)\n",
    "resenetModel = resenetModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "def train_resnet50(model,train_loader,val_loader,epochs=10,lr=1e-2):\n",
    "    # model.to(device)\n",
    "    current_valid_loss , best_valid_loss = float('inf'), float('inf')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_ = []\n",
    "        val_loss_ = []\n",
    "        for i,(images,classes) in enumerate(train_loader):\n",
    "            images = datasets_utils.convert_images_dict_to_tensor(images, resize=224)\n",
    "            \n",
    "            images = images.to(device)\n",
    "            classes = classes.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs  = model(images)\n",
    "            loss = criterion(outputs, classes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_.append(loss.item())\n",
    "            # #every 100 batches, print the loss\n",
    "        train_loss.append(np.mean(train_loss_))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i,(images,classes) in enumerate(val_loader):\n",
    "                images = datasets_utils.convert_images_dict_to_tensor(images, resize=224)\n",
    "                    \n",
    "                images = images.to(device)\n",
    "                classes = classes.to(device)\n",
    "                # optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, classes)\n",
    "                current_valid_loss = loss.item()\n",
    "                val_loss_.append(loss.item())\n",
    "               \n",
    "            mean_loss = np.mean(val_loss_)\n",
    "            current_valid_loss = mean_loss\n",
    "            if current_valid_loss < best_valid_loss:\n",
    "                best_valid_loss = current_valid_loss\n",
    "                print(f\"\\nBest validation loss: {best_valid_loss}\")\n",
    "                print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "                torch.save(optimizer.state_dict(), 'jacoExperiments/BEST_resnet50_finetuned_sulle_cellule.pth')\n",
    "            val_loss.append(mean_loss)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}')\n",
    "        torch.save(model.state_dict(),f'jacoExperiments/resnet50_finetuned_sulle_cellule.pth')\n",
    "        \n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.627157555686103\n",
      "\n",
      "Saving best model for epoch: 1\n",
      "\n",
      "Epoch: 1, Train Loss: 0.7004589578018913, Val Loss: 0.627157555686103\n",
      "\n",
      "Best validation loss: 0.508217023478614\n",
      "\n",
      "Saving best model for epoch: 2\n",
      "\n",
      "Epoch: 2, Train Loss: 0.48901021291938007, Val Loss: 0.508217023478614\n",
      "\n",
      "Best validation loss: 0.4846656123797099\n",
      "\n",
      "Saving best model for epoch: 3\n",
      "\n",
      "Epoch: 3, Train Loss: 0.394599812883365, Val Loss: 0.4846656123797099\n",
      "\n",
      "Best validation loss: 0.44166891773541767\n",
      "\n",
      "Saving best model for epoch: 4\n",
      "\n",
      "Epoch: 4, Train Loss: 0.3344371679085719, Val Loss: 0.44166891773541767\n",
      "Epoch: 5, Train Loss: 0.3047790693331368, Val Loss: 0.4444691406355964\n",
      "\n",
      "Best validation loss: 0.39635882609420353\n",
      "\n",
      "Saving best model for epoch: 6\n",
      "\n",
      "Epoch: 6, Train Loss: 0.2621708895209469, Val Loss: 0.39635882609420353\n",
      "Epoch: 7, Train Loss: 0.22997126511380642, Val Loss: 0.47706950704256695\n",
      "Epoch: 8, Train Loss: 0.2079773612226112, Val Loss: 0.4128217366006639\n",
      "\n",
      "Best validation loss: 0.3611361053254869\n",
      "\n",
      "Saving best model for epoch: 9\n",
      "\n",
      "Epoch: 9, Train Loss: 0.1749471982445898, Val Loss: 0.3611361053254869\n",
      "Epoch: 10, Train Loss: 0.17190560325980186, Val Loss: 0.4717073043187459\n",
      "Epoch: 11, Train Loss: 0.1844478353763683, Val Loss: 0.45510410600238377\n",
      "Epoch: 12, Train Loss: 0.15956978990307338, Val Loss: 0.3849569145176146\n",
      "Epoch: 13, Train Loss: 0.1355119509787499, Val Loss: 0.3627886341677772\n",
      "Epoch: 14, Train Loss: 0.12475711208638511, Val Loss: 0.43058590839306515\n",
      "Epoch: 15, Train Loss: 0.11949709767499302, Val Loss: 0.3823227220111423\n",
      "\n",
      "Best validation loss: 0.35234683255354565\n",
      "\n",
      "Saving best model for epoch: 16\n",
      "\n",
      "Epoch: 16, Train Loss: 0.10562504240889338, Val Loss: 0.35234683255354565\n",
      "Epoch: 17, Train Loss: 0.1069732661583001, Val Loss: 0.4075448529587852\n",
      "Epoch: 18, Train Loss: 0.09016894582140295, Val Loss: 0.43006879339615506\n",
      "\n",
      "Best validation loss: 0.33579039242532516\n",
      "\n",
      "Saving best model for epoch: 19\n",
      "\n",
      "Epoch: 19, Train Loss: 0.09874624169514148, Val Loss: 0.33579039242532516\n",
      "Epoch: 20, Train Loss: 0.10654706654103496, Val Loss: 0.37529439479112625\n",
      "Epoch: 21, Train Loss: 0.09529891343716579, Val Loss: 0.35030892656909096\n",
      "Epoch: 22, Train Loss: 0.08676397875894475, Val Loss: 0.405286173025767\n",
      "Epoch: 23, Train Loss: 0.08590818714067529, Val Loss: 0.39671007295449573\n",
      "Epoch: 24, Train Loss: 0.08106139363555968, Val Loss: 0.4794584943188561\n",
      "Epoch: 25, Train Loss: 0.07700962401172028, Val Loss: 0.4124660657511817\n",
      "Epoch: 26, Train Loss: 0.07056532914549866, Val Loss: 0.4315577016936408\n",
      "Epoch: 27, Train Loss: 0.0695177959160337, Val Loss: 0.3727240314086278\n",
      "Epoch: 28, Train Loss: 0.07181586861563256, Val Loss: 0.42130202386114335\n",
      "Epoch: 29, Train Loss: 0.0716641833180491, Val Loss: 0.4270557247930103\n",
      "Epoch: 30, Train Loss: 0.0662301083838072, Val Loss: 0.427567258477211\n",
      "Epoch: 31, Train Loss: 0.08026163333649688, Val Loss: 0.38026484847068787\n",
      "Epoch: 32, Train Loss: 0.07435913184728427, Val Loss: 0.42748381528589463\n",
      "Epoch: 33, Train Loss: 0.06195147026660321, Val Loss: 0.46477964106533265\n",
      "Epoch: 34, Train Loss: 0.06451632892359284, Val Loss: 0.41166161000728607\n",
      "Epoch: 35, Train Loss: 0.045130600309871795, Val Loss: 0.3917982396152284\n",
      "Epoch: 36, Train Loss: 0.07201596477976706, Val Loss: 0.43900853726598954\n",
      "Epoch: 37, Train Loss: 0.07634586606269018, Val Loss: 0.48180374171998763\n",
      "Epoch: 38, Train Loss: 0.06941659319438512, Val Loss: 0.434136559565862\n",
      "Epoch: 39, Train Loss: 0.04861870392732605, Val Loss: 0.4346398413181305\n",
      "Epoch: 40, Train Loss: 0.04258256249389128, Val Loss: 0.39774901005956864\n",
      "Epoch: 41, Train Loss: 0.05410228094050684, Val Loss: 0.5254730300770866\n",
      "Epoch: 42, Train Loss: 0.06209247279322789, Val Loss: 0.41827012764083016\n",
      "Epoch: 43, Train Loss: 0.05986998282090018, Val Loss: 0.4479205111662547\n",
      "Epoch: 44, Train Loss: 0.05243217860218845, Val Loss: 0.4727298998170429\n",
      "Epoch: 45, Train Loss: 0.04964602838824444, Val Loss: 0.4490549564361572\n",
      "Epoch: 46, Train Loss: 0.053229956616517866, Val Loss: 0.44139954447746277\n",
      "Epoch: 47, Train Loss: 0.04143534382050739, Val Loss: 0.44040626618597245\n",
      "Epoch: 48, Train Loss: 0.044716609494548434, Val Loss: 0.46352607674068874\n",
      "Epoch: 49, Train Loss: 0.0531062939988237, Val Loss: 0.4662457009156545\n",
      "Epoch: 50, Train Loss: 0.04698555193004446, Val Loss: 0.5035155903961923\n",
      "Epoch: 51, Train Loss: 0.05925097759765915, Val Loss: 0.47187838289472794\n",
      "Epoch: 52, Train Loss: 0.0715027039964931, Val Loss: 0.5492343447274632\n",
      "Epoch: 53, Train Loss: 0.049914375758034334, Val Loss: 0.4370300869146983\n",
      "Epoch: 54, Train Loss: 0.03714318052995243, Val Loss: 0.5286299917432997\n",
      "Epoch: 55, Train Loss: 0.051968141218729884, Val Loss: 0.3865152531199985\n",
      "Epoch: 56, Train Loss: 0.056809742221773804, Val Loss: 0.4536746202243699\n",
      "Epoch: 57, Train Loss: 0.048510662925465006, Val Loss: 0.4358822852373123\n",
      "Epoch: 58, Train Loss: 0.04593422533089554, Val Loss: 0.43913408120473224\n",
      "Epoch: 59, Train Loss: 0.02527029329286063, Val Loss: 0.3822164340979523\n",
      "Epoch: 60, Train Loss: 0.04959305128837122, Val Loss: 0.48318494690789116\n",
      "Epoch: 61, Train Loss: 0.06152952580323702, Val Loss: 0.42743996116850114\n",
      "Epoch: 62, Train Loss: 0.02915463257651729, Val Loss: 0.4792399654785792\n",
      "Epoch: 63, Train Loss: 0.046665531730491525, Val Loss: 0.5514959990978241\n",
      "Epoch: 64, Train Loss: 0.034127124815094695, Val Loss: 0.3997163987822003\n",
      "Epoch: 65, Train Loss: 0.04196403243820643, Val Loss: 0.4291467567284902\n",
      "Epoch: 66, Train Loss: 0.04648908459333868, Val Loss: 0.4545271115170585\n",
      "Epoch: 67, Train Loss: 0.037747612697532094, Val Loss: 0.5004178881645203\n",
      "Epoch: 68, Train Loss: 0.03244228732530499, Val Loss: 0.41544371512201095\n",
      "Epoch: 69, Train Loss: 0.048806359973747895, Val Loss: 0.45817941923936206\n",
      "Epoch: 70, Train Loss: 0.03214970340840283, Val Loss: 0.5050146149264442\n",
      "Epoch: 71, Train Loss: 0.050558246429007546, Val Loss: 0.4625672863589393\n",
      "Epoch: 72, Train Loss: 0.04769151392274925, Val Loss: 0.4699190739128325\n",
      "Epoch: 73, Train Loss: 0.04188943431046614, Val Loss: 0.43764039211803013\n",
      "Epoch: 74, Train Loss: 0.03808055873783423, Val Loss: 0.3496496379375458\n",
      "Epoch: 75, Train Loss: 0.037690914451156424, Val Loss: 0.4673430456055535\n",
      "Epoch: 76, Train Loss: 0.0286208954787094, Val Loss: 0.4333671298291948\n",
      "Epoch: 77, Train Loss: 0.030546796873589105, Val Loss: 0.4618050025569068\n",
      "Epoch: 78, Train Loss: 0.030477609678430932, Val Loss: 0.4921719233194987\n",
      "Epoch: 79, Train Loss: 0.04211004002349852, Val Loss: 0.4820198582278358\n",
      "Epoch: 80, Train Loss: 0.029582426701289237, Val Loss: 0.4879203844401572\n",
      "Epoch: 81, Train Loss: 0.025852962441247285, Val Loss: 0.4575792733165953\n",
      "Epoch: 82, Train Loss: 0.03219573144635916, Val Loss: 0.4073360198073917\n",
      "Epoch: 83, Train Loss: 0.04775879737520237, Val Loss: 0.5002483046717114\n",
      "Epoch: 84, Train Loss: 0.04553195855232498, Val Loss: 0.3860079300486379\n",
      "Epoch: 85, Train Loss: 0.05313051957931794, Val Loss: 0.44862888173924553\n",
      "Epoch: 86, Train Loss: 0.03614409678450585, Val Loss: 0.38427692982885575\n",
      "Epoch: 87, Train Loss: 0.02450278366892304, Val Loss: 0.42857249991761315\n",
      "Epoch: 88, Train Loss: 0.025505543898649608, Val Loss: 0.39221638441085815\n",
      "Epoch: 89, Train Loss: 0.04545103481313496, Val Loss: 0.4722992347346412\n",
      "Epoch: 90, Train Loss: 0.05382443737250434, Val Loss: 0.4453938951094945\n",
      "Epoch: 91, Train Loss: 0.032794834966358696, Val Loss: 0.447296819753117\n",
      "Epoch: 92, Train Loss: 0.03910935609129882, Val Loss: 0.4315771874454286\n",
      "Epoch: 93, Train Loss: 0.036617315250531406, Val Loss: 0.48424367275502944\n",
      "Epoch: 94, Train Loss: 0.03344265363750908, Val Loss: 0.3480752437478966\n",
      "Epoch: 95, Train Loss: 0.03844396546158845, Val Loss: 0.46366705497105914\n",
      "Epoch: 96, Train Loss: 0.029382781953774865, Val Loss: 0.3935164039333661\n",
      "Epoch: 97, Train Loss: 0.028002545127020325, Val Loss: 0.4532911649180783\n",
      "Epoch: 98, Train Loss: 0.021915617841001177, Val Loss: 0.49349584844377303\n",
      "Epoch: 99, Train Loss: 0.020239458986723066, Val Loss: 0.3691301668683688\n",
      "Epoch: 100, Train Loss: 0.034174532575762674, Val Loss: 0.4232343120707406\n",
      "Epoch: 101, Train Loss: 0.031198895635095106, Val Loss: 0.4531429343753391\n",
      "Epoch: 102, Train Loss: 0.034069261201981546, Val Loss: 0.4914610866043303\n",
      "Epoch: 103, Train Loss: 0.04210666293584848, Val Loss: 0.6173748241530524\n",
      "Epoch: 104, Train Loss: 0.0338705738187072, Val Loss: 0.401016251080566\n",
      "Epoch: 105, Train Loss: 0.035540283190475516, Val Loss: 0.4240100102292167\n",
      "Epoch: 106, Train Loss: 0.02561876483562202, Val Loss: 0.43768121467696297\n",
      "Epoch: 107, Train Loss: 0.021288604259673695, Val Loss: 0.45670222325457466\n",
      "Epoch: 108, Train Loss: 0.03508044064746391, Val Loss: 0.5161883731683096\n",
      "Epoch: 109, Train Loss: 0.035722813303759204, Val Loss: 0.4946247579322921\n",
      "Epoch: 110, Train Loss: 0.029692167196646805, Val Loss: 0.5171171194977231\n",
      "Epoch: 111, Train Loss: 0.03567509795833804, Val Loss: 0.5183934271335602\n",
      "Epoch: 112, Train Loss: 0.026298461046476578, Val Loss: 0.44193629092640346\n",
      "Epoch: 113, Train Loss: 0.028287128809120366, Val Loss: 0.4050275592340363\n",
      "Epoch: 114, Train Loss: 0.030329615117899507, Val Loss: 0.45612528423468274\n",
      "Epoch: 115, Train Loss: 0.035602363530852936, Val Loss: 0.4802514596117867\n",
      "Epoch: 116, Train Loss: 0.029302349317040812, Val Loss: 0.4384726385275523\n",
      "Epoch: 117, Train Loss: 0.03166146519081078, Val Loss: 0.4377770225207011\n",
      "Epoch: 118, Train Loss: 0.021343305995805733, Val Loss: 0.36714672380023533\n",
      "Epoch: 119, Train Loss: 0.0335641435592117, Val Loss: 0.4103126409980986\n",
      "\n",
      "Best validation loss: 0.31232218113210464\n",
      "\n",
      "Saving best model for epoch: 120\n",
      "\n",
      "Epoch: 120, Train Loss: 0.028493213156367877, Val Loss: 0.31232218113210464\n",
      "Epoch: 121, Train Loss: 0.03725731125708575, Val Loss: 0.4139467378457387\n",
      "Epoch: 122, Train Loss: 0.02562324823944275, Val Loss: 0.5444993144936032\n",
      "Epoch: 123, Train Loss: 0.02880684197423508, Val Loss: 0.39536243346002364\n",
      "Epoch: 124, Train Loss: 0.030981055024681212, Val Loss: 0.3889734297990799\n",
      "Epoch: 125, Train Loss: 0.024035305125179075, Val Loss: 0.4447120146618949\n",
      "Epoch: 126, Train Loss: 0.018751396757809895, Val Loss: 0.49744556844234467\n",
      "Epoch: 127, Train Loss: 0.024690102552995086, Val Loss: 0.44539110859235126\n",
      "Epoch: 128, Train Loss: 0.03319898330572627, Val Loss: 0.4412681758403778\n",
      "Epoch: 129, Train Loss: 0.02953537447743495, Val Loss: 0.4659652163585027\n",
      "Epoch: 130, Train Loss: 0.027581263934792596, Val Loss: 0.49709338777595097\n",
      "Epoch: 131, Train Loss: 0.02848001671778156, Val Loss: 0.4539518670903312\n",
      "Epoch: 132, Train Loss: 0.035067393895403684, Val Loss: 0.45930925011634827\n",
      "Epoch: 133, Train Loss: 0.022829032422019804, Val Loss: 0.43774590227339005\n",
      "Epoch: 134, Train Loss: 0.027910845851543346, Val Loss: 0.4581580509742101\n",
      "Epoch: 135, Train Loss: 0.035254409088741376, Val Loss: 0.39559856057167053\n",
      "Epoch: 136, Train Loss: 0.03486814526133711, Val Loss: 0.499169248673651\n",
      "Epoch: 137, Train Loss: 0.026635359677993044, Val Loss: 0.5546032852596707\n",
      "Epoch: 138, Train Loss: 0.024692520902262222, Val Loss: 0.3637158171170288\n",
      "Epoch: 139, Train Loss: 0.020519133980306383, Val Loss: 0.4775155335664749\n",
      "Epoch: 140, Train Loss: 0.019539051883724293, Val Loss: 0.43993739949332344\n",
      "Epoch: 141, Train Loss: 0.017618471047401878, Val Loss: 0.4209190888537301\n",
      "Epoch: 142, Train Loss: 0.020687901941378588, Val Loss: 0.4380999671088325\n",
      "Epoch: 143, Train Loss: 0.016733039477014842, Val Loss: 0.4484650161531236\n",
      "Epoch: 144, Train Loss: 0.016586962329515526, Val Loss: 0.4940034829907947\n",
      "Epoch: 145, Train Loss: 0.02261249464899714, Val Loss: 0.5797104752726026\n",
      "Epoch: 146, Train Loss: 0.038498965053689445, Val Loss: 0.557641370428933\n",
      "Epoch: 147, Train Loss: 0.03229221576070295, Val Loss: 0.3866141099068854\n",
      "Epoch: 148, Train Loss: 0.024582440155127994, Val Loss: 0.5870838463306427\n",
      "Epoch: 149, Train Loss: 0.026303123754944324, Val Loss: 0.48343709690703285\n",
      "Epoch: 150, Train Loss: 0.030261235279207932, Val Loss: 0.4870510755313767\n",
      "Epoch: 151, Train Loss: 0.02642899778188218, Val Loss: 0.41532998118135667\n",
      "Epoch: 152, Train Loss: 0.02060231728047498, Val Loss: 0.45306790702872807\n",
      "Epoch: 153, Train Loss: 0.022927648454898662, Val Loss: 0.3895535667737325\n",
      "Epoch: 154, Train Loss: 0.01734176976237847, Val Loss: 0.4082839637994766\n",
      "Epoch: 155, Train Loss: 0.03013702234565693, Val Loss: 0.37664840122063953\n",
      "\n",
      "Best validation loss: 0.27526439136515063\n",
      "\n",
      "Saving best model for epoch: 156\n",
      "\n",
      "Epoch: 156, Train Loss: 0.02792659843682394, Val Loss: 0.27526439136515063\n",
      "Epoch: 157, Train Loss: 0.027175200451018077, Val Loss: 0.47143271565437317\n",
      "Epoch: 158, Train Loss: 0.03797594019981054, Val Loss: 0.4049799376063877\n",
      "Epoch: 159, Train Loss: 0.01852430070638421, Val Loss: 0.4039447208245595\n",
      "Epoch: 160, Train Loss: 0.01300564882492122, Val Loss: 0.42743492995699245\n",
      "Epoch: 161, Train Loss: 0.02712405103780447, Val Loss: 0.47332550833622616\n",
      "Epoch: 162, Train Loss: 0.027067515561784487, Val Loss: 0.39010856880082023\n",
      "Epoch: 163, Train Loss: 0.03352170983400149, Val Loss: 0.41586758030785453\n",
      "Epoch: 164, Train Loss: 0.020470099180312003, Val Loss: 0.3753746019469367\n",
      "Epoch: 165, Train Loss: 0.025468282888590394, Val Loss: 0.5733620127042135\n",
      "Epoch: 166, Train Loss: 0.022911363468458287, Val Loss: 0.5309342063135571\n",
      "Epoch: 167, Train Loss: 0.026341903506766393, Val Loss: 0.48718956112861633\n",
      "Epoch: 168, Train Loss: 0.015443702920228007, Val Loss: 0.400041530529658\n",
      "Epoch: 169, Train Loss: 0.011700182861377355, Val Loss: 0.3910265937447548\n",
      "Epoch: 170, Train Loss: 0.007914132338026066, Val Loss: 0.5032331877284579\n",
      "Epoch: 171, Train Loss: 0.01853547262007966, Val Loss: 0.47924813959333634\n",
      "Epoch: 172, Train Loss: 0.02207489533842058, Val Loss: 0.4424386007918252\n",
      "Epoch: 173, Train Loss: 0.025584568878889224, Val Loss: 0.48779603507783675\n",
      "Epoch: 174, Train Loss: 0.03018571947895792, Val Loss: 0.5524342689249251\n",
      "Epoch: 175, Train Loss: 0.03565644586305547, Val Loss: 0.5516529828310013\n",
      "Epoch: 176, Train Loss: 0.01731911104222571, Val Loss: 0.4965941475497352\n",
      "Epoch: 177, Train Loss: 0.02130386407879075, Val Loss: 0.5074616041448381\n",
      "Epoch: 178, Train Loss: 0.02363261711029218, Val Loss: 0.5258966833353043\n",
      "Epoch: 179, Train Loss: 0.01805492251583882, Val Loss: 0.5010169678264194\n",
      "Epoch: 180, Train Loss: 0.01947876737420624, Val Loss: 0.4104577385716968\n",
      "Epoch: 181, Train Loss: 0.013932987646777419, Val Loss: 0.39443719469838673\n",
      "Epoch: 182, Train Loss: 0.014045172892997183, Val Loss: 0.4447393947177463\n",
      "Epoch: 183, Train Loss: 0.01898894334673186, Val Loss: 0.47573571072684395\n",
      "Epoch: 184, Train Loss: 0.034910963425306675, Val Loss: 0.4468022750483619\n",
      "Epoch: 185, Train Loss: 0.035396755223539716, Val Loss: 0.40598373611768085\n",
      "Epoch: 186, Train Loss: 0.034576016921797605, Val Loss: 0.6162067883544498\n",
      "Epoch: 187, Train Loss: 0.022513544704187427, Val Loss: 0.5483537117640177\n",
      "Epoch: 188, Train Loss: 0.023137197683895407, Val Loss: 0.470022761159473\n",
      "Epoch: 189, Train Loss: 0.017160009609718065, Val Loss: 0.5770516246557236\n",
      "Epoch: 190, Train Loss: 0.032527377816429955, Val Loss: 0.53842850195037\n",
      "Epoch: 191, Train Loss: 0.02050463720615305, Val Loss: 0.5776545107364655\n",
      "Epoch: 192, Train Loss: 0.016251438568844284, Val Loss: 0.46252331137657166\n",
      "Epoch: 193, Train Loss: 0.016816695831580488, Val Loss: 0.5280759168995751\n",
      "Epoch: 194, Train Loss: 0.008507598937331416, Val Loss: 0.4536343415578206\n",
      "Epoch: 195, Train Loss: 0.010453055915017495, Val Loss: 0.4848872340387768\n",
      "Epoch: 196, Train Loss: 0.014435005675790584, Val Loss: 0.4118148552046882\n",
      "Epoch: 197, Train Loss: 0.019658909347262, Val Loss: 0.44457008441289264\n",
      "Epoch: 198, Train Loss: 0.023333043159174296, Val Loss: 0.3763277091913753\n",
      "Epoch: 199, Train Loss: 0.014935833901900775, Val Loss: 0.34274165249533123\n",
      "Epoch: 200, Train Loss: 0.016250047100342432, Val Loss: 0.47748786128229564\n",
      "Epoch: 201, Train Loss: 0.01890581348082758, Val Loss: 0.4046717468235228\n",
      "Epoch: 202, Train Loss: 0.01917119932440522, Val Loss: 0.4867660353581111\n",
      "Epoch: 203, Train Loss: 0.03182656022784878, Val Loss: 0.4752012971374724\n",
      "Epoch: 204, Train Loss: 0.03618984832723118, Val Loss: 0.5006812645329369\n",
      "Epoch: 205, Train Loss: 0.011325486172896904, Val Loss: 0.5238993863264719\n",
      "Epoch: 206, Train Loss: 0.015959286577837496, Val Loss: 0.5578041656149758\n",
      "Epoch: 207, Train Loss: 0.015166612618670599, Val Loss: 0.5869297981262207\n",
      "Epoch: 208, Train Loss: 0.019341482816240454, Val Loss: 0.4469502576523357\n",
      "Epoch: 209, Train Loss: 0.015815833431507956, Val Loss: 0.5389268166489072\n",
      "Epoch: 210, Train Loss: 0.019782401353144383, Val Loss: 0.5687883479727639\n",
      "Epoch: 211, Train Loss: 0.023957116615551794, Val Loss: 0.42933424272471005\n",
      "Epoch: 212, Train Loss: 0.01990730276948749, Val Loss: 0.4780661877658632\n",
      "Epoch: 213, Train Loss: 0.013097041896234939, Val Loss: 0.4284183283646901\n",
      "Epoch: 214, Train Loss: 0.021984120592168425, Val Loss: 0.4831516080432468\n",
      "Epoch: 215, Train Loss: 0.03235324668105621, Val Loss: 0.3766832864946789\n",
      "Epoch: 216, Train Loss: 0.022582528884655242, Val Loss: 0.45922302620278466\n",
      "Epoch: 217, Train Loss: 0.024790055872990362, Val Loss: 0.41836221019426983\n",
      "Epoch: 218, Train Loss: 0.021998878438733046, Val Loss: 0.5496400644381841\n",
      "Epoch: 219, Train Loss: 0.019771994651174407, Val Loss: 0.4689030945301056\n",
      "Epoch: 220, Train Loss: 0.00879991835107013, Val Loss: 0.4491799465484089\n",
      "Epoch: 221, Train Loss: 0.017613617846074726, Val Loss: 0.39881937868065304\n",
      "Epoch: 222, Train Loss: 0.02196870029563544, Val Loss: 0.48561445706420475\n",
      "Epoch: 223, Train Loss: 0.021351253701141104, Val Loss: 0.4929993036720488\n",
      "Epoch: 224, Train Loss: 0.011391709874018649, Val Loss: 0.6672370235125223\n",
      "Epoch: 225, Train Loss: 0.021987353428709024, Val Loss: 0.5929476585653093\n",
      "Epoch: 226, Train Loss: 0.014255930276888222, Val Loss: 0.5359550714492798\n",
      "Epoch: 227, Train Loss: 0.025614406437542335, Val Loss: 0.4923686037460963\n",
      "Epoch: 228, Train Loss: 0.018515546384703697, Val Loss: 0.6276952226956686\n",
      "Epoch: 229, Train Loss: 0.023060500169765276, Val Loss: 0.42316080754001933\n",
      "Epoch: 230, Train Loss: 0.02372985530778754, Val Loss: 0.4753989047474331\n",
      "Epoch: 231, Train Loss: 0.015623295578009249, Val Loss: 0.5217478167679575\n",
      "Epoch: 232, Train Loss: 0.014207706548888059, Val Loss: 0.5195096929868063\n",
      "Epoch: 233, Train Loss: 0.01014322737567834, Val Loss: 0.4956689145829942\n",
      "Epoch: 234, Train Loss: 0.015784814412251727, Val Loss: 0.612155020236969\n",
      "Epoch: 235, Train Loss: 0.0126413946482795, Val Loss: 0.533180026544465\n",
      "Epoch: 236, Train Loss: 0.012042853468255423, Val Loss: 0.4598090677625603\n",
      "Epoch: 237, Train Loss: 0.016184776282760233, Val Loss: 0.45471040573385024\n",
      "Epoch: 238, Train Loss: 0.01891700451795006, Val Loss: 0.5519487278329002\n",
      "Epoch: 239, Train Loss: 0.026253199792036764, Val Loss: 0.49177562197049457\n",
      "Epoch: 240, Train Loss: 0.026625252464385348, Val Loss: 0.656923332148128\n",
      "Epoch: 241, Train Loss: 0.02571846014168703, Val Loss: 0.3660832155081961\n",
      "Epoch: 242, Train Loss: 0.01753961833494515, Val Loss: 0.445981598769625\n",
      "Epoch: 243, Train Loss: 0.009966172983695287, Val Loss: 0.5251485210739903\n",
      "Epoch: 244, Train Loss: 0.01082131767327178, Val Loss: 0.4129184815618727\n",
      "Epoch: 245, Train Loss: 0.006356368264842107, Val Loss: 0.4644116626845466\n",
      "Epoch: 246, Train Loss: 0.011177068539100103, Val Loss: 0.5004946879214711\n",
      "Epoch: 247, Train Loss: 0.02698291817575375, Val Loss: 0.626989656024509\n",
      "Epoch: 248, Train Loss: 0.017867055432753094, Val Loss: 0.39689907597170937\n",
      "Epoch: 249, Train Loss: 0.013769205969915271, Val Loss: 0.5719618201255798\n",
      "Epoch: 250, Train Loss: 0.021223744876127208, Val Loss: 0.45644885632726884\n",
      "Epoch: 251, Train Loss: 0.014815481586118017, Val Loss: 0.4843400973412726\n",
      "Epoch: 252, Train Loss: 0.012694216406457657, Val Loss: 0.4181617349386215\n",
      "Epoch: 253, Train Loss: 0.018782777574703192, Val Loss: 0.5080506470468309\n",
      "Epoch: 254, Train Loss: 0.023978526349241337, Val Loss: 0.5070149368709989\n",
      "Epoch: 255, Train Loss: 0.018622114913997866, Val Loss: 0.45431937277317047\n",
      "Epoch: 256, Train Loss: 0.011059999107302144, Val Loss: 0.4452782053914335\n",
      "Epoch: 257, Train Loss: 0.010832340710559165, Val Loss: 0.41666298401024604\n",
      "Epoch: 258, Train Loss: 0.01106558348563547, Val Loss: 0.44178131967782974\n",
      "Epoch: 259, Train Loss: 0.014985882274657665, Val Loss: 0.3896183549529976\n",
      "Epoch: 260, Train Loss: 0.02141548656266112, Val Loss: 0.4162498224112723\n",
      "Epoch: 261, Train Loss: 0.012654987351727213, Val Loss: 0.5868411660194397\n",
      "Epoch: 262, Train Loss: 0.015655677315051065, Val Loss: 0.4935196191072464\n",
      "Epoch: 263, Train Loss: 0.011947923319641195, Val Loss: 0.4577914567457305\n",
      "Epoch: 264, Train Loss: 0.026016067782022013, Val Loss: 0.6193727486663394\n",
      "Epoch: 265, Train Loss: 0.02120630407140036, Val Loss: 0.6106583144929674\n",
      "Epoch: 266, Train Loss: 0.021575772271291177, Val Loss: 0.4514464918110106\n",
      "Epoch: 267, Train Loss: 0.02127267493825123, Val Loss: 0.4447968602180481\n",
      "Epoch: 268, Train Loss: 0.012756163239459908, Val Loss: 0.3872638584838973\n",
      "Epoch: 269, Train Loss: 0.017314687489879705, Val Loss: 0.5770167758067449\n",
      "Epoch: 270, Train Loss: 0.013298292992554776, Val Loss: 0.5057008647256427\n",
      "Epoch: 271, Train Loss: 0.014817703189438834, Val Loss: 0.36809222772717476\n",
      "Epoch: 272, Train Loss: 0.018319038347461958, Val Loss: 0.49664024263620377\n",
      "Epoch: 273, Train Loss: 0.018747419636855726, Val Loss: 0.4494568184018135\n",
      "Epoch: 274, Train Loss: 0.014573848243401964, Val Loss: 0.43977315061622196\n",
      "Epoch: 275, Train Loss: 0.014954359558539148, Val Loss: 0.38230105075571275\n",
      "Epoch: 276, Train Loss: 0.018024396996420538, Val Loss: 0.45663968059751725\n",
      "Epoch: 277, Train Loss: 0.016229924900402738, Val Loss: 0.4566336472829183\n",
      "Epoch: 278, Train Loss: 0.013433068666697032, Val Loss: 0.47274042169253033\n",
      "Epoch: 279, Train Loss: 0.013668862742679842, Val Loss: 0.5451549092928568\n",
      "Epoch: 280, Train Loss: 0.014002994617540832, Val Loss: 0.4723482413424386\n",
      "Epoch: 281, Train Loss: 0.019665492169892038, Val Loss: 0.49501986304918927\n",
      "Epoch: 282, Train Loss: 0.01350486738908641, Val Loss: 0.47885952558782363\n",
      "Epoch: 283, Train Loss: 0.012751018608584293, Val Loss: 0.5062635259495841\n",
      "Epoch: 284, Train Loss: 0.023230843983525765, Val Loss: 0.5456965797477298\n",
      "Epoch: 285, Train Loss: 0.024171196787081704, Val Loss: 0.4318537811438243\n",
      "Epoch: 286, Train Loss: 0.016008040208304656, Val Loss: 0.4960802280240589\n",
      "Epoch: 287, Train Loss: 0.015824130464410054, Val Loss: 0.5140266004535887\n",
      "Epoch: 288, Train Loss: 0.01843560970339724, Val Loss: 0.5226055035988489\n",
      "Epoch: 289, Train Loss: 0.01190270472902221, Val Loss: 0.5126232074366676\n",
      "Epoch: 290, Train Loss: 0.011082795203244903, Val Loss: 0.49116450051466626\n",
      "Epoch: 291, Train Loss: 0.004657490936425129, Val Loss: 0.5303426500823762\n",
      "Epoch: 292, Train Loss: 0.005552192056366269, Val Loss: 0.5651272535324097\n",
      "Epoch: 293, Train Loss: 0.006968963675567869, Val Loss: 0.6010701772239473\n",
      "Epoch: 294, Train Loss: 0.005233660698442829, Val Loss: 0.5758350425296359\n",
      "Epoch: 295, Train Loss: 0.008017104869222864, Val Loss: 0.5432679951190948\n",
      "Epoch: 296, Train Loss: 0.01883598555857905, Val Loss: 0.5134042277932167\n",
      "Epoch: 297, Train Loss: 0.017746094274231457, Val Loss: 0.5383701142337587\n",
      "Epoch: 298, Train Loss: 0.018774106257147143, Val Loss: 0.6424116525385115\n",
      "Epoch: 299, Train Loss: 0.015937198863417806, Val Loss: 0.57730967261725\n",
      "Epoch: 300, Train Loss: 0.013620356389960933, Val Loss: 0.5710080928272672\n",
      "Epoch: 301, Train Loss: 0.012395877179820857, Val Loss: 0.45792998373508453\n",
      "Epoch: 302, Train Loss: 0.018733048908260674, Val Loss: 0.48613761199845207\n",
      "Epoch: 303, Train Loss: 0.0149202693989272, Val Loss: 0.6419910225603316\n",
      "Epoch: 304, Train Loss: 0.013702975689025324, Val Loss: 0.49258668306801057\n",
      "Epoch: 305, Train Loss: 0.010217783849758591, Val Loss: 0.5972302969959047\n",
      "Epoch: 306, Train Loss: 0.011246063636359464, Val Loss: 0.5433776552478472\n",
      "Epoch: 307, Train Loss: 0.01140265521126257, Val Loss: 0.558985187775559\n",
      "Epoch: 308, Train Loss: 0.023053037745106666, Val Loss: 0.5060612807671229\n",
      "Epoch: 309, Train Loss: 0.017514748612265516, Val Loss: 0.4668118721908993\n",
      "Epoch: 310, Train Loss: 0.010918667409079048, Val Loss: 0.49273406796985203\n",
      "Epoch: 311, Train Loss: 0.015178410390898869, Val Loss: 0.47736789368920857\n",
      "Epoch: 312, Train Loss: 0.008728702061535507, Val Loss: 0.5561970008744134\n",
      "Epoch: 313, Train Loss: 0.016518831806024536, Val Loss: 0.5428413152694702\n",
      "Epoch: 314, Train Loss: 0.01598331048020716, Val Loss: 0.554781135585573\n",
      "Epoch: 315, Train Loss: 0.01727031608818753, Val Loss: 0.5377910501427121\n",
      "Epoch: 316, Train Loss: 0.011978905863785097, Val Loss: 0.5613991204235289\n",
      "Epoch: 317, Train Loss: 0.007859094113168966, Val Loss: 0.5785194370481703\n",
      "Epoch: 318, Train Loss: 0.008516049481242382, Val Loss: 0.5273349417580498\n",
      "Epoch: 319, Train Loss: 0.014651300362384984, Val Loss: 0.5607152150736915\n",
      "Epoch: 320, Train Loss: 0.013004636174271214, Val Loss: 0.5485221793254217\n",
      "Epoch: 321, Train Loss: 0.025568097424022912, Val Loss: 0.5005609260665046\n",
      "Epoch: 322, Train Loss: 0.019288019327995003, Val Loss: 0.499054708207647\n",
      "Epoch: 323, Train Loss: 0.014618480400863115, Val Loss: 0.4716443287001716\n",
      "Epoch: 324, Train Loss: 0.009422381633975178, Val Loss: 0.4596914094355371\n",
      "Epoch: 325, Train Loss: 0.015109142968176918, Val Loss: 0.5008007751570808\n",
      "Epoch: 326, Train Loss: 0.011817023222728137, Val Loss: 0.47603839221927857\n",
      "Epoch: 327, Train Loss: 0.007780601649583447, Val Loss: 0.5107796457078722\n",
      "Epoch: 328, Train Loss: 0.008914749291312845, Val Loss: 0.5312187522649765\n",
      "Epoch: 329, Train Loss: 0.009701808952073246, Val Loss: 0.4892606793178452\n",
      "Epoch: 330, Train Loss: 0.008369802466934088, Val Loss: 0.49052391946315765\n",
      "Epoch: 331, Train Loss: 0.009181438861330058, Val Loss: 0.5993676119380527\n",
      "Epoch: 332, Train Loss: 0.01792491645507273, Val Loss: 0.6583237563156419\n",
      "Epoch: 333, Train Loss: 0.01890582504388994, Val Loss: 0.4677848716576894\n",
      "Epoch: 334, Train Loss: 0.010480174884315038, Val Loss: 0.4037930973702007\n",
      "Epoch: 335, Train Loss: 0.013056729434377107, Val Loss: 0.4947215798828337\n",
      "Epoch: 336, Train Loss: 0.01305304064940001, Val Loss: 0.6656919452879164\n",
      "Epoch: 337, Train Loss: 0.012428624594427168, Val Loss: 0.5990754663944244\n",
      "Epoch: 338, Train Loss: 0.010528471533988625, Val Loss: 0.6012188427978091\n",
      "Epoch: 339, Train Loss: 0.016813581100041695, Val Loss: 0.5430783612860574\n",
      "Epoch: 340, Train Loss: 0.014589252054641212, Val Loss: 0.48085232410166\n",
      "Epoch: 341, Train Loss: 0.015622440685823491, Val Loss: 0.551817269788848\n",
      "Epoch: 342, Train Loss: 0.010731351014766017, Val Loss: 0.46071527401606244\n",
      "Epoch: 343, Train Loss: 0.01450741199019094, Val Loss: 0.49851377142800224\n",
      "Epoch: 344, Train Loss: 0.011049787512714414, Val Loss: 0.542156414853202\n",
      "Epoch: 345, Train Loss: 0.013935519596399692, Val Loss: 0.5944309466414981\n",
      "Epoch: 346, Train Loss: 0.009857584012519786, Val Loss: 0.44826041493150925\n",
      "Epoch: 347, Train Loss: 0.019555777847351317, Val Loss: 0.5458345595333312\n",
      "Epoch: 348, Train Loss: 0.007741289216431176, Val Loss: 0.5320796039369371\n",
      "Epoch: 349, Train Loss: 0.006808057404749532, Val Loss: 0.5266117850939432\n",
      "Epoch: 350, Train Loss: 0.01379035379887924, Val Loss: 0.6356355448563894\n",
      "Epoch: 351, Train Loss: 0.0031709135340101564, Val Loss: 0.5327499773767259\n",
      "Epoch: 352, Train Loss: 0.005492515901269029, Val Loss: 0.46309467322296566\n",
      "Epoch: 353, Train Loss: 0.01700507957696262, Val Loss: 0.5970115297370486\n",
      "Epoch: 354, Train Loss: 0.0226861512615593, Val Loss: 0.4019025464852651\n",
      "Epoch: 355, Train Loss: 0.01659097053980767, Val Loss: 0.5594468514124552\n",
      "Epoch: 356, Train Loss: 0.01644160083507606, Val Loss: 0.5020417438613044\n",
      "Epoch: 357, Train Loss: 0.00856631860572438, Val Loss: 0.5613317340612411\n",
      "Epoch: 358, Train Loss: 0.01033421872889909, Val Loss: 0.7022404554817412\n",
      "Epoch: 359, Train Loss: 0.01628246143312176, Val Loss: 0.6127062108781602\n",
      "Epoch: 360, Train Loss: 0.01284546991979668, Val Loss: 0.6173410796456866\n",
      "Epoch: 361, Train Loss: 0.015521354100569682, Val Loss: 0.5434492983751826\n",
      "Epoch: 362, Train Loss: 0.010777003177580912, Val Loss: 0.5021392910016907\n",
      "Epoch: 363, Train Loss: 0.007156286465355543, Val Loss: 0.5678045174313916\n",
      "Epoch: 364, Train Loss: 0.008273828827832749, Val Loss: 0.5491302394204669\n",
      "Epoch: 365, Train Loss: 0.011254311264127853, Val Loss: 0.6007937126689487\n",
      "Epoch: 366, Train Loss: 0.010089781850667027, Val Loss: 0.48992352849907345\n",
      "Epoch: 367, Train Loss: 0.012922444341032282, Val Loss: 0.5834298961692386\n",
      "Epoch: 368, Train Loss: 0.015117671006562519, Val Loss: 0.4836447834968567\n",
      "Epoch: 369, Train Loss: 0.016819740510751975, Val Loss: 0.5929581522941589\n",
      "Epoch: 370, Train Loss: 0.016226823169987095, Val Loss: 0.5494686894946628\n",
      "Epoch: 371, Train Loss: 0.00978655262146207, Val Loss: 0.45860959423912895\n",
      "Epoch: 372, Train Loss: 0.00572968401444731, Val Loss: 0.484689542506304\n",
      "Epoch: 373, Train Loss: 0.013524896580552023, Val Loss: 0.4686842660109202\n",
      "Epoch: 374, Train Loss: 0.010310614744608019, Val Loss: 0.5299248380793465\n",
      "Epoch: 375, Train Loss: 0.0046909350714739485, Val Loss: 0.4248005168305503\n",
      "Epoch: 376, Train Loss: 0.008401061216928508, Val Loss: 0.5187874419821633\n",
      "Epoch: 377, Train Loss: 0.014495562539230355, Val Loss: 0.5020356426636378\n",
      "Epoch: 378, Train Loss: 0.009495245162736555, Val Loss: 0.48659421544935966\n",
      "Epoch: 379, Train Loss: 0.009902857179728431, Val Loss: 0.5098441077603234\n",
      "Epoch: 380, Train Loss: 0.009926693114456485, Val Loss: 0.5061214417219162\n",
      "Epoch: 381, Train Loss: 0.018312403191908123, Val Loss: 0.629236618677775\n",
      "Epoch: 382, Train Loss: 0.022892896437433936, Val Loss: 0.6132945716381073\n",
      "Epoch: 383, Train Loss: 0.011774145836700226, Val Loss: 0.45715325077374774\n",
      "Epoch: 384, Train Loss: 0.010610765921573827, Val Loss: 0.45673782626787823\n",
      "Epoch: 385, Train Loss: 0.007717074112302795, Val Loss: 0.4485721422566308\n",
      "Epoch: 386, Train Loss: 0.008059347212847685, Val Loss: 0.4880225840542052\n",
      "Epoch: 387, Train Loss: 0.018069299189351747, Val Loss: 0.45919593009683823\n",
      "Epoch: 388, Train Loss: 0.008737346038621661, Val Loss: 0.5183065285285314\n",
      "Epoch: 389, Train Loss: 0.012746445305057495, Val Loss: 0.6329177303446664\n",
      "Epoch: 390, Train Loss: 0.014760718839884076, Val Loss: 0.5175956901576784\n",
      "Epoch: 391, Train Loss: 0.01325597298660174, Val Loss: 0.4842613554663128\n",
      "Epoch: 392, Train Loss: 0.01722627372786335, Val Loss: 0.5509313212500678\n",
      "Epoch: 393, Train Loss: 0.010787079524550191, Val Loss: 0.5440558741490046\n",
      "Epoch: 394, Train Loss: 0.012239161870535168, Val Loss: 0.571591607398457\n",
      "Epoch: 395, Train Loss: 0.017748843797919477, Val Loss: 0.46746742394235397\n",
      "Epoch: 396, Train Loss: 0.01028024806008126, Val Loss: 0.4285162455505795\n",
      "Epoch: 397, Train Loss: 0.01111079828363852, Val Loss: 0.5337916670574082\n",
      "Epoch: 398, Train Loss: 0.006515534502370106, Val Loss: 0.512733068731096\n",
      "Epoch: 399, Train Loss: 0.011423076488150585, Val Loss: 0.5789892541037666\n",
      "Epoch: 400, Train Loss: 0.008399130285489505, Val Loss: 0.6177015801270803\n",
      "Epoch: 401, Train Loss: 0.01044393707357076, Val Loss: 0.6389954595102204\n",
      "Epoch: 402, Train Loss: 0.006318843681192102, Val Loss: 0.5389443271689944\n",
      "Epoch: 403, Train Loss: 0.006443839498203914, Val Loss: 0.5620106938812468\n",
      "Epoch: 404, Train Loss: 0.010341874449871204, Val Loss: 0.6127318839232127\n",
      "Epoch: 405, Train Loss: 0.010399364825423595, Val Loss: 0.49257583419481915\n",
      "Epoch: 406, Train Loss: 0.013883219193648754, Val Loss: 0.5811227427588569\n",
      "Epoch: 407, Train Loss: 0.010627809496592035, Val Loss: 0.6676248100896677\n",
      "Epoch: 408, Train Loss: 0.02274383817868649, Val Loss: 0.5653358333640628\n",
      "Epoch: 409, Train Loss: 0.02405192025018343, Val Loss: 0.5854225870635774\n",
      "Epoch: 410, Train Loss: 0.02339352242397351, Val Loss: 0.5646680427922143\n",
      "Epoch: 411, Train Loss: 0.019485500780239265, Val Loss: 0.5326958878172768\n",
      "Epoch: 412, Train Loss: 0.009101225874980594, Val Loss: 0.5688969377014372\n",
      "Epoch: 413, Train Loss: 0.008710409978885775, Val Loss: 0.5870493931902779\n",
      "Epoch: 414, Train Loss: 0.008001987664124787, Val Loss: 0.49775462680392796\n",
      "Epoch: 415, Train Loss: 0.005478550720690231, Val Loss: 0.48711056841744316\n",
      "Epoch: 416, Train Loss: 0.010924601722383413, Val Loss: 0.6166005697515275\n",
      "Epoch: 417, Train Loss: 0.008555873565444327, Val Loss: 0.5042659143606821\n",
      "Epoch: 418, Train Loss: 0.0062144179843201105, Val Loss: 0.5199407703346677\n",
      "Epoch: 419, Train Loss: 0.006637222762003171, Val Loss: 0.5786656075053744\n",
      "Epoch: 420, Train Loss: 0.00516959806778295, Val Loss: 0.6063802407847511\n",
      "Epoch: 421, Train Loss: 0.005846957587005196, Val Loss: 0.6440009507868025\n",
      "Epoch: 422, Train Loss: 0.00534346047609262, Val Loss: 0.6184030754698647\n",
      "Epoch: 423, Train Loss: 0.005573511590792126, Val Loss: 0.6323696656359566\n",
      "Epoch: 424, Train Loss: 0.009589958795821744, Val Loss: 0.5628888110319773\n",
      "Epoch: 425, Train Loss: 0.015561198715129395, Val Loss: 0.5669144557582008\n",
      "Epoch: 426, Train Loss: 0.00822226239725302, Val Loss: 0.5909663538138071\n",
      "Epoch: 427, Train Loss: 0.011379226149926315, Val Loss: 0.5561330980724759\n",
      "Epoch: 428, Train Loss: 0.010975912921488008, Val Loss: 0.700534956322776\n",
      "Epoch: 429, Train Loss: 0.0124439623174066, Val Loss: 0.6595261014170117\n",
      "Epoch: 430, Train Loss: 0.014843830955399, Val Loss: 0.5570360405577554\n",
      "Epoch: 431, Train Loss: 0.021893100072541333, Val Loss: 0.6167354583740234\n",
      "Epoch: 432, Train Loss: 0.014445115247545352, Val Loss: 0.5773845927582847\n",
      "Epoch: 433, Train Loss: 0.009448258718328339, Val Loss: 0.4982115046845542\n",
      "Epoch: 434, Train Loss: 0.01380424529855225, Val Loss: 0.5003030565049913\n",
      "Epoch: 435, Train Loss: 0.0055228700633661795, Val Loss: 0.5968559334675471\n",
      "Epoch: 436, Train Loss: 0.007243022698149398, Val Loss: 0.531499617629581\n",
      "Epoch: 437, Train Loss: 0.011764319222193887, Val Loss: 0.5870285100407071\n",
      "Epoch: 438, Train Loss: 0.006158929749773639, Val Loss: 0.6052710397375954\n",
      "Epoch: 439, Train Loss: 0.01157098471493198, Val Loss: 0.5235987835460238\n",
      "Epoch: 440, Train Loss: 0.008193475790811804, Val Loss: 0.524643717540635\n",
      "Epoch: 441, Train Loss: 0.007741851419498893, Val Loss: 0.5280276752180524\n",
      "Epoch: 442, Train Loss: 0.010494772136035299, Val Loss: 0.4688628257976638\n",
      "Epoch: 443, Train Loss: 0.012865613982699593, Val Loss: 0.6731766644451354\n",
      "Epoch: 444, Train Loss: 0.00989376234866552, Val Loss: 0.6219814179672135\n",
      "Epoch: 445, Train Loss: 0.009879646241101967, Val Loss: 0.5649047477377785\n",
      "Epoch: 446, Train Loss: 0.01378542133944059, Val Loss: 0.5749532911512587\n",
      "Epoch: 447, Train Loss: 0.012918471006062795, Val Loss: 0.5482245923744308\n",
      "Epoch: 448, Train Loss: 0.008850777324679959, Val Loss: 0.6142588886949751\n",
      "Epoch: 449, Train Loss: 0.01231570533887355, Val Loss: 0.5623709542883767\n",
      "Epoch: 450, Train Loss: 0.009755521904400113, Val Loss: 0.5536101890934838\n",
      "Epoch: 451, Train Loss: 0.012564858943986547, Val Loss: 0.5759788337681029\n",
      "Epoch: 452, Train Loss: 0.02472811196768609, Val Loss: 0.7008353173732758\n",
      "Epoch: 453, Train Loss: 0.011251624349778801, Val Loss: 0.6085012985600365\n",
      "Epoch: 454, Train Loss: 0.010793611632558353, Val Loss: 0.5476647069056829\n",
      "Epoch: 455, Train Loss: 0.00757126421100295, Val Loss: 0.6255667358636856\n",
      "Epoch: 456, Train Loss: 0.0070061957187504105, Val Loss: 0.5677956872516208\n",
      "Epoch: 457, Train Loss: 0.007067174211316123, Val Loss: 0.6229612314038806\n",
      "Epoch: 458, Train Loss: 0.00616042811332913, Val Loss: 0.5778159879975848\n",
      "Epoch: 459, Train Loss: 0.007520229346229388, Val Loss: 0.6243065363830991\n",
      "Epoch: 460, Train Loss: 0.007788953052493272, Val Loss: 0.5513850731982125\n",
      "Epoch: 461, Train Loss: 0.005159484827333783, Val Loss: 0.6161492450369729\n",
      "Epoch: 462, Train Loss: 0.007731653551534784, Val Loss: 0.5106398620539241\n",
      "Epoch: 463, Train Loss: 0.006202031141537525, Val Loss: 0.5307342178291745\n",
      "Epoch: 464, Train Loss: 0.008470453794042085, Val Loss: 0.5346183263593249\n",
      "Epoch: 465, Train Loss: 0.012942320139654264, Val Loss: 0.636358337269889\n",
      "Epoch: 466, Train Loss: 0.011624606293066157, Val Loss: 0.5775582078430388\n",
      "Epoch: 467, Train Loss: 0.011698297160007856, Val Loss: 0.5688486413823234\n",
      "Epoch: 468, Train Loss: 0.012017789594014913, Val Loss: 0.4584817157851325\n",
      "Epoch: 469, Train Loss: 0.013467125270237168, Val Loss: 0.6552896797657013\n",
      "Epoch: 470, Train Loss: 0.015133726622123136, Val Loss: 0.5380907787217034\n",
      "Epoch: 471, Train Loss: 0.01934437981944478, Val Loss: 0.5587670008341471\n",
      "Epoch: 472, Train Loss: 0.013488313244895926, Val Loss: 0.612999728984303\n",
      "Epoch: 473, Train Loss: 0.009401744142612384, Val Loss: 0.5124744358989928\n",
      "Epoch: 474, Train Loss: 0.009099616939295894, Val Loss: 0.49152301417456734\n",
      "Epoch: 475, Train Loss: 0.00782416884000288, Val Loss: 0.4909551011191474\n",
      "Epoch: 476, Train Loss: 0.009061163775969144, Val Loss: 0.5450522071785397\n",
      "Epoch: 477, Train Loss: 0.006809830123161133, Val Loss: 0.5271986689832475\n",
      "Epoch: 478, Train Loss: 0.005831463826924144, Val Loss: 0.6826593346065946\n",
      "Epoch: 479, Train Loss: 0.008842793510010306, Val Loss: 0.6392295526133643\n",
      "Epoch: 480, Train Loss: 0.010582977367909124, Val Loss: 0.6809421049224006\n",
      "Epoch: 481, Train Loss: 0.00759206396257549, Val Loss: 0.6411691970295377\n",
      "Epoch: 482, Train Loss: 0.013491023863393879, Val Loss: 0.5303047961658902\n",
      "Epoch: 483, Train Loss: 0.011686684665849647, Val Loss: 0.4712463749779595\n",
      "Epoch: 484, Train Loss: 0.004648773799953567, Val Loss: 0.5577652189466689\n",
      "Epoch: 485, Train Loss: 0.008243621017892928, Val Loss: 0.4970383428865009\n",
      "Epoch: 486, Train Loss: 0.00999535502321319, Val Loss: 0.7439290649361081\n",
      "Epoch: 487, Train Loss: 0.019489431284662576, Val Loss: 0.5803537534342872\n",
      "Epoch: 488, Train Loss: 0.012899247826393134, Val Loss: 0.4702470766173469\n",
      "Epoch: 489, Train Loss: 0.012083061876827615, Val Loss: 0.5344599650965797\n",
      "Epoch: 490, Train Loss: 0.005506434686197546, Val Loss: 0.5709519237279892\n",
      "Epoch: 491, Train Loss: 0.006938669968711778, Val Loss: 0.5743129667308595\n",
      "Epoch: 492, Train Loss: 0.0058206823637710495, Val Loss: 0.5781765712632073\n",
      "Epoch: 493, Train Loss: 0.011496143746488179, Val Loss: 0.6511586176024543\n",
      "Epoch: 494, Train Loss: 0.007842998785892195, Val Loss: 0.54555810491244\n",
      "Epoch: 495, Train Loss: 0.007279045516411373, Val Loss: 0.6058168378141191\n",
      "Epoch: 496, Train Loss: 0.008131989515703349, Val Loss: 0.4861683315700955\n",
      "Epoch: 497, Train Loss: 0.009277467333153185, Val Loss: 0.5509223772419823\n",
      "Epoch: 498, Train Loss: 0.007895673134826902, Val Loss: 0.5978549834754732\n",
      "Epoch: 499, Train Loss: 0.014528714940959479, Val Loss: 0.6310317288670275\n",
      "Epoch: 500, Train Loss: 0.00671021519727081, Val Loss: 0.4803640834159321\n"
     ]
    }
   ],
   "source": [
    "# do the finetuning\n",
    "train_loss2,val_loss2 = train_resnet50(resenetModel, train_loader,val_loader,epochs=500,lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_resnet50(model, test_loader):\n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = datasets_utils.convert_images_dict_to_tensor(images, resize=224)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "               \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_acc += (predicted == labels).sum().item()\n",
    "\n",
    "        # print results\n",
    "        print('Test Loss: {:.4f}, Test Acc: {:.4f}'.format(test_loss/len(test_loader), test_acc/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9932, Test Acc: 0.8518\n"
     ]
    }
   ],
   "source": [
    "test_resnet50(resenetModel, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_refo_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbde0c4279c3ddc2983b0416a35647bb63e8617f6532d6730a8523f4e3df93a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
