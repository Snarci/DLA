{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Banana', 'Carambola', 'Guava', 'Kiwi', 'Mango', 'Orange', 'Peach', 'Pear', 'Persimmon', 'Pitaya', 'Plum', 'Pomegranate', 'Tomatoes', 'muskmelon']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>./vegetables/Apple\\Apple 01.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple</td>\n",
       "      <td>./vegetables/Apple\\Apple 010.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>./vegetables/Apple\\Apple 0100.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>./vegetables/Apple\\Apple 01000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>./vegetables/Apple\\Apple 01001.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                path\n",
       "0  Apple     ./vegetables/Apple\\Apple 01.png\n",
       "1  Apple    ./vegetables/Apple\\Apple 010.png\n",
       "2  Apple   ./vegetables/Apple\\Apple 0100.png\n",
       "3  Apple  ./vegetables/Apple\\Apple 01000.png\n",
       "4  Apple  ./vegetables/Apple\\Apple 01001.png"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all files from the folder CUB_200_2011 and assign the subfolder as a class\n",
    "#the subfolder name is the class name\n",
    "\n",
    "path = './vegetables/'\n",
    "classes = os.listdir(path)\n",
    "classes.sort()\n",
    "print(classes)\n",
    "#read all files from the subfolders\n",
    "\n",
    "data = []\n",
    "for i in range(len(classes)):\n",
    "    folder = os.path.join(path,classes[i])\n",
    "    files = os.listdir(folder)\n",
    "    for j in range(len(files)):\n",
    "        data.append([classes[i],os.path.join(folder,files[j])])\n",
    "\n",
    "#convert the list to a dataframe\n",
    "df = pd.DataFrame(data,columns=['class','path'])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test and validation\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "import datasets_utils\n",
    "train_df, val_df, test_df = datasets_utils.get_train_valid_test_split(df, SPLITS=[0.6,0.2,0.2], SEED=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumenations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#transformations for train and validation\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224)\n",
    "    ,ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create the Dataset class\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "# class BirdDataset(Dataset):\n",
    "#     def __init__(self,df):\n",
    "#         self.df = df\n",
    "#         self.images = self.df['path'].values\n",
    "#         self.classes = self.df['class'].values\n",
    "#         self.classes = np.array([classes.index(i) for i in self.classes])\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self,idx):\n",
    "#         image = self.images[idx]\n",
    "#         image = plt.imread(image)\n",
    "#         #print(image.shape)\n",
    "#         if len(image.shape) == 2:\n",
    "#             image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "#         image = transform(image=image)['image']\n",
    "#         if image.shape[1] == 1:\n",
    "#             image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "#         image=image_processor(images=image, return_tensors=\"pt\")\n",
    "#         #print(image['pixel_values'].shape)\n",
    "#         return image\n",
    "#create the Dataset class\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "class MelanzaDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.images = self.df['path'].values\n",
    "        self.classes = self.df['class'].values\n",
    "        self.classes = np.array([classes.index(i) for i in self.classes])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = self.images[idx]\n",
    "        image = plt.imread(image)\n",
    "        #print(image.shape)\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "        image = transform(image=image)['image']\n",
    "        if image.shape[1] == 1:\n",
    "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "        image=image_processor(images=image, return_tensors=\"pt\")\n",
    "        #print(image['pixel_values'].shape)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(MelanzaDataset(train_df),batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(MelanzaDataset(val_df),batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(MelanzaDataset(test_df),batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#try the dataloader\n",
    "for index,i in enumerate(train_loader):\n",
    "    #print pixel values and class\n",
    "    #print(i.shape)\n",
    "    #image=image_processor(images=i, return_tensors=\"pt\")\n",
    "    #reshape the pixel values to nx3x224x224\n",
    "    i['pixel_values']=i['pixel_values'].to(torch.float32)\n",
    "    s = i['pixel_values'].shape\n",
    "    i['pixel_values']=i['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "    print(i['pixel_values'].shape)\n",
    "    #if index == 1000:\n",
    "    #    break\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEForPreTraining(\n",
       "  (vit): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "from torchvision.utils import save_image\n",
    "output_path = './models/melanzana/'\n",
    "def train(model,train_loader,val_loader,epochs=10,lr=1e-2):\n",
    "    current_valid_loss , best_valid_loss = float('inf'), float('inf')\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_ = []\n",
    "        val_loss_ = []\n",
    "        for i,images in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "            s = images['pixel_values'].shape\n",
    "            images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "            optimizer.zero_grad()\n",
    "            #print(images['pixel_values'].shape)\n",
    "            output = model(**images)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_.append(loss.item())\n",
    "            #every 100 batches, print the loss\n",
    "            if i%128 == 0:\n",
    "\n",
    "                print(f'Epoch: {epoch+1}, Batch: {i}, Loss: {train_loss_[-1]}')\n",
    "\n",
    "                #reconstruct the original image shape\n",
    "                masks = output.mask\n",
    "                #print(masks.shape)\n",
    "                #porzione immagine\n",
    "                img = output.logits\n",
    "                img = img.reshape(\n",
    "                batch_size, 14, 14, 16, 16, 3\n",
    "                )\n",
    "                #batch\n",
    "                img = torch.einsum(\"nhwpqc->nchpwq\", img)\n",
    "                img = img.reshape(batch_size, 3, 224, 224)\n",
    "                #first img of the batch\n",
    "                img = img[0]\n",
    "                img = img.view(3,224,224)\n",
    "                img_recon = img.cpu().detach().numpy().transpose(1,2,0)\n",
    "                #img = np.maximum(img,0)\n",
    "                img_recon = (img_recon-img_recon.min())/(img_recon.max()-img_recon.min())#np.minimum(img,1)\n",
    "                #print(img.shape)\n",
    "                \n",
    "                img_originale = images['pixel_values'][0].cpu()\n",
    "                #bring image to range 0-1\n",
    "                img_originale = (img_originale-img_originale.min())/(img_originale.max()-img_originale.min())\n",
    "                #print max and min values\n",
    "                #print(img_originale.max(),img_originale.min())\n",
    "                img_originale = img_originale.numpy().transpose(1,2,0)\n",
    "                #cat immagine originale e immagine con maschera\n",
    "                #porzione maschera sopra immagine originale\n",
    "                mask = masks[0]\n",
    "                mask_new = torch.ones(196,768).to(device)\n",
    "                for j in range(196):\n",
    "                    mask_new[j,:] =( 1-mask[j])*mask_new[j,:]\n",
    "                img = mask_new\n",
    "                img = img.reshape(\n",
    "                 14, 14, 16, 16, 3\n",
    "                )\n",
    "                #batch\n",
    "                img = torch.einsum(\"hwpqc->chpwq\", img)\n",
    "                img = img.reshape(3, 224, 224)\n",
    "                #first img of the batch\n",
    "                \n",
    "                img = img.view(3,224,224)\n",
    "                img_masked = img.cpu().detach().numpy().transpose(1,2,0)\n",
    "                img_masked_org = img_masked*img_originale\n",
    "                img_reconv2 = ((1-img_masked)*img_recon)+img_masked_org\n",
    "                img_recon= ((1-img_masked)*img_recon)\n",
    "                #img = np.maximum(img,0)\n",
    "                #img_masked = (img_masked-img_masked.min())/(img_masked.max()-img_masked.min())#np.minimum(img,1)\n",
    "                img_cat = np.concatenate((img_originale,img_masked_org,img_recon,img_reconv2),axis=1)\n",
    "                plt.imsave(f'outputs_newB/epoch_{epoch+1}_batch_{i}.png',img_cat)\n",
    "        train_loss.append(np.mean(train_loss_))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i,images in enumerate(val_loader):\n",
    "                \n",
    "                images = images.to(device)\n",
    "                images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "                s = images['pixel_values'].shape\n",
    "                images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "                output = model(**images)\n",
    "                loss = output.loss\n",
    "                val_loss_.append(loss.item())\n",
    "\n",
    "            mean_loss = np.mean(val_loss_)\n",
    "            current_valid_loss = mean_loss\n",
    "            if current_valid_loss < best_valid_loss:\n",
    "                best_valid_loss = current_valid_loss\n",
    "                print(f\"\\nBest validation loss: {best_valid_loss}\")\n",
    "                print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "                # torch.save(optimizer.state_dict(), 'jacoExperiments/best_distilled_model.pth')\n",
    "            model.save_pretrained(output_path + '_(call_save_pretrained)_BESTmodel.pth')\n",
    "            val_loss.append(mean_loss)\n",
    "                \n",
    "            val_loss.append(mean_loss)\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}')\n",
    "        #save the last output image on the disk\n",
    "        #save the model\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        if epoch%101==0: \n",
    "            model.save_pretrained(output_path + '_(call_save_pretrained)_model_' + str(epoch) + '.pth')\n",
    "        #torch.save(model.state_dict(),f'last3v2.pth')\n",
    "        \n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 0.21643948554992676\n",
      "Epoch: 1, Batch: 128, Loss: 0.17925329506397247\n",
      "Epoch: 1, Batch: 256, Loss: 0.15531757473945618\n",
      "Epoch: 1, Batch: 384, Loss: 0.14093269407749176\n",
      "\n",
      "Best validation loss: 0.1566525823878546\n",
      "\n",
      "Saving best model for epoch: 1\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ViTMAEForPreTraining' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss,val_loss \u001b[39m=\u001b[39m train(model,train_loader,val_loader,epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,lr\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[19], line 102\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# torch.save(optimizer.state_dict(), 'jacoExperiments/best_distilled_model.pth')\u001b[39;00m\n\u001b[0;32m    101\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(output_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_(call_save_pretrained)_BESTmodel.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m model\u001b[39m.\u001b[39;49msave(output_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_(call_save_pretrained)_BESTmodel.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    103\u001b[0m val_loss\u001b[39m.\u001b[39mappend(mean_loss)\n\u001b[0;32m    105\u001b[0m val_loss\u001b[39m.\u001b[39mappend(mean_loss)\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ViTMAEForPreTraining' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "train_loss,val_loss = train(model,train_loader,val_loader,epochs=500,lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
