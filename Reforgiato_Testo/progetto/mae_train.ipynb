{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.Black_footed_Albatross', '002.Laysan_Albatross', '003.Sooty_Albatross', '004.Groove_billed_Ani', '005.Crested_Auklet', '006.Least_Auklet', '007.Parakeet_Auklet', '008.Rhinoceros_Auklet', '009.Brewer_Blackbird', '010.Red_winged_Blackbird', '011.Rusty_Blackbird', '012.Yellow_headed_Blackbird', '013.Bobolink', '014.Indigo_Bunting', '015.Lazuli_Bunting', '016.Painted_Bunting', '017.Cardinal', '018.Spotted_Catbird', '019.Gray_Catbird', '020.Yellow_breasted_Chat', '021.Eastern_Towhee', '022.Chuck_will_Widow', '023.Brandt_Cormorant', '024.Red_faced_Cormorant', '025.Pelagic_Cormorant', '026.Bronzed_Cowbird', '027.Shiny_Cowbird', '028.Brown_Creeper', '029.American_Crow', '030.Fish_Crow', '031.Black_billed_Cuckoo', '032.Mangrove_Cuckoo', '033.Yellow_billed_Cuckoo', '034.Gray_crowned_Rosy_Finch', '035.Purple_Finch', '036.Northern_Flicker', '037.Acadian_Flycatcher', '038.Great_Crested_Flycatcher', '039.Least_Flycatcher', '040.Olive_sided_Flycatcher', '041.Scissor_tailed_Flycatcher', '042.Vermilion_Flycatcher', '043.Yellow_bellied_Flycatcher', '044.Frigatebird', '045.Northern_Fulmar', '046.Gadwall', '047.American_Goldfinch', '048.European_Goldfinch', '049.Boat_tailed_Grackle', '050.Eared_Grebe', '051.Horned_Grebe', '052.Pied_billed_Grebe', '053.Western_Grebe', '054.Blue_Grosbeak', '055.Evening_Grosbeak', '056.Pine_Grosbeak', '057.Rose_breasted_Grosbeak', '058.Pigeon_Guillemot', '059.California_Gull', '060.Glaucous_winged_Gull', '061.Heermann_Gull', '062.Herring_Gull', '063.Ivory_Gull', '064.Ring_billed_Gull', '065.Slaty_backed_Gull', '066.Western_Gull', '067.Anna_Hummingbird', '068.Ruby_throated_Hummingbird', '069.Rufous_Hummingbird', '070.Green_Violetear', '071.Long_tailed_Jaeger', '072.Pomarine_Jaeger', '073.Blue_Jay', '074.Florida_Jay', '075.Green_Jay', '076.Dark_eyed_Junco', '077.Tropical_Kingbird', '078.Gray_Kingbird', '079.Belted_Kingfisher', '080.Green_Kingfisher', '081.Pied_Kingfisher', '082.Ringed_Kingfisher', '083.White_breasted_Kingfisher', '084.Red_legged_Kittiwake', '085.Horned_Lark', '086.Pacific_Loon', '087.Mallard', '088.Western_Meadowlark', '089.Hooded_Merganser', '090.Red_breasted_Merganser', '091.Mockingbird', '092.Nighthawk', '093.Clark_Nutcracker', '094.White_breasted_Nuthatch', '095.Baltimore_Oriole', '096.Hooded_Oriole', '097.Orchard_Oriole', '098.Scott_Oriole', '099.Ovenbird', '100.Brown_Pelican', '101.White_Pelican', '102.Western_Wood_Pewee', '103.Sayornis', '104.American_Pipit', '105.Whip_poor_Will', '106.Horned_Puffin', '107.Common_Raven', '108.White_necked_Raven', '109.American_Redstart', '110.Geococcyx', '111.Loggerhead_Shrike', '112.Great_Grey_Shrike', '113.Baird_Sparrow', '114.Black_throated_Sparrow', '115.Brewer_Sparrow', '116.Chipping_Sparrow', '117.Clay_colored_Sparrow', '118.House_Sparrow', '119.Field_Sparrow', '120.Fox_Sparrow', '121.Grasshopper_Sparrow', '122.Harris_Sparrow', '123.Henslow_Sparrow', '124.Le_Conte_Sparrow', '125.Lincoln_Sparrow', '126.Nelson_Sharp_tailed_Sparrow', '127.Savannah_Sparrow', '128.Seaside_Sparrow', '129.Song_Sparrow', '130.Tree_Sparrow', '131.Vesper_Sparrow', '132.White_crowned_Sparrow', '133.White_throated_Sparrow', '134.Cape_Glossy_Starling', '135.Bank_Swallow', '136.Barn_Swallow', '137.Cliff_Swallow', '138.Tree_Swallow', '139.Scarlet_Tanager', '140.Summer_Tanager', '141.Artic_Tern', '142.Black_Tern', '143.Caspian_Tern', '144.Common_Tern', '145.Elegant_Tern', '146.Forsters_Tern', '147.Least_Tern', '148.Green_tailed_Towhee', '149.Brown_Thrasher', '150.Sage_Thrasher', '151.Black_capped_Vireo', '152.Blue_headed_Vireo', '153.Philadelphia_Vireo', '154.Red_eyed_Vireo', '155.Warbling_Vireo', '156.White_eyed_Vireo', '157.Yellow_throated_Vireo', '158.Bay_breasted_Warbler', '159.Black_and_white_Warbler', '160.Black_throated_Blue_Warbler', '161.Blue_winged_Warbler', '162.Canada_Warbler', '163.Cape_May_Warbler', '164.Cerulean_Warbler', '165.Chestnut_sided_Warbler', '166.Golden_winged_Warbler', '167.Hooded_Warbler', '168.Kentucky_Warbler', '169.Magnolia_Warbler', '170.Mourning_Warbler', '171.Myrtle_Warbler', '172.Nashville_Warbler', '173.Orange_crowned_Warbler', '174.Palm_Warbler', '175.Pine_Warbler', '176.Prairie_Warbler', '177.Prothonotary_Warbler', '178.Swainson_Warbler', '179.Tennessee_Warbler', '180.Wilson_Warbler', '181.Worm_eating_Warbler', '182.Yellow_Warbler', '183.Northern_Waterthrush', '184.Louisiana_Waterthrush', '185.Bohemian_Waxwing', '186.Cedar_Waxwing', '187.American_Three_toed_Woodpecker', '188.Pileated_Woodpecker', '189.Red_bellied_Woodpecker', '190.Red_cockaded_Woodpecker', '191.Red_headed_Woodpecker', '192.Downy_Woodpecker', '193.Bewick_Wren', '194.Cactus_Wren', '195.Carolina_Wren', '196.House_Wren', '197.Marsh_Wren', '198.Rock_Wren', '199.Winter_Wren', '200.Common_Yellowthroat']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        class  \\\n",
       "0  001.Black_footed_Albatross   \n",
       "1  001.Black_footed_Albatross   \n",
       "2  001.Black_footed_Albatross   \n",
       "3  001.Black_footed_Albatross   \n",
       "4  001.Black_footed_Albatross   \n",
       "\n",
       "                                                path  \n",
       "0  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "1  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "2  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "3  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "4  ./CUB_200_2011/images/001.Black_footed_Albatro...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all files from the folder CUB_200_2011 and assign the subfolder as a class\n",
    "#the subfolder name is the class name\n",
    "\n",
    "path = './CUB_200_2011/images/'\n",
    "classes = os.listdir(path)\n",
    "classes.sort()\n",
    "print(classes)\n",
    "#read all files from the subfolders\n",
    "\n",
    "data = []\n",
    "for i in range(len(classes)):\n",
    "    folder = os.path.join(path,classes[i])\n",
    "    files = os.listdir(folder)\n",
    "    for j in range(len(files)):\n",
    "        data.append([classes[i],os.path.join(folder,files[j])])\n",
    "\n",
    "#convert the list to a dataframe\n",
    "df = pd.DataFrame(data,columns=['class','path'])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumenations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#transformations for train and validation\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 512)\n",
    "    ,ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the Dataset class\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.images = self.df['path'].values\n",
    "        self.classes = self.df['class'].values\n",
    "        self.classes = np.array([classes.index(i) for i in self.classes])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = self.images[idx]\n",
    "        image = plt.imread(image)\n",
    "        #print(image.shape)\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "        image = transform(image=image)['image']\n",
    "        if image.shape[1] == 1:\n",
    "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "        image=image_processor(images=image, return_tensors=\"pt\")\n",
    "        #print(image['pixel_values'].shape)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(BirdDataset(train_df),batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(BirdDataset(val_df),batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(BirdDataset(test_df),batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#try the dataloader\n",
    "for index,i in enumerate(train_loader):\n",
    "    #print pixel values and class\n",
    "    #print(i.shape)\n",
    "    #image=image_processor(images=i, return_tensors=\"pt\")\n",
    "    #reshape the pixel values to nx3x224x224\n",
    "    i['pixel_values']=i['pixel_values'].to(torch.float32)\n",
    "    s = i['pixel_values'].shape\n",
    "    i['pixel_values']=i['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "    print(i['pixel_values'].shape)\n",
    "    #if index == 1000:\n",
    "    #    break\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEForPreTraining(\n",
       "  (vit): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "from torchvision.utils import save_image\n",
    "output_path = './models/mae_cub/'\n",
    "def train(model,train_loader,val_loader,epochs=10,lr=1e-2):\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_ = []\n",
    "        val_loss_ = []\n",
    "        for i,images in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "            s = images['pixel_values'].shape\n",
    "            images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "            optimizer.zero_grad()\n",
    "            #print(images['pixel_values'].shape)\n",
    "            output = model(**images)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_.append(loss.item())\n",
    "            #every 100 batches, print the loss\n",
    "            if i%64 == 0:\n",
    "\n",
    "                print(f'Epoch: {epoch+1}, Batch: {i}, Loss: {train_loss_[-1]}')\n",
    "\n",
    "                #reconstruct the original image shape\n",
    "                masks = output.mask\n",
    "                #print(masks.shape)\n",
    "                #porzione immagine\n",
    "                img = output.logits\n",
    "                img = img.reshape(\n",
    "                batch_size, 14, 14, 16, 16, 3\n",
    "                )\n",
    "                #batch\n",
    "                img = torch.einsum(\"nhwpqc->nchpwq\", img)\n",
    "                img = img.reshape(batch_size, 3, 224, 224)\n",
    "                #first img of the batch\n",
    "                img = img[0]\n",
    "                img = img.view(3,224,224)\n",
    "                img_recon = img.cpu().detach().numpy().transpose(1,2,0)\n",
    "                #img = np.maximum(img,0)\n",
    "                img_recon = (img_recon-img_recon.min())/(img_recon.max()-img_recon.min())#np.minimum(img,1)\n",
    "                #print(img.shape)\n",
    "                \n",
    "                img_originale = images['pixel_values'][0].cpu()\n",
    "                #bring image to range 0-1\n",
    "                img_originale = (img_originale-img_originale.min())/(img_originale.max()-img_originale.min())\n",
    "                #print max and min values\n",
    "                #print(img_originale.max(),img_originale.min())\n",
    "                img_originale = img_originale.numpy().transpose(1,2,0)\n",
    "                #cat immagine originale e immagine con maschera\n",
    "                #porzione maschera sopra immagine originale\n",
    "                mask = masks[0]\n",
    "                mask_new = torch.ones(196,768).to(device)\n",
    "                for j in range(196):\n",
    "                    mask_new[j,:] =( 1-mask[j])*mask_new[j,:]\n",
    "                img = mask_new\n",
    "                img = img.reshape(\n",
    "                 14, 14, 16, 16, 3\n",
    "                )\n",
    "                #batch\n",
    "                img = torch.einsum(\"hwpqc->chpwq\", img)\n",
    "                img = img.reshape(3, 224, 224)\n",
    "                #first img of the batch\n",
    "                \n",
    "                img = img.view(3,224,224)\n",
    "                img_masked = img.cpu().detach().numpy().transpose(1,2,0)\n",
    "                img_masked_org = img_masked*img_originale\n",
    "                img_reconv2 = ((1-img_masked)*img_recon)+img_masked_org\n",
    "                img_recon= ((1-img_masked)*img_recon)\n",
    "                #img = np.maximum(img,0)\n",
    "                #img_masked = (img_masked-img_masked.min())/(img_masked.max()-img_masked.min())#np.minimum(img,1)\n",
    "                img_cat = np.concatenate((img_originale,img_masked_org,img_recon,img_reconv2),axis=1)\n",
    "                plt.imsave(f'outputs_newB/epoch_{epoch+1}_batch_{i}.png',img_cat)\n",
    "        train_loss.append(np.mean(train_loss_))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i,images in enumerate(val_loader):\n",
    "                \n",
    "                images = images.to(device)\n",
    "                images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "                s = images['pixel_values'].shape\n",
    "                images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "                output = model(**images)\n",
    "                loss = output.loss\n",
    "                val_loss_.append(loss.item())\n",
    "                \n",
    "            val_loss.append(np.mean(val_loss_))\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}')\n",
    "        #save the last output image on the disk\n",
    "        #save the model\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        if epoch%10==0: \n",
    "            model.save_pretrained(output_path + 'model_' + str(epoch) + '.pth')\n",
    "        #torch.save(model.state_dict(),f'last3v2.pth')\n",
    "        \n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 0.18139439821243286\n",
      "Epoch: 1, Batch: 64, Loss: 0.20359893143177032\n",
      "Epoch: 1, Train Loss: 0.20456070175110283, Val Loss: 0.20927359461784362\n",
      "Epoch: 2, Batch: 0, Loss: 0.21668057143688202\n",
      "Epoch: 2, Batch: 64, Loss: 0.18825380504131317\n",
      "Epoch: 2, Train Loss: 0.2029977775478767, Val Loss: 0.2097858841220538\n",
      "Epoch: 3, Batch: 0, Loss: 0.20425207912921906\n",
      "Epoch: 3, Batch: 64, Loss: 0.21252861618995667\n",
      "Epoch: 3, Train Loss: 0.20262462595256708, Val Loss: 0.210016930103302\n",
      "Epoch: 4, Batch: 0, Loss: 0.20173120498657227\n",
      "Epoch: 4, Batch: 64, Loss: 0.20470134913921356\n",
      "Epoch: 4, Train Loss: 0.2001144752158957, Val Loss: 0.20983086824417113\n",
      "Epoch: 5, Batch: 0, Loss: 0.20359370112419128\n",
      "Epoch: 5, Batch: 64, Loss: 0.19235554337501526\n",
      "Epoch: 5, Train Loss: 0.1998602867884151, Val Loss: 0.20870208938916524\n",
      "Epoch: 6, Batch: 0, Loss: 0.20553003251552582\n",
      "Epoch: 6, Batch: 64, Loss: 0.176763653755188\n",
      "Epoch: 6, Train Loss: 0.19824350675788976, Val Loss: 0.20973188678423563\n",
      "Epoch: 7, Batch: 0, Loss: 0.19550156593322754\n",
      "Epoch: 7, Batch: 64, Loss: 0.1945314258337021\n",
      "Epoch: 7, Train Loss: 0.19866283566264784, Val Loss: 0.21168835312128068\n",
      "Epoch: 8, Batch: 0, Loss: 0.15270178020000458\n",
      "Epoch: 8, Batch: 64, Loss: 0.20293323695659637\n",
      "Epoch: 8, Train Loss: 0.19712306634854462, Val Loss: 0.20992528746525446\n",
      "Epoch: 9, Batch: 0, Loss: 0.21403595805168152\n",
      "Epoch: 9, Batch: 64, Loss: 0.184915691614151\n",
      "Epoch: 9, Train Loss: 0.1976739353800224, Val Loss: 0.2097287133336067\n",
      "Epoch: 10, Batch: 0, Loss: 0.22263360023498535\n",
      "Epoch: 10, Batch: 64, Loss: 0.2186136096715927\n",
      "Epoch: 10, Train Loss: 0.1965012042704275, Val Loss: 0.21172213107347487\n",
      "Epoch: 11, Batch: 0, Loss: 0.17975914478302002\n",
      "Epoch: 11, Batch: 64, Loss: 0.2230948656797409\n",
      "Epoch: 11, Train Loss: 0.19631668660095183, Val Loss: 0.2108532910545667\n",
      "Epoch: 12, Batch: 0, Loss: 0.1783805936574936\n",
      "Epoch: 12, Batch: 64, Loss: 0.18665596842765808\n",
      "Epoch: 12, Train Loss: 0.19474873275069868, Val Loss: 0.20956565886735917\n",
      "Epoch: 13, Batch: 0, Loss: 0.19195881485939026\n",
      "Epoch: 13, Batch: 64, Loss: 0.1892610639333725\n",
      "Epoch: 13, Train Loss: 0.19490713891336472, Val Loss: 0.21047984063625336\n",
      "Epoch: 14, Batch: 0, Loss: 0.18073873221874237\n",
      "Epoch: 14, Batch: 64, Loss: 0.17024391889572144\n",
      "Epoch: 14, Train Loss: 0.19429524116596933, Val Loss: 0.21218257000048954\n",
      "Epoch: 15, Batch: 0, Loss: 0.20077206194400787\n",
      "Epoch: 15, Batch: 64, Loss: 0.1905113011598587\n",
      "Epoch: 15, Train Loss: 0.1934880302366564, Val Loss: 0.21034200390179952\n",
      "Epoch: 16, Batch: 0, Loss: 0.19380410015583038\n",
      "Epoch: 16, Batch: 64, Loss: 0.20411504805088043\n",
      "Epoch: 16, Train Loss: 0.19255516412904708, Val Loss: 0.20988821585973103\n",
      "Epoch: 17, Batch: 0, Loss: 0.2166750431060791\n",
      "Epoch: 17, Batch: 64, Loss: 0.2135058492422104\n",
      "Epoch: 17, Train Loss: 0.1933080912646601, Val Loss: 0.20898163417975107\n",
      "Epoch: 18, Batch: 0, Loss: 0.18114353716373444\n",
      "Epoch: 18, Batch: 64, Loss: 0.15490834414958954\n",
      "Epoch: 18, Train Loss: 0.1914713142786996, Val Loss: 0.2120344360669454\n",
      "Epoch: 19, Batch: 0, Loss: 0.18529891967773438\n",
      "Epoch: 19, Batch: 64, Loss: 0.22403468191623688\n",
      "Epoch: 19, Train Loss: 0.19183118073111874, Val Loss: 0.2105697696407636\n",
      "Epoch: 20, Batch: 0, Loss: 0.18272997438907623\n",
      "Epoch: 20, Batch: 64, Loss: 0.2193397432565689\n",
      "Epoch: 20, Train Loss: 0.1904359305814161, Val Loss: 0.21104419281085332\n",
      "Epoch: 21, Batch: 0, Loss: 0.1781647801399231\n",
      "Epoch: 21, Batch: 64, Loss: 0.17725428938865662\n",
      "Epoch: 21, Train Loss: 0.19076451974905143, Val Loss: 0.21086106300354004\n",
      "Epoch: 22, Batch: 0, Loss: 0.19925233721733093\n",
      "Epoch: 22, Batch: 64, Loss: 0.19873565435409546\n",
      "Epoch: 22, Train Loss: 0.19012842062166183, Val Loss: 0.20995813955863316\n",
      "Epoch: 23, Batch: 0, Loss: 0.1679675430059433\n",
      "Epoch: 23, Batch: 64, Loss: 0.19548441469669342\n",
      "Epoch: 23, Train Loss: 0.18924969081151283, Val Loss: 0.21186726093292235\n",
      "Epoch: 24, Batch: 0, Loss: 0.1702485978603363\n",
      "Epoch: 24, Batch: 64, Loss: 0.182340607047081\n",
      "Epoch: 24, Train Loss: 0.18912823169918383, Val Loss: 0.21083034425973893\n",
      "Epoch: 25, Batch: 0, Loss: 0.1677417904138565\n",
      "Epoch: 25, Batch: 64, Loss: 0.17395687103271484\n",
      "Epoch: 25, Train Loss: 0.18825305051217645, Val Loss: 0.2124493882060051\n",
      "Epoch: 26, Batch: 0, Loss: 0.18029344081878662\n",
      "Epoch: 26, Batch: 64, Loss: 0.19003504514694214\n",
      "Epoch: 26, Train Loss: 0.18761218970609925, Val Loss: 0.21238458007574082\n",
      "Epoch: 27, Batch: 0, Loss: 0.1873481571674347\n",
      "Epoch: 27, Batch: 64, Loss: 0.22067001461982727\n",
      "Epoch: 27, Train Loss: 0.1869304223080813, Val Loss: 0.213644473751386\n",
      "Epoch: 28, Batch: 0, Loss: 0.17698895931243896\n",
      "Epoch: 28, Batch: 64, Loss: 0.1884748339653015\n",
      "Epoch: 28, Train Loss: 0.18687693219063645, Val Loss: 0.2118724857767423\n",
      "Epoch: 29, Batch: 0, Loss: 0.17972105741500854\n",
      "Epoch: 29, Batch: 64, Loss: 0.18476749956607819\n",
      "Epoch: 29, Train Loss: 0.18587616187030986, Val Loss: 0.21194528837998708\n",
      "Epoch: 30, Batch: 0, Loss: 0.17979809641838074\n",
      "Epoch: 30, Batch: 64, Loss: 0.18349352478981018\n",
      "Epoch: 30, Train Loss: 0.1863927083500361, Val Loss: 0.21276089996099473\n",
      "Epoch: 31, Batch: 0, Loss: 0.1558658629655838\n",
      "Epoch: 31, Batch: 64, Loss: 0.17486147582530975\n",
      "Epoch: 31, Train Loss: 0.18577155925459782, Val Loss: 0.2130474145213763\n",
      "Epoch: 32, Batch: 0, Loss: 0.18015429377555847\n",
      "Epoch: 32, Batch: 64, Loss: 0.18909494578838348\n",
      "Epoch: 32, Train Loss: 0.18545270319712365, Val Loss: 0.2125506470600764\n",
      "Epoch: 33, Batch: 0, Loss: 0.15762563049793243\n",
      "Epoch: 33, Batch: 64, Loss: 0.20847848057746887\n",
      "Epoch: 33, Train Loss: 0.18473387635865454, Val Loss: 0.21336061557133992\n",
      "Epoch: 34, Batch: 0, Loss: 0.2009628862142563\n",
      "Epoch: 34, Batch: 64, Loss: 0.16286443173885345\n",
      "Epoch: 34, Train Loss: 0.18467016012991888, Val Loss: 0.2142348249753316\n",
      "Epoch: 35, Batch: 0, Loss: 0.18981513381004333\n",
      "Epoch: 35, Batch: 64, Loss: 0.1748884916305542\n",
      "Epoch: 35, Train Loss: 0.18366102924791433, Val Loss: 0.21335462232430777\n",
      "Epoch: 36, Batch: 0, Loss: 0.18657121062278748\n",
      "Epoch: 36, Batch: 64, Loss: 0.15467368066310883\n",
      "Epoch: 36, Train Loss: 0.18282717428470063, Val Loss: 0.21331375688314438\n",
      "Epoch: 37, Batch: 0, Loss: 0.18032582104206085\n",
      "Epoch: 37, Batch: 64, Loss: 0.17562520503997803\n",
      "Epoch: 37, Train Loss: 0.18281844914969753, Val Loss: 0.21426546971003216\n",
      "Epoch: 38, Batch: 0, Loss: 0.16074228286743164\n",
      "Epoch: 38, Batch: 64, Loss: 0.18500357866287231\n",
      "Epoch: 38, Train Loss: 0.18219230980691264, Val Loss: 0.21542685528596242\n",
      "Epoch: 39, Batch: 0, Loss: 0.17386190593242645\n",
      "Epoch: 39, Batch: 64, Loss: 0.16762176156044006\n",
      "Epoch: 39, Train Loss: 0.18163060914661924, Val Loss: 0.21282082746426265\n",
      "Epoch: 40, Batch: 0, Loss: 0.17761674523353577\n",
      "Epoch: 40, Batch: 64, Loss: 0.20191630721092224\n",
      "Epoch: 40, Train Loss: 0.18046494788032466, Val Loss: 0.21361506084601084\n",
      "Epoch: 41, Batch: 0, Loss: 0.18441805243492126\n",
      "Epoch: 41, Batch: 64, Loss: 0.18876630067825317\n",
      "Epoch: 41, Train Loss: 0.18148028825299214, Val Loss: 0.21434773057699202\n",
      "Epoch: 42, Batch: 0, Loss: 0.18789756298065186\n",
      "Epoch: 42, Batch: 64, Loss: 0.17881503701210022\n",
      "Epoch: 42, Train Loss: 0.18070447356519054, Val Loss: 0.21324908137321472\n",
      "Epoch: 43, Batch: 0, Loss: 0.17897988855838776\n",
      "Epoch: 43, Batch: 64, Loss: 0.13560441136360168\n",
      "Epoch: 43, Train Loss: 0.17978815090353206, Val Loss: 0.21542506416638693\n",
      "Epoch: 44, Batch: 0, Loss: 0.19651466608047485\n",
      "Epoch: 44, Batch: 64, Loss: 0.17974251508712769\n",
      "Epoch: 44, Train Loss: 0.17924898711301512, Val Loss: 0.2155114844441414\n",
      "Epoch: 45, Batch: 0, Loss: 0.18334831297397614\n",
      "Epoch: 45, Batch: 64, Loss: 0.17253658175468445\n",
      "Epoch: 45, Train Loss: 0.1781730838751389, Val Loss: 0.21479828457037609\n",
      "Epoch: 46, Batch: 0, Loss: 0.18828493356704712\n",
      "Epoch: 46, Batch: 64, Loss: 0.19120775163173676\n",
      "Epoch: 46, Train Loss: 0.17808574665400942, Val Loss: 0.21561968276898066\n",
      "Epoch: 47, Batch: 0, Loss: 0.1848764419555664\n",
      "Epoch: 47, Batch: 64, Loss: 0.1983528584241867\n",
      "Epoch: 47, Train Loss: 0.17846393042196662, Val Loss: 0.2155461460351944\n",
      "Epoch: 48, Batch: 0, Loss: 0.1500849574804306\n",
      "Epoch: 48, Batch: 64, Loss: 0.16661305725574493\n",
      "Epoch: 48, Train Loss: 0.1772512347768929, Val Loss: 0.21594961484273276\n",
      "Epoch: 49, Batch: 0, Loss: 0.1914738267660141\n",
      "Epoch: 49, Batch: 64, Loss: 0.1735031008720398\n",
      "Epoch: 49, Train Loss: 0.17709550387778525, Val Loss: 0.21643003274997075\n",
      "Epoch: 50, Batch: 0, Loss: 0.18209540843963623\n",
      "Epoch: 50, Batch: 64, Loss: 0.1844673454761505\n",
      "Epoch: 50, Train Loss: 0.17627350556648383, Val Loss: 0.21715785761674244\n",
      "Epoch: 51, Batch: 0, Loss: 0.18926697969436646\n",
      "Epoch: 51, Batch: 64, Loss: 0.17462928593158722\n",
      "Epoch: 51, Train Loss: 0.17717214345426882, Val Loss: 0.21436600089073182\n",
      "Epoch: 52, Batch: 0, Loss: 0.1835530698299408\n",
      "Epoch: 52, Batch: 64, Loss: 0.20059065520763397\n",
      "Epoch: 52, Train Loss: 0.17636102741047488, Val Loss: 0.2158855155110359\n",
      "Epoch: 53, Batch: 0, Loss: 0.15771989524364471\n",
      "Epoch: 53, Batch: 64, Loss: 0.1807158887386322\n",
      "Epoch: 53, Train Loss: 0.17541711891101577, Val Loss: 0.21514844447374343\n",
      "Epoch: 54, Batch: 0, Loss: 0.1807767003774643\n",
      "Epoch: 54, Batch: 64, Loss: 0.17223480343818665\n",
      "Epoch: 54, Train Loss: 0.17446700016320763, Val Loss: 0.21801048864920933\n",
      "Epoch: 55, Batch: 0, Loss: 0.18931317329406738\n",
      "Epoch: 55, Batch: 64, Loss: 0.1804361790418625\n",
      "Epoch: 55, Train Loss: 0.17502595433744333, Val Loss: 0.21700933029254277\n",
      "Epoch: 56, Batch: 0, Loss: 0.16308578848838806\n",
      "Epoch: 56, Batch: 64, Loss: 0.1688784956932068\n",
      "Epoch: 56, Train Loss: 0.17440927344358573, Val Loss: 0.21564576427141827\n",
      "Epoch: 57, Batch: 0, Loss: 0.19271403551101685\n",
      "Epoch: 57, Batch: 64, Loss: 0.18467412889003754\n",
      "Epoch: 57, Train Loss: 0.17338432977765295, Val Loss: 0.2186207761367162\n",
      "Epoch: 58, Batch: 0, Loss: 0.15903623402118683\n",
      "Epoch: 58, Batch: 64, Loss: 0.17249424755573273\n",
      "Epoch: 58, Train Loss: 0.1732874627588159, Val Loss: 0.21650971521933873\n",
      "Epoch: 59, Batch: 0, Loss: 0.18478131294250488\n",
      "Epoch: 59, Batch: 64, Loss: 0.18009263277053833\n",
      "Epoch: 59, Train Loss: 0.1735703278396089, Val Loss: 0.21779741048812867\n",
      "Epoch: 60, Batch: 0, Loss: 0.15456214547157288\n",
      "Epoch: 60, Batch: 64, Loss: 0.16541530191898346\n",
      "Epoch: 60, Train Loss: 0.172214253095247, Val Loss: 0.21577271819114685\n",
      "Epoch: 61, Batch: 0, Loss: 0.1502722203731537\n",
      "Epoch: 61, Batch: 64, Loss: 0.1682296097278595\n",
      "Epoch: 61, Train Loss: 0.1720171678116766, Val Loss: 0.21773509234189986\n",
      "Epoch: 62, Batch: 0, Loss: 0.15034456551074982\n",
      "Epoch: 62, Batch: 64, Loss: 0.15052348375320435\n",
      "Epoch: 62, Train Loss: 0.17297451147588633, Val Loss: 0.21694113065799078\n",
      "Epoch: 63, Batch: 0, Loss: 0.15791495144367218\n",
      "Epoch: 63, Batch: 64, Loss: 0.1971438080072403\n",
      "Epoch: 63, Train Loss: 0.17172811938039326, Val Loss: 0.21764161984125774\n",
      "Epoch: 64, Batch: 0, Loss: 0.17386676371097565\n",
      "Epoch: 64, Batch: 64, Loss: 0.17428980767726898\n",
      "Epoch: 64, Train Loss: 0.17127494236170235, Val Loss: 0.21941228459278742\n",
      "Epoch: 65, Batch: 0, Loss: 0.1646176427602768\n",
      "Epoch: 65, Batch: 64, Loss: 0.15873312950134277\n",
      "Epoch: 65, Train Loss: 0.17040168178283563, Val Loss: 0.21737513244152068\n",
      "Epoch: 66, Batch: 0, Loss: 0.16000264883041382\n",
      "Epoch: 66, Batch: 64, Loss: 0.1908062994480133\n",
      "Epoch: 66, Train Loss: 0.16988858353283445, Val Loss: 0.2182792047659556\n",
      "Epoch: 67, Batch: 0, Loss: 0.1411040723323822\n",
      "Epoch: 67, Batch: 64, Loss: 0.13839726150035858\n",
      "Epoch: 67, Train Loss: 0.16980367191767287, Val Loss: 0.22056954950094224\n",
      "Epoch: 68, Batch: 0, Loss: 0.1641736775636673\n",
      "Epoch: 68, Batch: 64, Loss: 0.17057734727859497\n",
      "Epoch: 68, Train Loss: 0.16952440218400147, Val Loss: 0.21878020713726679\n",
      "Epoch: 69, Batch: 0, Loss: 0.15917693078517914\n",
      "Epoch: 69, Batch: 64, Loss: 0.17337368428707123\n",
      "Epoch: 69, Train Loss: 0.168606148054034, Val Loss: 0.22008204658826191\n",
      "Epoch: 70, Batch: 0, Loss: 0.17651276290416718\n",
      "Epoch: 70, Batch: 64, Loss: 0.172165647149086\n",
      "Epoch: 70, Train Loss: 0.16918302238997768, Val Loss: 0.2193041682243347\n",
      "Epoch: 71, Batch: 0, Loss: 0.17936773598194122\n",
      "Epoch: 71, Batch: 64, Loss: 0.19473741948604584\n",
      "Epoch: 71, Train Loss: 0.16779130318407284, Val Loss: 0.2187805692354838\n",
      "Epoch: 72, Batch: 0, Loss: 0.1696435511112213\n",
      "Epoch: 72, Batch: 64, Loss: 0.1815335601568222\n",
      "Epoch: 72, Train Loss: 0.1674912953275745, Val Loss: 0.21965079257885614\n",
      "Epoch: 73, Batch: 0, Loss: 0.1953280121088028\n",
      "Epoch: 73, Batch: 64, Loss: 0.1616211086511612\n",
      "Epoch: 73, Train Loss: 0.1673780055874485, Val Loss: 0.21961772094170254\n",
      "Epoch: 74, Batch: 0, Loss: 0.15392954647541046\n",
      "Epoch: 74, Batch: 64, Loss: 0.17325085401535034\n",
      "Epoch: 74, Train Loss: 0.16718972038667082, Val Loss: 0.22017575750748317\n",
      "Epoch: 75, Batch: 0, Loss: 0.17777405679225922\n",
      "Epoch: 75, Batch: 64, Loss: 0.15563927590847015\n",
      "Epoch: 75, Train Loss: 0.16717894766795433, Val Loss: 0.21898249983787538\n",
      "Epoch: 76, Batch: 0, Loss: 0.18614211678504944\n",
      "Epoch: 76, Batch: 64, Loss: 0.17236344516277313\n",
      "Epoch: 76, Train Loss: 0.1660558385616642, Val Loss: 0.22195399403572083\n",
      "Epoch: 77, Batch: 0, Loss: 0.15847767889499664\n",
      "Epoch: 77, Batch: 64, Loss: 0.18365360796451569\n",
      "Epoch: 77, Train Loss: 0.1661472331921933, Val Loss: 0.21911832491556804\n",
      "Epoch: 78, Batch: 0, Loss: 0.17658479511737823\n",
      "Epoch: 78, Batch: 64, Loss: 0.17089776694774628\n",
      "Epoch: 78, Train Loss: 0.16486417716842586, Val Loss: 0.21991862257321676\n",
      "Epoch: 79, Batch: 0, Loss: 0.16755370795726776\n",
      "Epoch: 79, Batch: 64, Loss: 0.15698839724063873\n",
      "Epoch: 79, Train Loss: 0.1648338240081981, Val Loss: 0.21990298082431156\n",
      "Epoch: 80, Batch: 0, Loss: 0.15125882625579834\n",
      "Epoch: 80, Batch: 64, Loss: 0.16307838261127472\n",
      "Epoch: 80, Train Loss: 0.16502397696850662, Val Loss: 0.2216288074851036\n",
      "Epoch: 81, Batch: 0, Loss: 0.17786918580532074\n",
      "Epoch: 81, Batch: 64, Loss: 0.1558992713689804\n",
      "Epoch: 81, Train Loss: 0.16460785010861137, Val Loss: 0.2220205972592036\n",
      "Epoch: 82, Batch: 0, Loss: 0.1645531803369522\n",
      "Epoch: 82, Batch: 64, Loss: 0.16814713180065155\n",
      "Epoch: 82, Train Loss: 0.16398548221184037, Val Loss: 0.22273640036582948\n",
      "Epoch: 83, Batch: 0, Loss: 0.16979368031024933\n",
      "Epoch: 83, Batch: 64, Loss: 0.17253080010414124\n",
      "Epoch: 83, Train Loss: 0.16413728349794776, Val Loss: 0.22126473436752955\n",
      "Epoch: 84, Batch: 0, Loss: 0.16089248657226562\n",
      "Epoch: 84, Batch: 64, Loss: 0.15291640162467957\n",
      "Epoch: 84, Train Loss: 0.16314246278193037, Val Loss: 0.22278104722499847\n",
      "Epoch: 85, Batch: 0, Loss: 0.15440984070301056\n",
      "Epoch: 85, Batch: 64, Loss: 0.18170644342899323\n",
      "Epoch: 85, Train Loss: 0.16286920036299754, Val Loss: 0.22059269944826762\n",
      "Epoch: 86, Batch: 0, Loss: 0.13545572757720947\n",
      "Epoch: 86, Batch: 64, Loss: 0.15568847954273224\n",
      "Epoch: 86, Train Loss: 0.162818571647345, Val Loss: 0.22090139190355937\n",
      "Epoch: 87, Batch: 0, Loss: 0.15217077732086182\n",
      "Epoch: 87, Batch: 64, Loss: 0.16790702939033508\n",
      "Epoch: 87, Train Loss: 0.16225939572362577, Val Loss: 0.22334128866593042\n",
      "Epoch: 88, Batch: 0, Loss: 0.1676141619682312\n",
      "Epoch: 88, Batch: 64, Loss: 0.16765843331813812\n",
      "Epoch: 88, Train Loss: 0.16258670781123435, Val Loss: 0.22390019992987314\n",
      "Epoch: 89, Batch: 0, Loss: 0.13408955931663513\n",
      "Epoch: 89, Batch: 64, Loss: 0.16163234412670135\n",
      "Epoch: 89, Train Loss: 0.1612958452837952, Val Loss: 0.22208871295054752\n",
      "Epoch: 90, Batch: 0, Loss: 0.14472050964832306\n",
      "Epoch: 90, Batch: 64, Loss: 0.16470842063426971\n",
      "Epoch: 90, Train Loss: 0.16167386595980596, Val Loss: 0.2240910400946935\n",
      "Epoch: 91, Batch: 0, Loss: 0.16956445574760437\n",
      "Epoch: 91, Batch: 64, Loss: 0.17806576192378998\n",
      "Epoch: 91, Train Loss: 0.16104947055800486, Val Loss: 0.2218987117211024\n",
      "Epoch: 92, Batch: 0, Loss: 0.15791697800159454\n",
      "Epoch: 92, Batch: 64, Loss: 0.15547800064086914\n",
      "Epoch: 92, Train Loss: 0.16011506445327048, Val Loss: 0.22323043445746105\n",
      "Epoch: 93, Batch: 0, Loss: 0.13914307951927185\n",
      "Epoch: 93, Batch: 64, Loss: 0.17587003111839294\n",
      "Epoch: 93, Train Loss: 0.1602278428815179, Val Loss: 0.2230849012732506\n",
      "Epoch: 94, Batch: 0, Loss: 0.16498181223869324\n",
      "Epoch: 94, Batch: 64, Loss: 0.1560513973236084\n",
      "Epoch: 94, Train Loss: 0.15976955640619084, Val Loss: 0.2245085303982099\n",
      "Epoch: 95, Batch: 0, Loss: 0.171642005443573\n",
      "Epoch: 95, Batch: 64, Loss: 0.1626584529876709\n",
      "Epoch: 95, Train Loss: 0.1598867411583157, Val Loss: 0.22207082708676656\n",
      "Epoch: 96, Batch: 0, Loss: 0.15481036901474\n",
      "Epoch: 96, Batch: 64, Loss: 0.15141691267490387\n",
      "Epoch: 96, Train Loss: 0.15909886177060967, Val Loss: 0.22336641152699788\n",
      "Epoch: 97, Batch: 0, Loss: 0.143654927611351\n",
      "Epoch: 97, Batch: 64, Loss: 0.15741421282291412\n",
      "Epoch: 97, Train Loss: 0.1588896309672776, Val Loss: 0.22435581733783086\n",
      "Epoch: 98, Batch: 0, Loss: 0.18126974999904633\n",
      "Epoch: 98, Batch: 64, Loss: 0.1467062532901764\n",
      "Epoch: 98, Train Loss: 0.1582969997899007, Val Loss: 0.2225047489007314\n",
      "Epoch: 99, Batch: 0, Loss: 0.13325157761573792\n",
      "Epoch: 99, Batch: 64, Loss: 0.14914080500602722\n",
      "Epoch: 99, Train Loss: 0.15806034725096266, Val Loss: 0.22603037307659785\n",
      "Epoch: 100, Batch: 0, Loss: 0.15986362099647522\n",
      "Epoch: 100, Batch: 64, Loss: 0.14682507514953613\n",
      "Epoch: 100, Train Loss: 0.157585299242351, Val Loss: 0.2237465461095174\n",
      "Epoch: 101, Batch: 0, Loss: 0.13336791098117828\n",
      "Epoch: 101, Batch: 64, Loss: 0.18422701954841614\n",
      "Epoch: 101, Train Loss: 0.15808859088663327, Val Loss: 0.22473074247439703\n",
      "Epoch: 102, Batch: 0, Loss: 0.14777760207653046\n",
      "Epoch: 102, Batch: 64, Loss: 0.1589571088552475\n",
      "Epoch: 102, Train Loss: 0.15714712043182325, Val Loss: 0.22489854097366332\n",
      "Epoch: 103, Batch: 0, Loss: 0.16177594661712646\n",
      "Epoch: 103, Batch: 64, Loss: 0.14419430494308472\n",
      "Epoch: 103, Train Loss: 0.15627051877268291, Val Loss: 0.2241297647356987\n",
      "Epoch: 104, Batch: 0, Loss: 0.14823324978351593\n",
      "Epoch: 104, Batch: 64, Loss: 0.1882619857788086\n",
      "Epoch: 104, Train Loss: 0.15575948547003632, Val Loss: 0.2246271957953771\n",
      "Epoch: 105, Batch: 0, Loss: 0.15660622715950012\n",
      "Epoch: 105, Batch: 64, Loss: 0.17933978140354156\n",
      "Epoch: 105, Train Loss: 0.15584919064984484, Val Loss: 0.225952909886837\n",
      "Epoch: 106, Batch: 0, Loss: 0.15480414032936096\n",
      "Epoch: 106, Batch: 64, Loss: 0.16994506120681763\n",
      "Epoch: 106, Train Loss: 0.15645692756367943, Val Loss: 0.22616748362779618\n",
      "Epoch: 107, Batch: 0, Loss: 0.1589663028717041\n",
      "Epoch: 107, Batch: 64, Loss: 0.16144800186157227\n",
      "Epoch: 107, Train Loss: 0.15555328553763487, Val Loss: 0.2255929594238599\n",
      "Epoch: 108, Batch: 0, Loss: 0.1622488647699356\n",
      "Epoch: 108, Batch: 64, Loss: 0.16411039233207703\n",
      "Epoch: 108, Train Loss: 0.15494093021093788, Val Loss: 0.22596527983744938\n",
      "Epoch: 109, Batch: 0, Loss: 0.18000033497810364\n",
      "Epoch: 109, Batch: 64, Loss: 0.15867607295513153\n",
      "Epoch: 109, Train Loss: 0.15477677150550534, Val Loss: 0.22508951524893442\n",
      "Epoch: 110, Batch: 0, Loss: 0.1707589477300644\n",
      "Epoch: 110, Batch: 64, Loss: 0.143818661570549\n",
      "Epoch: 110, Train Loss: 0.15434894571870061, Val Loss: 0.2264501949151357\n",
      "Epoch: 111, Batch: 0, Loss: 0.15634025633335114\n",
      "Epoch: 111, Batch: 64, Loss: 0.13813407719135284\n",
      "Epoch: 111, Train Loss: 0.15392486675310943, Val Loss: 0.22585317939519883\n",
      "Epoch: 112, Batch: 0, Loss: 0.15962450206279755\n",
      "Epoch: 112, Batch: 64, Loss: 0.1672268509864807\n",
      "Epoch: 112, Train Loss: 0.15394041182126028, Val Loss: 0.22668514251708985\n",
      "Epoch: 113, Batch: 0, Loss: 0.15068566799163818\n",
      "Epoch: 113, Batch: 64, Loss: 0.166053906083107\n",
      "Epoch: 113, Train Loss: 0.153012869216628, Val Loss: 0.22613561451435088\n",
      "Epoch: 114, Batch: 0, Loss: 0.1716078668832779\n",
      "Epoch: 114, Batch: 64, Loss: 0.13890287280082703\n",
      "Epoch: 114, Train Loss: 0.15303430731518794, Val Loss: 0.22660410801569622\n",
      "Epoch: 115, Batch: 0, Loss: 0.15054255723953247\n",
      "Epoch: 115, Batch: 64, Loss: 0.17750146985054016\n",
      "Epoch: 115, Train Loss: 0.1529027055001865, Val Loss: 0.22607165376345317\n",
      "Epoch: 116, Batch: 0, Loss: 0.13371361792087555\n",
      "Epoch: 116, Batch: 64, Loss: 0.14781342446804047\n",
      "Epoch: 116, Train Loss: 0.15273170943482448, Val Loss: 0.2299578125278155\n",
      "Epoch: 117, Batch: 0, Loss: 0.1682603359222412\n",
      "Epoch: 117, Batch: 64, Loss: 0.15250685811042786\n",
      "Epoch: 117, Train Loss: 0.15310625005829132, Val Loss: 0.22940522929032645\n",
      "Epoch: 118, Batch: 0, Loss: 0.14794592559337616\n",
      "Epoch: 118, Batch: 64, Loss: 0.18058471381664276\n",
      "Epoch: 118, Train Loss: 0.1521701324036566, Val Loss: 0.22896428306897482\n",
      "Epoch: 119, Batch: 0, Loss: 0.1564948409795761\n",
      "Epoch: 119, Batch: 64, Loss: 0.14732573926448822\n",
      "Epoch: 119, Train Loss: 0.15136634893083978, Val Loss: 0.22697336276372274\n",
      "Epoch: 120, Batch: 0, Loss: 0.14653423428535461\n",
      "Epoch: 120, Batch: 64, Loss: 0.1382560133934021\n",
      "Epoch: 120, Train Loss: 0.15122330889610922, Val Loss: 0.22868219017982483\n",
      "Epoch: 121, Batch: 0, Loss: 0.12948106229305267\n",
      "Epoch: 121, Batch: 64, Loss: 0.1486731618642807\n",
      "Epoch: 121, Train Loss: 0.1500522059909368, Val Loss: 0.22833499014377595\n",
      "Epoch: 122, Batch: 0, Loss: 0.15714257955551147\n",
      "Epoch: 122, Batch: 64, Loss: 0.1372566670179367\n",
      "Epoch: 122, Train Loss: 0.15103011666718175, Val Loss: 0.23102608273426692\n",
      "Epoch: 123, Batch: 0, Loss: 0.15835249423980713\n",
      "Epoch: 123, Batch: 64, Loss: 0.1461828351020813\n",
      "Epoch: 123, Train Loss: 0.14985627744157434, Val Loss: 0.22944605896870296\n",
      "Epoch: 124, Batch: 0, Loss: 0.14911063015460968\n",
      "Epoch: 124, Batch: 64, Loss: 0.1365569829940796\n",
      "Epoch: 124, Train Loss: 0.14959395146470958, Val Loss: 0.22834180891513825\n",
      "Epoch: 125, Batch: 0, Loss: 0.14326927065849304\n",
      "Epoch: 125, Batch: 64, Loss: 0.1384689211845398\n",
      "Epoch: 125, Train Loss: 0.1498955035487474, Val Loss: 0.23108695894479753\n",
      "Epoch: 126, Batch: 0, Loss: 0.16150109469890594\n",
      "Epoch: 126, Batch: 64, Loss: 0.14972059428691864\n",
      "Epoch: 126, Train Loss: 0.14918682465361335, Val Loss: 0.2287396381298701\n",
      "Epoch: 127, Batch: 0, Loss: 0.15923385322093964\n",
      "Epoch: 127, Batch: 64, Loss: 0.15722912549972534\n",
      "Epoch: 127, Train Loss: 0.14968624635268066, Val Loss: 0.22899830241998037\n",
      "Epoch: 128, Batch: 0, Loss: 0.142779141664505\n",
      "Epoch: 128, Batch: 64, Loss: 0.15265607833862305\n",
      "Epoch: 128, Train Loss: 0.14950626135124997, Val Loss: 0.23101655542850494\n",
      "Epoch: 129, Batch: 0, Loss: 0.14419028162956238\n",
      "Epoch: 129, Batch: 64, Loss: 0.14316323399543762\n",
      "Epoch: 129, Train Loss: 0.14894351072735706, Val Loss: 0.23068401118119558\n",
      "Epoch: 130, Batch: 0, Loss: 0.16655486822128296\n",
      "Epoch: 130, Batch: 64, Loss: 0.149422749876976\n",
      "Epoch: 130, Train Loss: 0.1477690489362862, Val Loss: 0.2311863695581754\n",
      "Epoch: 131, Batch: 0, Loss: 0.14091409742832184\n",
      "Epoch: 131, Batch: 64, Loss: 0.13354897499084473\n",
      "Epoch: 131, Train Loss: 0.14812604116938882, Val Loss: 0.2288323496778806\n",
      "Epoch: 132, Batch: 0, Loss: 0.14309868216514587\n",
      "Epoch: 132, Batch: 64, Loss: 0.1402011662721634\n",
      "Epoch: 132, Train Loss: 0.1474769167223219, Val Loss: 0.22834771821896235\n",
      "Epoch: 133, Batch: 0, Loss: 0.13514545559883118\n",
      "Epoch: 133, Batch: 64, Loss: 0.19023030996322632\n",
      "Epoch: 133, Train Loss: 0.14714559129739213, Val Loss: 0.23010092228651047\n",
      "Epoch: 134, Batch: 0, Loss: 0.15908624231815338\n",
      "Epoch: 134, Batch: 64, Loss: 0.1516631841659546\n",
      "Epoch: 134, Train Loss: 0.14704081921254175, Val Loss: 0.2297218898932139\n",
      "Epoch: 135, Batch: 0, Loss: 0.15543916821479797\n",
      "Epoch: 135, Batch: 64, Loss: 0.15304981172084808\n",
      "Epoch: 135, Train Loss: 0.14667239980929989, Val Loss: 0.22990184525648752\n",
      "Epoch: 136, Batch: 0, Loss: 0.15461459755897522\n",
      "Epoch: 136, Batch: 64, Loss: 0.13999642431735992\n",
      "Epoch: 136, Train Loss: 0.1469762174381038, Val Loss: 0.23159586389859518\n",
      "Epoch: 137, Batch: 0, Loss: 0.1569228172302246\n",
      "Epoch: 137, Batch: 64, Loss: 0.1377439945936203\n",
      "Epoch: 137, Train Loss: 0.1460008423586013, Val Loss: 0.23090691367785135\n",
      "Epoch: 138, Batch: 0, Loss: 0.1677994430065155\n",
      "Epoch: 138, Batch: 64, Loss: 0.16227670013904572\n",
      "Epoch: 138, Train Loss: 0.14543531854021347, Val Loss: 0.23195657084385554\n",
      "Epoch: 139, Batch: 0, Loss: 0.12964951992034912\n",
      "Epoch: 139, Batch: 64, Loss: 0.16067680716514587\n",
      "Epoch: 139, Train Loss: 0.14529364840206455, Val Loss: 0.23142994791269303\n",
      "Epoch: 140, Batch: 0, Loss: 0.13070867955684662\n",
      "Epoch: 140, Batch: 64, Loss: 0.13217656314373016\n",
      "Epoch: 140, Train Loss: 0.14517888597260087, Val Loss: 0.232914736866951\n",
      "Epoch: 141, Batch: 0, Loss: 0.13271081447601318\n",
      "Epoch: 141, Batch: 64, Loss: 0.14130878448486328\n",
      "Epoch: 141, Train Loss: 0.14471830553927664, Val Loss: 0.23314536263545355\n",
      "Epoch: 142, Batch: 0, Loss: 0.13520893454551697\n",
      "Epoch: 142, Batch: 64, Loss: 0.17187470197677612\n",
      "Epoch: 142, Train Loss: 0.14479485280432944, Val Loss: 0.23182068864504496\n",
      "Epoch: 143, Batch: 0, Loss: 0.1484304964542389\n",
      "Epoch: 143, Batch: 64, Loss: 0.12769246101379395\n",
      "Epoch: 143, Train Loss: 0.14437813943220398, Val Loss: 0.23334993968407314\n",
      "Epoch: 144, Batch: 0, Loss: 0.13632088899612427\n",
      "Epoch: 144, Batch: 64, Loss: 0.14184093475341797\n",
      "Epoch: 144, Train Loss: 0.14380708823012092, Val Loss: 0.23332750499248506\n",
      "Epoch: 145, Batch: 0, Loss: 0.13250042498111725\n",
      "Epoch: 145, Batch: 64, Loss: 0.13937333226203918\n",
      "Epoch: 145, Train Loss: 0.14345091019394035, Val Loss: 0.2315183937549591\n",
      "Epoch: 146, Batch: 0, Loss: 0.14599113166332245\n",
      "Epoch: 146, Batch: 64, Loss: 0.1297868937253952\n",
      "Epoch: 146, Train Loss: 0.14309663982209514, Val Loss: 0.23295974483092627\n",
      "Epoch: 147, Batch: 0, Loss: 0.14588920772075653\n",
      "Epoch: 147, Batch: 64, Loss: 0.16085389256477356\n",
      "Epoch: 147, Train Loss: 0.1437920453184742, Val Loss: 0.23275666187206903\n",
      "Epoch: 148, Batch: 0, Loss: 0.14543910324573517\n",
      "Epoch: 148, Batch: 64, Loss: 0.15688416361808777\n",
      "Epoch: 148, Train Loss: 0.14303812233068175, Val Loss: 0.2328319013118744\n",
      "Epoch: 149, Batch: 0, Loss: 0.13038691878318787\n",
      "Epoch: 149, Batch: 64, Loss: 0.1492127627134323\n",
      "Epoch: 149, Train Loss: 0.1428262504859496, Val Loss: 0.23338472843170166\n",
      "Epoch: 150, Batch: 0, Loss: 0.13213792443275452\n",
      "Epoch: 150, Batch: 64, Loss: 0.1403207629919052\n",
      "Epoch: 150, Train Loss: 0.14238368852411287, Val Loss: 0.23245446036259335\n",
      "Epoch: 151, Batch: 0, Loss: 0.1490304321050644\n",
      "Epoch: 151, Batch: 64, Loss: 0.1382829248905182\n",
      "Epoch: 151, Train Loss: 0.14186006449794364, Val Loss: 0.2333199883500735\n",
      "Epoch: 152, Batch: 0, Loss: 0.16633211076259613\n",
      "Epoch: 152, Batch: 64, Loss: 0.1512652039527893\n",
      "Epoch: 152, Train Loss: 0.1418195431894165, Val Loss: 0.23374223560094834\n",
      "Epoch: 153, Batch: 0, Loss: 0.12955936789512634\n",
      "Epoch: 153, Batch: 64, Loss: 0.13293005526065826\n",
      "Epoch: 153, Train Loss: 0.14143923872103126, Val Loss: 0.23291594833135604\n",
      "Epoch: 154, Batch: 0, Loss: 0.13768942654132843\n",
      "Epoch: 154, Batch: 64, Loss: 0.1252368837594986\n",
      "Epoch: 154, Train Loss: 0.1410342686635963, Val Loss: 0.23391332377990087\n",
      "Epoch: 155, Batch: 0, Loss: 0.1469884216785431\n",
      "Epoch: 155, Batch: 64, Loss: 0.12824766337871552\n",
      "Epoch: 155, Train Loss: 0.14067993723487449, Val Loss: 0.23361641665299734\n",
      "Epoch: 156, Batch: 0, Loss: 0.1471056193113327\n",
      "Epoch: 156, Batch: 64, Loss: 0.14716729521751404\n",
      "Epoch: 156, Train Loss: 0.14051784316109397, Val Loss: 0.23396397431691487\n",
      "Epoch: 157, Batch: 0, Loss: 0.14703284204006195\n",
      "Epoch: 157, Batch: 64, Loss: 0.14837181568145752\n",
      "Epoch: 157, Train Loss: 0.1400471555100659, Val Loss: 0.23127167870601018\n",
      "Epoch: 158, Batch: 0, Loss: 0.15743395686149597\n",
      "Epoch: 158, Batch: 64, Loss: 0.14027491211891174\n",
      "Epoch: 158, Train Loss: 0.13933197592022056, Val Loss: 0.2342278887828191\n",
      "Epoch: 159, Batch: 0, Loss: 0.1288328766822815\n",
      "Epoch: 159, Batch: 64, Loss: 0.13842171430587769\n",
      "Epoch: 159, Train Loss: 0.13997373338473046, Val Loss: 0.23430753052234649\n",
      "Epoch: 160, Batch: 0, Loss: 0.1379031240940094\n",
      "Epoch: 160, Batch: 64, Loss: 0.14953653514385223\n",
      "Epoch: 160, Train Loss: 0.1395895563823692, Val Loss: 0.23437106360991797\n",
      "Epoch: 161, Batch: 0, Loss: 0.1158687025308609\n",
      "Epoch: 161, Batch: 64, Loss: 0.14292582869529724\n",
      "Epoch: 161, Train Loss: 0.1392428415306544, Val Loss: 0.23561114420493443\n",
      "Epoch: 162, Batch: 0, Loss: 0.14410114288330078\n",
      "Epoch: 162, Batch: 64, Loss: 0.1298348754644394\n",
      "Epoch: 162, Train Loss: 0.13987725474319215, Val Loss: 0.2369904895623525\n",
      "Epoch: 163, Batch: 0, Loss: 0.1210382804274559\n",
      "Epoch: 163, Batch: 64, Loss: 0.14191964268684387\n",
      "Epoch: 163, Train Loss: 0.138586625583091, Val Loss: 0.23409532407919567\n",
      "Epoch: 164, Batch: 0, Loss: 0.14204762876033783\n",
      "Epoch: 164, Batch: 64, Loss: 0.13812431693077087\n",
      "Epoch: 164, Train Loss: 0.13779266841583332, Val Loss: 0.2356418659289678\n",
      "Epoch: 165, Batch: 0, Loss: 0.13230423629283905\n",
      "Epoch: 165, Batch: 64, Loss: 0.1437491476535797\n",
      "Epoch: 165, Train Loss: 0.13878401103666274, Val Loss: 0.23409491628408433\n",
      "Epoch: 166, Batch: 0, Loss: 0.1448914110660553\n",
      "Epoch: 166, Batch: 64, Loss: 0.13505075871944427\n",
      "Epoch: 166, Train Loss: 0.13762563586992732, Val Loss: 0.23612492978572847\n",
      "Epoch: 167, Batch: 0, Loss: 0.13764597475528717\n",
      "Epoch: 167, Batch: 64, Loss: 0.14197441935539246\n",
      "Epoch: 167, Train Loss: 0.13785707590691113, Val Loss: 0.23592754652102788\n",
      "Epoch: 168, Batch: 0, Loss: 0.13154441118240356\n",
      "Epoch: 168, Batch: 64, Loss: 0.14761671423912048\n",
      "Epoch: 168, Train Loss: 0.13824324859148365, Val Loss: 0.23731790582338969\n",
      "Epoch: 169, Batch: 0, Loss: 0.12969055771827698\n",
      "Epoch: 169, Batch: 64, Loss: 0.1554492563009262\n",
      "Epoch: 169, Train Loss: 0.13797861760703184, Val Loss: 0.2350840618213018\n",
      "Epoch: 170, Batch: 0, Loss: 0.14813588559627533\n",
      "Epoch: 170, Batch: 64, Loss: 0.12402509897947311\n",
      "Epoch: 170, Train Loss: 0.13743853480634044, Val Loss: 0.2389768987894058\n",
      "Epoch: 171, Batch: 0, Loss: 0.13760480284690857\n",
      "Epoch: 171, Batch: 64, Loss: 0.14857050776481628\n",
      "Epoch: 171, Train Loss: 0.13677389774534662, Val Loss: 0.23849759449561436\n",
      "Epoch: 172, Batch: 0, Loss: 0.12453773617744446\n",
      "Epoch: 172, Batch: 64, Loss: 0.15949712693691254\n",
      "Epoch: 172, Train Loss: 0.13656666489728428, Val Loss: 0.23526752193768818\n",
      "Epoch: 173, Batch: 0, Loss: 0.1315356343984604\n",
      "Epoch: 173, Batch: 64, Loss: 0.13195565342903137\n",
      "Epoch: 173, Train Loss: 0.13624670230231042, Val Loss: 0.2353649467229843\n",
      "Epoch: 174, Batch: 0, Loss: 0.1457185596227646\n",
      "Epoch: 174, Batch: 64, Loss: 0.12570008635520935\n",
      "Epoch: 174, Train Loss: 0.136149749538656, Val Loss: 0.23696825454632442\n",
      "Epoch: 175, Batch: 0, Loss: 0.13995061814785004\n",
      "Epoch: 175, Batch: 64, Loss: 0.17114724218845367\n",
      "Epoch: 175, Train Loss: 0.13612413905182127, Val Loss: 0.23874538739522297\n",
      "Epoch: 176, Batch: 0, Loss: 0.13046589493751526\n",
      "Epoch: 176, Batch: 64, Loss: 0.1250365823507309\n",
      "Epoch: 176, Train Loss: 0.13539217425099873, Val Loss: 0.23719030867020288\n",
      "Epoch: 177, Batch: 0, Loss: 0.12283939868211746\n",
      "Epoch: 177, Batch: 64, Loss: 0.11765831708908081\n",
      "Epoch: 177, Train Loss: 0.13596901348081686, Val Loss: 0.2373116617401441\n",
      "Epoch: 178, Batch: 0, Loss: 0.14195242524147034\n",
      "Epoch: 178, Batch: 64, Loss: 0.12035971879959106\n",
      "Epoch: 178, Train Loss: 0.1351333116575823, Val Loss: 0.23775121221939724\n",
      "Epoch: 179, Batch: 0, Loss: 0.13656701147556305\n",
      "Epoch: 179, Batch: 64, Loss: 0.1234573945403099\n",
      "Epoch: 179, Train Loss: 0.13490682594099287, Val Loss: 0.2379722038904826\n",
      "Epoch: 180, Batch: 0, Loss: 0.12339922785758972\n",
      "Epoch: 180, Batch: 64, Loss: 0.1303299367427826\n",
      "Epoch: 180, Train Loss: 0.13361518242854184, Val Loss: 0.23666707624991734\n",
      "Epoch: 181, Batch: 0, Loss: 0.1219821572303772\n",
      "Epoch: 181, Batch: 64, Loss: 0.1515546590089798\n",
      "Epoch: 181, Train Loss: 0.13350506764599832, Val Loss: 0.24048398435115814\n",
      "Epoch: 182, Batch: 0, Loss: 0.16823655366897583\n",
      "Epoch: 182, Batch: 64, Loss: 0.13502469658851624\n",
      "Epoch: 182, Train Loss: 0.1332997641194675, Val Loss: 0.23685969064633053\n",
      "Epoch: 183, Batch: 0, Loss: 0.13890869915485382\n",
      "Epoch: 183, Batch: 64, Loss: 0.13343016803264618\n",
      "Epoch: 183, Train Loss: 0.1336546287698261, Val Loss: 0.2411333441734314\n",
      "Epoch: 184, Batch: 0, Loss: 0.13967593014240265\n",
      "Epoch: 184, Batch: 64, Loss: 0.12423636019229889\n",
      "Epoch: 184, Train Loss: 0.13358571083616402, Val Loss: 0.23852507323026656\n",
      "Epoch: 185, Batch: 0, Loss: 0.12372487783432007\n",
      "Epoch: 185, Batch: 64, Loss: 0.14409546554088593\n",
      "Epoch: 185, Train Loss: 0.13371363477939266, Val Loss: 0.24039867719014485\n",
      "Epoch: 186, Batch: 0, Loss: 0.14135192334651947\n",
      "Epoch: 186, Batch: 64, Loss: 0.13745392858982086\n",
      "Epoch: 186, Train Loss: 0.13298473437711344, Val Loss: 0.24036477307478588\n",
      "Epoch: 187, Batch: 0, Loss: 0.11730509251356125\n",
      "Epoch: 187, Batch: 64, Loss: 0.1298057585954666\n",
      "Epoch: 187, Train Loss: 0.13239274887462793, Val Loss: 0.23895507603883742\n",
      "Epoch: 188, Batch: 0, Loss: 0.13392683863639832\n",
      "Epoch: 188, Batch: 64, Loss: 0.11337276548147202\n",
      "Epoch: 188, Train Loss: 0.13233244823197188, Val Loss: 0.24080248375733693\n",
      "Epoch: 189, Batch: 0, Loss: 0.12884613871574402\n",
      "Epoch: 189, Batch: 64, Loss: 0.14104633033275604\n",
      "Epoch: 189, Train Loss: 0.13255583545414068, Val Loss: 0.23811908960342407\n",
      "Epoch: 190, Batch: 0, Loss: 0.1273009330034256\n",
      "Epoch: 190, Batch: 64, Loss: 0.1336163580417633\n",
      "Epoch: 190, Train Loss: 0.13194398828229661, Val Loss: 0.23995192497968673\n",
      "Epoch: 191, Batch: 0, Loss: 0.14113682508468628\n",
      "Epoch: 191, Batch: 64, Loss: 0.1491326540708542\n",
      "Epoch: 191, Train Loss: 0.1321098185065439, Val Loss: 0.23948689202467602\n",
      "Epoch: 192, Batch: 0, Loss: 0.13393186032772064\n",
      "Epoch: 192, Batch: 64, Loss: 0.1434333175420761\n",
      "Epoch: 192, Train Loss: 0.1320581783430051, Val Loss: 0.24029927253723143\n",
      "Epoch: 193, Batch: 0, Loss: 0.15356560051441193\n",
      "Epoch: 193, Batch: 64, Loss: 0.13520361483097076\n",
      "Epoch: 193, Train Loss: 0.13138897060337712, Val Loss: 0.24167442520459492\n",
      "Epoch: 194, Batch: 0, Loss: 0.13415823876857758\n",
      "Epoch: 194, Batch: 64, Loss: 0.1291787326335907\n",
      "Epoch: 194, Train Loss: 0.13082805522165056, Val Loss: 0.24158046940962474\n",
      "Epoch: 195, Batch: 0, Loss: 0.14106397330760956\n",
      "Epoch: 195, Batch: 64, Loss: 0.1231827363371849\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss,val_loss \u001b[39m=\u001b[39m train(model,train_loader,val_loader,epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,lr\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[0;32m     13\u001b[0m train_loss_ \u001b[39m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m val_loss_ \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfor\u001b[39;00m i,images \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     16\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m     images[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mimages[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mBirdDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m,idx):\n\u001b[0;32m     14\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[idx]\n\u001b[1;32m---> 15\u001b[0m     image \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49mimread(image)\n\u001b[0;32m     16\u001b[0m     \u001b[39m#print(image.shape)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(image\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py:2131\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   2129\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimread)\n\u001b[0;32m   2130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(fname, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 2131\u001b[0m     \u001b[39mreturn\u001b[39;00m matplotlib\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimread(fname, \u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\image.py:1544\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1536\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1537\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1538\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1539\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1540\u001b[0m         )\n\u001b[0;32m   1541\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[0;32m   1542\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   1543\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[1;32m-> 1544\u001b[0m             pil_to_array(image))\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\image.py:1688\u001b[0m, in \u001b[0;36mpil_to_array\u001b[1;34m(pilImage)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[39mLoad a `PIL image`_ and return it as a numpy int array.\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1684\u001b[0m \u001b[39m    - (M, N, 4) for RGBA images.\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[39mif\u001b[39;00m pilImage\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRGBX\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m   1687\u001b[0m     \u001b[39m# return MxNx4 RGBA, MxNx3 RBA, or MxN luminance array\u001b[39;00m\n\u001b[1;32m-> 1688\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(pilImage)\n\u001b[0;32m   1689\u001b[0m \u001b[39melif\u001b[39;00m pilImage\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1690\u001b[0m     \u001b[39m# return MxN luminance array of uint16\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m     raw \u001b[39m=\u001b[39m pilImage\u001b[39m.\u001b[39mtobytes(\u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m, pilImage\u001b[39m.\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 701\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[0;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (\u001b[39mMemoryError\u001b[39;00m, \u001b[39mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:758\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m encoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[0;32m    756\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[1;32m--> 758\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    760\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    761\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss,val_loss = train(model,train_loader,val_loader,epochs=500,lr=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
