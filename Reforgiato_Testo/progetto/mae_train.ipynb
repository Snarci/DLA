{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.Black_footed_Albatross', '002.Laysan_Albatross', '003.Sooty_Albatross', '004.Groove_billed_Ani', '005.Crested_Auklet', '006.Least_Auklet', '007.Parakeet_Auklet', '008.Rhinoceros_Auklet', '009.Brewer_Blackbird', '010.Red_winged_Blackbird', '011.Rusty_Blackbird', '012.Yellow_headed_Blackbird', '013.Bobolink', '014.Indigo_Bunting', '015.Lazuli_Bunting', '016.Painted_Bunting', '017.Cardinal', '018.Spotted_Catbird', '019.Gray_Catbird', '020.Yellow_breasted_Chat', '021.Eastern_Towhee', '022.Chuck_will_Widow', '023.Brandt_Cormorant', '024.Red_faced_Cormorant', '025.Pelagic_Cormorant', '026.Bronzed_Cowbird', '027.Shiny_Cowbird', '028.Brown_Creeper', '029.American_Crow', '030.Fish_Crow', '031.Black_billed_Cuckoo', '032.Mangrove_Cuckoo', '033.Yellow_billed_Cuckoo', '034.Gray_crowned_Rosy_Finch', '035.Purple_Finch', '036.Northern_Flicker', '037.Acadian_Flycatcher', '038.Great_Crested_Flycatcher', '039.Least_Flycatcher', '040.Olive_sided_Flycatcher', '041.Scissor_tailed_Flycatcher', '042.Vermilion_Flycatcher', '043.Yellow_bellied_Flycatcher', '044.Frigatebird', '045.Northern_Fulmar', '046.Gadwall', '047.American_Goldfinch', '048.European_Goldfinch', '049.Boat_tailed_Grackle', '050.Eared_Grebe', '051.Horned_Grebe', '052.Pied_billed_Grebe', '053.Western_Grebe', '054.Blue_Grosbeak', '055.Evening_Grosbeak', '056.Pine_Grosbeak', '057.Rose_breasted_Grosbeak', '058.Pigeon_Guillemot', '059.California_Gull', '060.Glaucous_winged_Gull', '061.Heermann_Gull', '062.Herring_Gull', '063.Ivory_Gull', '064.Ring_billed_Gull', '065.Slaty_backed_Gull', '066.Western_Gull', '067.Anna_Hummingbird', '068.Ruby_throated_Hummingbird', '069.Rufous_Hummingbird', '070.Green_Violetear', '071.Long_tailed_Jaeger', '072.Pomarine_Jaeger', '073.Blue_Jay', '074.Florida_Jay', '075.Green_Jay', '076.Dark_eyed_Junco', '077.Tropical_Kingbird', '078.Gray_Kingbird', '079.Belted_Kingfisher', '080.Green_Kingfisher', '081.Pied_Kingfisher', '082.Ringed_Kingfisher', '083.White_breasted_Kingfisher', '084.Red_legged_Kittiwake', '085.Horned_Lark', '086.Pacific_Loon', '087.Mallard', '088.Western_Meadowlark', '089.Hooded_Merganser', '090.Red_breasted_Merganser', '091.Mockingbird', '092.Nighthawk', '093.Clark_Nutcracker', '094.White_breasted_Nuthatch', '095.Baltimore_Oriole', '096.Hooded_Oriole', '097.Orchard_Oriole', '098.Scott_Oriole', '099.Ovenbird', '100.Brown_Pelican', '101.White_Pelican', '102.Western_Wood_Pewee', '103.Sayornis', '104.American_Pipit', '105.Whip_poor_Will', '106.Horned_Puffin', '107.Common_Raven', '108.White_necked_Raven', '109.American_Redstart', '110.Geococcyx', '111.Loggerhead_Shrike', '112.Great_Grey_Shrike', '113.Baird_Sparrow', '114.Black_throated_Sparrow', '115.Brewer_Sparrow', '116.Chipping_Sparrow', '117.Clay_colored_Sparrow', '118.House_Sparrow', '119.Field_Sparrow', '120.Fox_Sparrow', '121.Grasshopper_Sparrow', '122.Harris_Sparrow', '123.Henslow_Sparrow', '124.Le_Conte_Sparrow', '125.Lincoln_Sparrow', '126.Nelson_Sharp_tailed_Sparrow', '127.Savannah_Sparrow', '128.Seaside_Sparrow', '129.Song_Sparrow', '130.Tree_Sparrow', '131.Vesper_Sparrow', '132.White_crowned_Sparrow', '133.White_throated_Sparrow', '134.Cape_Glossy_Starling', '135.Bank_Swallow', '136.Barn_Swallow', '137.Cliff_Swallow', '138.Tree_Swallow', '139.Scarlet_Tanager', '140.Summer_Tanager', '141.Artic_Tern', '142.Black_Tern', '143.Caspian_Tern', '144.Common_Tern', '145.Elegant_Tern', '146.Forsters_Tern', '147.Least_Tern', '148.Green_tailed_Towhee', '149.Brown_Thrasher', '150.Sage_Thrasher', '151.Black_capped_Vireo', '152.Blue_headed_Vireo', '153.Philadelphia_Vireo', '154.Red_eyed_Vireo', '155.Warbling_Vireo', '156.White_eyed_Vireo', '157.Yellow_throated_Vireo', '158.Bay_breasted_Warbler', '159.Black_and_white_Warbler', '160.Black_throated_Blue_Warbler', '161.Blue_winged_Warbler', '162.Canada_Warbler', '163.Cape_May_Warbler', '164.Cerulean_Warbler', '165.Chestnut_sided_Warbler', '166.Golden_winged_Warbler', '167.Hooded_Warbler', '168.Kentucky_Warbler', '169.Magnolia_Warbler', '170.Mourning_Warbler', '171.Myrtle_Warbler', '172.Nashville_Warbler', '173.Orange_crowned_Warbler', '174.Palm_Warbler', '175.Pine_Warbler', '176.Prairie_Warbler', '177.Prothonotary_Warbler', '178.Swainson_Warbler', '179.Tennessee_Warbler', '180.Wilson_Warbler', '181.Worm_eating_Warbler', '182.Yellow_Warbler', '183.Northern_Waterthrush', '184.Louisiana_Waterthrush', '185.Bohemian_Waxwing', '186.Cedar_Waxwing', '187.American_Three_toed_Woodpecker', '188.Pileated_Woodpecker', '189.Red_bellied_Woodpecker', '190.Red_cockaded_Woodpecker', '191.Red_headed_Woodpecker', '192.Downy_Woodpecker', '193.Bewick_Wren', '194.Cactus_Wren', '195.Carolina_Wren', '196.House_Wren', '197.Marsh_Wren', '198.Rock_Wren', '199.Winter_Wren', '200.Common_Yellowthroat']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        class  \\\n",
       "0  001.Black_footed_Albatross   \n",
       "1  001.Black_footed_Albatross   \n",
       "2  001.Black_footed_Albatross   \n",
       "3  001.Black_footed_Albatross   \n",
       "4  001.Black_footed_Albatross   \n",
       "\n",
       "                                                path  \n",
       "0  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "1  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "2  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "3  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "4  ./CUB_200_2011/images/001.Black_footed_Albatro...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all files from the folder CUB_200_2011 and assign the subfolder as a class\n",
    "#the subfolder name is the class name\n",
    "\n",
    "path = './CUB_200_2011/images/'\n",
    "classes = os.listdir(path)\n",
    "classes.sort()\n",
    "print(classes)\n",
    "#read all files from the subfolders\n",
    "\n",
    "data = []\n",
    "for i in range(len(classes)):\n",
    "    folder = os.path.join(path,classes[i])\n",
    "    files = os.listdir(folder)\n",
    "    for j in range(len(files)):\n",
    "        data.append([classes[i],os.path.join(folder,files[j])])\n",
    "\n",
    "#convert the list to a dataframe\n",
    "df = pd.DataFrame(data,columns=['class','path'])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumenations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#transformations for train and validation\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 512)\n",
    "    ,ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the Dataset class\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.images = self.df['path'].values\n",
    "        self.classes = self.df['class'].values\n",
    "        self.classes = np.array([classes.index(i) for i in self.classes])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = self.images[idx]\n",
    "        image = plt.imread(image)\n",
    "        #print(image.shape)\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "        image = transform(image=image)['image']\n",
    "        if image.shape[1] == 1:\n",
    "            image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "        image=image_processor(images=image, return_tensors=\"pt\")\n",
    "        #print(image['pixel_values'].shape)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(BirdDataset(train_df),batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(BirdDataset(val_df),batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(BirdDataset(test_df),batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#try the dataloader\n",
    "for index,i in enumerate(train_loader):\n",
    "    #print pixel values and class\n",
    "    #print(i.shape)\n",
    "    #image=image_processor(images=i, return_tensors=\"pt\")\n",
    "    #reshape the pixel values to nx3x224x224\n",
    "    i['pixel_values']=i['pixel_values'].to(torch.float32)\n",
    "    s = i['pixel_values'].shape\n",
    "    i['pixel_values']=i['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "    print(i['pixel_values'].shape)\n",
    "    #if index == 1000:\n",
    "    #    break\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTMAEForPreTraining(\n",
       "  (vit): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-large\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "from torchvision.utils import save_image\n",
    "def train(model,train_loader,val_loader,epochs=10,lr=1e-2):\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_ = []\n",
    "        val_loss_ = []\n",
    "        for i,images in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "            s = images['pixel_values'].shape\n",
    "            images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "            optimizer.zero_grad()\n",
    "            #print(images['pixel_values'].shape)\n",
    "            output = model(**images)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_.append(loss.item())\n",
    "            #every 100 batches, print the loss\n",
    "            if i%64 == 0:\n",
    "\n",
    "                print(f'Epoch: {epoch+1}, Batch: {i}, Loss: {train_loss_[-1]}')\n",
    "\n",
    "                #reconstruct the original image shape\n",
    "                masks = output.mask\n",
    "                #print(masks.shape)\n",
    "                #porzione immagine\n",
    "                img = output.logits\n",
    "                img = img.reshape(\n",
    "                batch_size, 14, 14, 16, 16, 3\n",
    "                )\n",
    "                #batch\n",
    "                img = torch.einsum(\"nhwpqc->nchpwq\", img)\n",
    "                img = img.reshape(batch_size, 3, 224, 224)\n",
    "                #first img of the batch\n",
    "                img = img[0]\n",
    "                img = img.view(3,224,224)\n",
    "                img_recon = img.cpu().detach().numpy().transpose(1,2,0)\n",
    "                #img = np.maximum(img,0)\n",
    "                img_recon = (img_recon-img_recon.min())/(img_recon.max()-img_recon.min())#np.minimum(img,1)\n",
    "                #print(img.shape)\n",
    "                \n",
    "                img_originale = images['pixel_values'][0].cpu()\n",
    "                #bring image to range 0-1\n",
    "                img_originale = (img_originale-img_originale.min())/(img_originale.max()-img_originale.min())\n",
    "                #print max and min values\n",
    "                #print(img_originale.max(),img_originale.min())\n",
    "                img_originale = img_originale.numpy().transpose(1,2,0)\n",
    "                #cat immagine originale e immagine con maschera\n",
    "                #porzione maschera sopra immagine originale\n",
    "                mask = masks[0]\n",
    "                mask_new = torch.ones(196,768).to(device)\n",
    "                for j in range(196):\n",
    "                    mask_new[j,:] =( 1-mask[j])*mask_new[j,:]\n",
    "                img = mask_new\n",
    "                img = img.reshape(\n",
    "                 14, 14, 16, 16, 3\n",
    "                )\n",
    "                #batch\n",
    "                img = torch.einsum(\"hwpqc->chpwq\", img)\n",
    "                img = img.reshape(3, 224, 224)\n",
    "                #first img of the batch\n",
    "                \n",
    "                img = img.view(3,224,224)\n",
    "                img_masked = img.cpu().detach().numpy().transpose(1,2,0)\n",
    "                img_masked_org = img_masked*img_originale\n",
    "                img_reconv2 = ((1-img_masked)*img_recon)+img_masked_org\n",
    "                img_recon= ((1-img_masked)*img_recon)\n",
    "                #img = np.maximum(img,0)\n",
    "                #img_masked = (img_masked-img_masked.min())/(img_masked.max()-img_masked.min())#np.minimum(img,1)\n",
    "                img_cat = np.concatenate((img_originale,img_masked_org,img_recon,img_reconv2),axis=1)\n",
    "                plt.imsave(f'outputs_newL/epoch_{epoch+1}_batch_{i}.png',img_cat)\n",
    "        train_loss.append(np.mean(train_loss_))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i,images in enumerate(val_loader):\n",
    "                \n",
    "                images = images.to(device)\n",
    "                images['pixel_values']=images['pixel_values'].to(torch.float32)\n",
    "                s = images['pixel_values'].shape\n",
    "                images['pixel_values']=images['pixel_values'].view(s[0],s[-3],s[-2],s[-1])\n",
    "                output = model(**images)\n",
    "                loss = output.loss\n",
    "                val_loss_.append(loss.item())\n",
    "                \n",
    "            val_loss.append(np.mean(val_loss_))\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}')\n",
    "        #save the last output image on the disk\n",
    "        #save the model\n",
    "        torch.save(model.state_dict(),f'last3l.pth')\n",
    "        \n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 0, Loss: 0.24588117003440857\n",
      "Epoch: 1, Batch: 64, Loss: 0.2342035174369812\n",
      "Epoch: 1, Batch: 128, Loss: 0.2543008625507355\n",
      "Epoch: 1, Batch: 192, Loss: 0.21421873569488525\n",
      "Epoch: 1, Train Loss: 0.22098694539676278, Val Loss: 0.22613456138109755\n",
      "Epoch: 2, Batch: 0, Loss: 0.22318096458911896\n",
      "Epoch: 2, Batch: 64, Loss: 0.21475069224834442\n",
      "Epoch: 2, Batch: 128, Loss: 0.21557511389255524\n",
      "Epoch: 2, Batch: 192, Loss: 0.1971559375524521\n",
      "Epoch: 2, Train Loss: 0.22037122638548834, Val Loss: 0.22794569902500864\n",
      "Epoch: 3, Batch: 0, Loss: 0.24961717426776886\n",
      "Epoch: 3, Batch: 64, Loss: 0.23962678015232086\n",
      "Epoch: 3, Batch: 128, Loss: 0.20404097437858582\n",
      "Epoch: 3, Batch: 192, Loss: 0.22111573815345764\n",
      "Epoch: 3, Train Loss: 0.21935206860051318, Val Loss: 0.23033750840162825\n",
      "Epoch: 4, Batch: 0, Loss: 0.23012675344944\n",
      "Epoch: 4, Batch: 64, Loss: 0.21092359721660614\n",
      "Epoch: 4, Batch: 128, Loss: 0.2186511754989624\n",
      "Epoch: 4, Batch: 192, Loss: 0.24471765756607056\n",
      "Epoch: 4, Train Loss: 0.21961218724816534, Val Loss: 0.22905899382243722\n",
      "Epoch: 5, Batch: 0, Loss: 0.23217760026454926\n",
      "Epoch: 5, Batch: 64, Loss: 0.17795853316783905\n",
      "Epoch: 5, Batch: 128, Loss: 0.20445501804351807\n",
      "Epoch: 5, Batch: 192, Loss: 0.24501951038837433\n",
      "Epoch: 5, Train Loss: 0.21792348979388254, Val Loss: 0.22768956448061992\n",
      "Epoch: 6, Batch: 0, Loss: 0.20554398000240326\n",
      "Epoch: 6, Batch: 64, Loss: 0.2849739193916321\n",
      "Epoch: 6, Batch: 128, Loss: 0.26002389192581177\n",
      "Epoch: 6, Batch: 192, Loss: 0.1705869436264038\n",
      "Epoch: 6, Train Loss: 0.21753278734572865, Val Loss: 0.2291576521376432\n",
      "Epoch: 7, Batch: 0, Loss: 0.1790897399187088\n",
      "Epoch: 7, Batch: 64, Loss: 0.23452985286712646\n",
      "Epoch: 7, Batch: 128, Loss: 0.24352754652500153\n",
      "Epoch: 7, Batch: 192, Loss: 0.21349601447582245\n",
      "Epoch: 7, Train Loss: 0.21792034817449116, Val Loss: 0.22893039124496914\n",
      "Epoch: 8, Batch: 0, Loss: 0.24710902571678162\n",
      "Epoch: 8, Batch: 64, Loss: 0.21456590294837952\n",
      "Epoch: 8, Batch: 128, Loss: 0.24882933497428894\n",
      "Epoch: 8, Batch: 192, Loss: 0.20559339225292206\n",
      "Epoch: 8, Train Loss: 0.2158839594635923, Val Loss: 0.22848991898156829\n",
      "Epoch: 9, Batch: 0, Loss: 0.23140259087085724\n",
      "Epoch: 9, Batch: 64, Loss: 0.21711944043636322\n",
      "Epoch: 9, Batch: 128, Loss: 0.22177982330322266\n",
      "Epoch: 9, Batch: 192, Loss: 0.18329553306102753\n",
      "Epoch: 9, Train Loss: 0.2155973252098439, Val Loss: 0.23009347536806332\n",
      "Epoch: 10, Batch: 0, Loss: 0.21615366637706757\n",
      "Epoch: 10, Batch: 64, Loss: 0.21617914736270905\n",
      "Epoch: 10, Batch: 128, Loss: 0.22480788826942444\n",
      "Epoch: 10, Batch: 192, Loss: 0.20271895825862885\n",
      "Epoch: 10, Train Loss: 0.2150502151225583, Val Loss: 0.22930871455346125\n",
      "Epoch: 11, Batch: 0, Loss: 0.18858051300048828\n",
      "Epoch: 11, Batch: 64, Loss: 0.17358969151973724\n",
      "Epoch: 11, Batch: 128, Loss: 0.2082926332950592\n",
      "Epoch: 11, Batch: 192, Loss: 0.21235059201717377\n",
      "Epoch: 11, Train Loss: 0.21322684716117585, Val Loss: 0.22826258812920522\n",
      "Epoch: 12, Batch: 0, Loss: 0.2648841440677643\n",
      "Epoch: 12, Batch: 64, Loss: 0.1991197019815445\n",
      "Epoch: 12, Batch: 128, Loss: 0.19459907710552216\n",
      "Epoch: 12, Batch: 192, Loss: 0.19932501018047333\n",
      "Epoch: 12, Train Loss: 0.2131440693165286, Val Loss: 0.2307012020531347\n",
      "Epoch: 13, Batch: 0, Loss: 0.22986997663974762\n",
      "Epoch: 13, Batch: 64, Loss: 0.23611289262771606\n",
      "Epoch: 13, Batch: 128, Loss: 0.19858746230602264\n",
      "Epoch: 13, Batch: 192, Loss: 0.21476934850215912\n",
      "Epoch: 13, Train Loss: 0.21490188296568596, Val Loss: 0.2305518818103661\n",
      "Epoch: 14, Batch: 0, Loss: 0.1855262815952301\n",
      "Epoch: 14, Batch: 64, Loss: 0.1696311980485916\n",
      "Epoch: 14, Batch: 128, Loss: 0.18913324177265167\n",
      "Epoch: 14, Batch: 192, Loss: 0.22186020016670227\n",
      "Epoch: 14, Train Loss: 0.21229655516602225, Val Loss: 0.23230629098617425\n",
      "Epoch: 15, Batch: 0, Loss: 0.1681807041168213\n",
      "Epoch: 15, Batch: 64, Loss: 0.17983253300189972\n",
      "Epoch: 15, Batch: 128, Loss: 0.21351578831672668\n",
      "Epoch: 15, Batch: 192, Loss: 0.24294935166835785\n",
      "Epoch: 15, Train Loss: 0.21325541123495265, Val Loss: 0.2330434304677834\n",
      "Epoch: 16, Batch: 0, Loss: 0.23877358436584473\n",
      "Epoch: 16, Batch: 64, Loss: 0.24912124872207642\n",
      "Epoch: 16, Batch: 128, Loss: 0.22278563678264618\n",
      "Epoch: 16, Batch: 192, Loss: 0.2468200922012329\n",
      "Epoch: 16, Train Loss: 0.20981552609700269, Val Loss: 0.22911249353724011\n",
      "Epoch: 17, Batch: 0, Loss: 0.21073780953884125\n",
      "Epoch: 17, Batch: 64, Loss: 0.19710569083690643\n",
      "Epoch: 17, Batch: 128, Loss: 0.2105133980512619\n",
      "Epoch: 17, Batch: 192, Loss: 0.2550348937511444\n",
      "Epoch: 17, Train Loss: 0.21043155058208157, Val Loss: 0.23126185319181217\n",
      "Epoch: 18, Batch: 0, Loss: 0.22013162076473236\n",
      "Epoch: 18, Batch: 64, Loss: 0.22839204967021942\n",
      "Epoch: 18, Batch: 128, Loss: 0.1774713695049286\n",
      "Epoch: 18, Batch: 192, Loss: 0.219716414809227\n",
      "Epoch: 18, Train Loss: 0.20924649936920506, Val Loss: 0.23179991214962328\n",
      "Epoch: 19, Batch: 0, Loss: 0.18358710408210754\n",
      "Epoch: 19, Batch: 64, Loss: 0.2328798472881317\n",
      "Epoch: 19, Batch: 128, Loss: 0.22967438399791718\n",
      "Epoch: 19, Batch: 192, Loss: 0.20815423130989075\n",
      "Epoch: 19, Train Loss: 0.20951300518492522, Val Loss: 0.2332032471895218\n",
      "Epoch: 20, Batch: 0, Loss: 0.2283557951450348\n",
      "Epoch: 20, Batch: 64, Loss: 0.22758318483829498\n",
      "Epoch: 20, Batch: 128, Loss: 0.1965826153755188\n",
      "Epoch: 20, Batch: 192, Loss: 0.20566841959953308\n",
      "Epoch: 20, Train Loss: 0.20897472687697005, Val Loss: 0.22887026834285865\n",
      "Epoch: 21, Batch: 0, Loss: 0.22795693576335907\n",
      "Epoch: 21, Batch: 64, Loss: 0.18609294295310974\n",
      "Epoch: 21, Batch: 128, Loss: 0.18240328133106232\n",
      "Epoch: 21, Batch: 192, Loss: 0.22975823283195496\n",
      "Epoch: 21, Train Loss: 0.20783506479051153, Val Loss: 0.2309066069833303\n",
      "Epoch: 22, Batch: 0, Loss: 0.18246257305145264\n",
      "Epoch: 22, Batch: 64, Loss: 0.20832960307598114\n",
      "Epoch: 22, Batch: 128, Loss: 0.2082303762435913\n",
      "Epoch: 22, Batch: 192, Loss: 0.271487832069397\n",
      "Epoch: 22, Train Loss: 0.20889981312014289, Val Loss: 0.23122591007564028\n",
      "Epoch: 23, Batch: 0, Loss: 0.22603732347488403\n",
      "Epoch: 23, Batch: 64, Loss: 0.17798258364200592\n",
      "Epoch: 23, Batch: 128, Loss: 0.19472894072532654\n",
      "Epoch: 23, Batch: 192, Loss: 0.19085176289081573\n",
      "Epoch: 23, Train Loss: 0.20679365294212002, Val Loss: 0.23030208606841202\n",
      "Epoch: 24, Batch: 0, Loss: 0.2582142949104309\n",
      "Epoch: 24, Batch: 64, Loss: 0.2186761200428009\n",
      "Epoch: 24, Batch: 128, Loss: 0.17832835018634796\n",
      "Epoch: 24, Batch: 192, Loss: 0.21944935619831085\n",
      "Epoch: 24, Train Loss: 0.2065868458505404, Val Loss: 0.23177837618326735\n",
      "Epoch: 25, Batch: 0, Loss: 0.21461789309978485\n",
      "Epoch: 25, Batch: 64, Loss: 0.19612903892993927\n",
      "Epoch: 25, Batch: 128, Loss: 0.18946000933647156\n",
      "Epoch: 25, Batch: 192, Loss: 0.26705220341682434\n",
      "Epoch: 25, Train Loss: 0.2070246826289064, Val Loss: 0.23019918950937562\n",
      "Epoch: 26, Batch: 0, Loss: 0.2168918251991272\n",
      "Epoch: 26, Batch: 64, Loss: 0.21508771181106567\n",
      "Epoch: 26, Batch: 128, Loss: 0.19488613307476044\n",
      "Epoch: 26, Batch: 192, Loss: 0.19615080952644348\n",
      "Epoch: 26, Train Loss: 0.20471214054752204, Val Loss: 0.2308497731968508\n",
      "Epoch: 27, Batch: 0, Loss: 0.1889190673828125\n",
      "Epoch: 27, Batch: 64, Loss: 0.20108193159103394\n",
      "Epoch: 27, Batch: 128, Loss: 0.2513703405857086\n",
      "Epoch: 27, Batch: 192, Loss: 0.18143533170223236\n",
      "Epoch: 27, Train Loss: 0.2046351315990343, Val Loss: 0.23336022385096145\n",
      "Epoch: 28, Batch: 0, Loss: 0.1699080765247345\n",
      "Epoch: 28, Batch: 64, Loss: 0.19219417870044708\n",
      "Epoch: 28, Batch: 128, Loss: 0.18060870468616486\n",
      "Epoch: 28, Batch: 192, Loss: 0.2077271044254303\n",
      "Epoch: 28, Train Loss: 0.20451706306914152, Val Loss: 0.23390976427975346\n",
      "Epoch: 29, Batch: 0, Loss: 0.2290443331003189\n",
      "Epoch: 29, Batch: 64, Loss: 0.20102325081825256\n",
      "Epoch: 29, Batch: 128, Loss: 0.20815116167068481\n",
      "Epoch: 29, Batch: 192, Loss: 0.2285323441028595\n",
      "Epoch: 29, Train Loss: 0.20409858668759717, Val Loss: 0.23390186969506538\n",
      "Epoch: 30, Batch: 0, Loss: 0.18505537509918213\n",
      "Epoch: 30, Batch: 64, Loss: 0.219246044754982\n",
      "Epoch: 30, Batch: 128, Loss: 0.20925064384937286\n",
      "Epoch: 30, Batch: 192, Loss: 0.1920294314622879\n",
      "Epoch: 30, Train Loss: 0.2026260699761116, Val Loss: 0.23308687518208715\n",
      "Epoch: 31, Batch: 0, Loss: 0.1783951371908188\n",
      "Epoch: 31, Batch: 64, Loss: 0.18649491667747498\n",
      "Epoch: 31, Batch: 128, Loss: 0.21430301666259766\n",
      "Epoch: 31, Batch: 192, Loss: 0.1767132729291916\n",
      "Epoch: 31, Train Loss: 0.20092871437891055, Val Loss: 0.2307747650449559\n",
      "Epoch: 32, Batch: 0, Loss: 0.23265205323696136\n",
      "Epoch: 32, Batch: 64, Loss: 0.18979507684707642\n",
      "Epoch: 32, Batch: 128, Loss: 0.23020540177822113\n",
      "Epoch: 32, Batch: 192, Loss: 0.21250106394290924\n",
      "Epoch: 32, Train Loss: 0.2012529638359102, Val Loss: 0.2311733706017672\n",
      "Epoch: 33, Batch: 0, Loss: 0.21363236010074615\n",
      "Epoch: 33, Batch: 64, Loss: 0.15705087780952454\n",
      "Epoch: 33, Batch: 128, Loss: 0.23643633723258972\n",
      "Epoch: 33, Batch: 192, Loss: 0.20230089128017426\n",
      "Epoch: 33, Train Loss: 0.20295572621842561, Val Loss: 0.23354787922511666\n",
      "Epoch: 34, Batch: 0, Loss: 0.1825573444366455\n",
      "Epoch: 34, Batch: 64, Loss: 0.17482180893421173\n",
      "Epoch: 34, Batch: 128, Loss: 0.20695540308952332\n",
      "Epoch: 34, Batch: 192, Loss: 0.21427972614765167\n",
      "Epoch: 34, Train Loss: 0.20155011774119685, Val Loss: 0.23221626524197853\n",
      "Epoch: 35, Batch: 0, Loss: 0.19085010886192322\n",
      "Epoch: 35, Batch: 64, Loss: 0.1665588617324829\n",
      "Epoch: 35, Batch: 128, Loss: 0.1908237636089325\n",
      "Epoch: 35, Batch: 192, Loss: 0.18659467995166779\n",
      "Epoch: 35, Train Loss: 0.19942340368436554, Val Loss: 0.2346982185618352\n",
      "Epoch: 36, Batch: 0, Loss: 0.231735959649086\n",
      "Epoch: 36, Batch: 64, Loss: 0.21029435098171234\n",
      "Epoch: 36, Batch: 128, Loss: 0.20355556905269623\n",
      "Epoch: 36, Batch: 192, Loss: 0.23533132672309875\n",
      "Epoch: 36, Train Loss: 0.19985998163031318, Val Loss: 0.2327462780778691\n",
      "Epoch: 37, Batch: 0, Loss: 0.18321339786052704\n",
      "Epoch: 37, Batch: 64, Loss: 0.1729796677827835\n",
      "Epoch: 37, Batch: 128, Loss: 0.1853875368833542\n",
      "Epoch: 37, Batch: 192, Loss: 0.20421123504638672\n",
      "Epoch: 37, Train Loss: 0.19911693181779425, Val Loss: 0.2319522444474495\n",
      "Epoch: 38, Batch: 0, Loss: 0.16851578652858734\n",
      "Epoch: 38, Batch: 64, Loss: 0.21488894522190094\n",
      "Epoch: 38, Batch: 128, Loss: 0.2072768211364746\n",
      "Epoch: 38, Batch: 192, Loss: 0.20319350063800812\n",
      "Epoch: 38, Train Loss: 0.1988089720134513, Val Loss: 0.23181765412880204\n",
      "Epoch: 39, Batch: 0, Loss: 0.21360433101654053\n",
      "Epoch: 39, Batch: 64, Loss: 0.19508129358291626\n",
      "Epoch: 39, Batch: 128, Loss: 0.1678404062986374\n",
      "Epoch: 39, Batch: 192, Loss: 0.17066000401973724\n",
      "Epoch: 39, Train Loss: 0.19729660609263486, Val Loss: 0.23262871599803536\n",
      "Epoch: 40, Batch: 0, Loss: 0.23967018723487854\n",
      "Epoch: 40, Batch: 64, Loss: 0.22858195006847382\n",
      "Epoch: 40, Batch: 128, Loss: 0.19803455471992493\n",
      "Epoch: 40, Batch: 192, Loss: 0.257046639919281\n",
      "Epoch: 40, Train Loss: 0.19734781551159034, Val Loss: 0.23429896669872738\n",
      "Epoch: 41, Batch: 0, Loss: 0.20963634550571442\n",
      "Epoch: 41, Batch: 64, Loss: 0.1911931186914444\n",
      "Epoch: 41, Batch: 128, Loss: 0.15676620602607727\n",
      "Epoch: 41, Batch: 192, Loss: 0.17367857694625854\n",
      "Epoch: 41, Train Loss: 0.19607397699255055, Val Loss: 0.23511582082611018\n",
      "Epoch: 42, Batch: 0, Loss: 0.16273429989814758\n",
      "Epoch: 42, Batch: 64, Loss: 0.19992904365062714\n",
      "Epoch: 42, Batch: 128, Loss: 0.18956774473190308\n",
      "Epoch: 42, Batch: 192, Loss: 0.19775813817977905\n",
      "Epoch: 42, Train Loss: 0.1953006009176626, Val Loss: 0.23451486911814093\n",
      "Epoch: 43, Batch: 0, Loss: 0.20937985181808472\n",
      "Epoch: 43, Batch: 64, Loss: 0.18523313105106354\n",
      "Epoch: 43, Batch: 128, Loss: 0.20826812088489532\n",
      "Epoch: 43, Batch: 192, Loss: 0.16787393391132355\n",
      "Epoch: 43, Train Loss: 0.19559299150260828, Val Loss: 0.23490695958420382\n",
      "Epoch: 44, Batch: 0, Loss: 0.18807461857795715\n",
      "Epoch: 44, Batch: 64, Loss: 0.16598135232925415\n",
      "Epoch: 44, Batch: 128, Loss: 0.19027192890644073\n",
      "Epoch: 44, Batch: 192, Loss: 0.16700506210327148\n",
      "Epoch: 44, Train Loss: 0.19301249187881664, Val Loss: 0.23462242822525864\n",
      "Epoch: 45, Batch: 0, Loss: 0.20585767924785614\n",
      "Epoch: 45, Batch: 64, Loss: 0.2253665328025818\n",
      "Epoch: 45, Batch: 128, Loss: 0.20506486296653748\n",
      "Epoch: 45, Batch: 192, Loss: 0.1938113272190094\n",
      "Epoch: 45, Train Loss: 0.19468115219625376, Val Loss: 0.2349994248252804\n",
      "Epoch: 46, Batch: 0, Loss: 0.1780339926481247\n",
      "Epoch: 46, Batch: 64, Loss: 0.21541497111320496\n",
      "Epoch: 46, Batch: 128, Loss: 0.2394179254770279\n",
      "Epoch: 46, Batch: 192, Loss: 0.19462570548057556\n",
      "Epoch: 46, Train Loss: 0.1934636418976016, Val Loss: 0.23713221464116693\n",
      "Epoch: 47, Batch: 0, Loss: 0.20939794182777405\n",
      "Epoch: 47, Batch: 64, Loss: 0.15644054114818573\n",
      "Epoch: 47, Batch: 128, Loss: 0.18012654781341553\n",
      "Epoch: 47, Batch: 192, Loss: 0.21469487249851227\n",
      "Epoch: 47, Train Loss: 0.19460968104964596, Val Loss: 0.23508429527282715\n",
      "Epoch: 48, Batch: 0, Loss: 0.19302578270435333\n",
      "Epoch: 48, Batch: 64, Loss: 0.1735130399465561\n",
      "Epoch: 48, Batch: 128, Loss: 0.22516590356826782\n",
      "Epoch: 48, Batch: 192, Loss: 0.21079345047473907\n",
      "Epoch: 48, Train Loss: 0.19180199611237495, Val Loss: 0.2349264434838699\n",
      "Epoch: 49, Batch: 0, Loss: 0.18529459834098816\n",
      "Epoch: 49, Batch: 64, Loss: 0.1889154314994812\n",
      "Epoch: 49, Batch: 128, Loss: 0.21992848813533783\n",
      "Epoch: 49, Batch: 192, Loss: 0.18610280752182007\n",
      "Epoch: 49, Train Loss: 0.19129268326244112, Val Loss: 0.23460956650265194\n",
      "Epoch: 50, Batch: 0, Loss: 0.19559559226036072\n",
      "Epoch: 50, Batch: 64, Loss: 0.18804453313350677\n",
      "Epoch: 50, Batch: 128, Loss: 0.1963621973991394\n",
      "Epoch: 50, Batch: 192, Loss: 0.15824128687381744\n",
      "Epoch: 50, Train Loss: 0.19277830832337928, Val Loss: 0.23411632165060206\n",
      "Epoch: 51, Batch: 0, Loss: 0.1896210014820099\n",
      "Epoch: 51, Batch: 64, Loss: 0.17135612666606903\n",
      "Epoch: 51, Batch: 128, Loss: 0.1803465336561203\n",
      "Epoch: 51, Batch: 192, Loss: 0.18145699799060822\n",
      "Epoch: 51, Train Loss: 0.19040647243796768, Val Loss: 0.23469040530212856\n",
      "Epoch: 52, Batch: 0, Loss: 0.202077716588974\n",
      "Epoch: 52, Batch: 64, Loss: 0.18371957540512085\n",
      "Epoch: 52, Batch: 128, Loss: 0.1925041228532791\n",
      "Epoch: 52, Batch: 192, Loss: 0.22978514432907104\n",
      "Epoch: 52, Train Loss: 0.18896646892367783, Val Loss: 0.23344353725344447\n",
      "Epoch: 53, Batch: 0, Loss: 0.20514944195747375\n",
      "Epoch: 53, Batch: 64, Loss: 0.16720154881477356\n",
      "Epoch: 53, Batch: 128, Loss: 0.19206009805202484\n",
      "Epoch: 53, Batch: 192, Loss: 0.18764705955982208\n",
      "Epoch: 53, Train Loss: 0.18896161152397173, Val Loss: 0.2353409594398434\n",
      "Epoch: 54, Batch: 0, Loss: 0.1775611937046051\n",
      "Epoch: 54, Batch: 64, Loss: 0.214114710688591\n",
      "Epoch: 54, Batch: 128, Loss: 0.18496307730674744\n",
      "Epoch: 54, Batch: 192, Loss: 0.17781122028827667\n",
      "Epoch: 54, Train Loss: 0.18775081918653796, Val Loss: 0.23359298731310893\n",
      "Epoch: 55, Batch: 0, Loss: 0.189559206366539\n",
      "Epoch: 55, Batch: 64, Loss: 0.1877022236585617\n",
      "Epoch: 55, Batch: 128, Loss: 0.17845343053340912\n",
      "Epoch: 55, Batch: 192, Loss: 0.19018088281154633\n",
      "Epoch: 55, Train Loss: 0.1878291526967186, Val Loss: 0.23646006569013758\n",
      "Epoch: 56, Batch: 0, Loss: 0.19934333860874176\n",
      "Epoch: 56, Batch: 64, Loss: 0.18049414455890656\n",
      "Epoch: 56, Batch: 128, Loss: 0.18617136776447296\n",
      "Epoch: 56, Batch: 192, Loss: 0.20820267498493195\n",
      "Epoch: 56, Train Loss: 0.18748500444373842, Val Loss: 0.23611667449191465\n",
      "Epoch: 57, Batch: 0, Loss: 0.2004874348640442\n",
      "Epoch: 57, Batch: 64, Loss: 0.2044200599193573\n",
      "Epoch: 57, Batch: 128, Loss: 0.17803125083446503\n",
      "Epoch: 57, Batch: 192, Loss: 0.19385185837745667\n",
      "Epoch: 57, Train Loss: 0.1873626830845566, Val Loss: 0.23380665152759875\n",
      "Epoch: 58, Batch: 0, Loss: 0.17317228019237518\n",
      "Epoch: 58, Batch: 64, Loss: 0.2109580636024475\n",
      "Epoch: 58, Batch: 128, Loss: 0.17984874546527863\n",
      "Epoch: 58, Batch: 192, Loss: 0.19052235782146454\n",
      "Epoch: 58, Train Loss: 0.18593330340365233, Val Loss: 0.23502512806552953\n",
      "Epoch: 59, Batch: 0, Loss: 0.1784854233264923\n",
      "Epoch: 59, Batch: 64, Loss: 0.19654254615306854\n",
      "Epoch: 59, Batch: 128, Loss: 0.22396616637706757\n",
      "Epoch: 59, Batch: 192, Loss: 0.18911948800086975\n",
      "Epoch: 59, Train Loss: 0.18519492485260558, Val Loss: 0.2366924725346646\n",
      "Epoch: 60, Batch: 0, Loss: 0.17055703699588776\n",
      "Epoch: 60, Batch: 64, Loss: 0.2122258096933365\n",
      "Epoch: 60, Batch: 128, Loss: 0.18213124573230743\n",
      "Epoch: 60, Batch: 192, Loss: 0.1676701456308365\n",
      "Epoch: 60, Train Loss: 0.18518544664069758, Val Loss: 0.23818054088091445\n",
      "Epoch: 61, Batch: 0, Loss: 0.21232850849628448\n",
      "Epoch: 61, Batch: 64, Loss: 0.1964840292930603\n",
      "Epoch: 61, Batch: 128, Loss: 0.20324473083019257\n",
      "Epoch: 61, Batch: 192, Loss: 0.15842010080814362\n",
      "Epoch: 61, Train Loss: 0.18432068679544886, Val Loss: 0.23819036655506845\n",
      "Epoch: 62, Batch: 0, Loss: 0.17052875459194183\n",
      "Epoch: 62, Batch: 64, Loss: 0.19178679585456848\n",
      "Epoch: 62, Batch: 128, Loss: 0.16894154250621796\n",
      "Epoch: 62, Batch: 192, Loss: 0.21516239643096924\n",
      "Epoch: 62, Train Loss: 0.18320598977349573, Val Loss: 0.2378945067777472\n",
      "Epoch: 63, Batch: 0, Loss: 0.16591744124889374\n",
      "Epoch: 63, Batch: 64, Loss: 0.14716745913028717\n",
      "Epoch: 63, Batch: 128, Loss: 0.15409545600414276\n",
      "Epoch: 63, Batch: 192, Loss: 0.16264428198337555\n",
      "Epoch: 63, Train Loss: 0.18251702221015753, Val Loss: 0.23805224769196268\n",
      "Epoch: 64, Batch: 0, Loss: 0.17898669838905334\n",
      "Epoch: 64, Batch: 64, Loss: 0.21840128302574158\n",
      "Epoch: 64, Batch: 128, Loss: 0.19185733795166016\n",
      "Epoch: 64, Batch: 192, Loss: 0.1972189098596573\n",
      "Epoch: 64, Train Loss: 0.18202733450521857, Val Loss: 0.23679917646666704\n",
      "Epoch: 65, Batch: 0, Loss: 0.1781376749277115\n",
      "Epoch: 65, Batch: 64, Loss: 0.16398939490318298\n",
      "Epoch: 65, Batch: 128, Loss: 0.21048608422279358\n",
      "Epoch: 65, Batch: 192, Loss: 0.1937907189130783\n",
      "Epoch: 65, Train Loss: 0.18284090008523504, Val Loss: 0.2397300057997138\n",
      "Epoch: 66, Batch: 0, Loss: 0.18061429262161255\n",
      "Epoch: 66, Batch: 64, Loss: 0.2085546851158142\n",
      "Epoch: 66, Batch: 128, Loss: 0.19365133345127106\n",
      "Epoch: 66, Batch: 192, Loss: 0.16195432841777802\n",
      "Epoch: 66, Train Loss: 0.18197226360187693, Val Loss: 0.2394350817648031\n",
      "Epoch: 67, Batch: 0, Loss: 0.17784565687179565\n",
      "Epoch: 67, Batch: 64, Loss: 0.20033881068229675\n",
      "Epoch: 67, Batch: 128, Loss: 0.20763176679611206\n",
      "Epoch: 67, Batch: 192, Loss: 0.14002715051174164\n",
      "Epoch: 67, Train Loss: 0.18061073358028623, Val Loss: 0.2364860101271484\n",
      "Epoch: 68, Batch: 0, Loss: 0.18925291299819946\n",
      "Epoch: 68, Batch: 64, Loss: 0.18889184296131134\n",
      "Epoch: 68, Batch: 128, Loss: 0.15755771100521088\n",
      "Epoch: 68, Batch: 192, Loss: 0.1840425580739975\n",
      "Epoch: 68, Train Loss: 0.17996030509219332, Val Loss: 0.23802379960730924\n",
      "Epoch: 69, Batch: 0, Loss: 0.17035208642482758\n",
      "Epoch: 69, Batch: 64, Loss: 0.16151012480258942\n",
      "Epoch: 69, Batch: 128, Loss: 0.1766691654920578\n",
      "Epoch: 69, Batch: 192, Loss: 0.18270628154277802\n",
      "Epoch: 69, Train Loss: 0.1799722138350293, Val Loss: 0.23896392952587645\n",
      "Epoch: 70, Batch: 0, Loss: 0.19869297742843628\n",
      "Epoch: 70, Batch: 64, Loss: 0.17397235333919525\n",
      "Epoch: 70, Batch: 128, Loss: 0.20211827754974365\n",
      "Epoch: 70, Batch: 192, Loss: 0.16402962803840637\n",
      "Epoch: 70, Train Loss: 0.17936238621251058, Val Loss: 0.23834645369295346\n",
      "Epoch: 71, Batch: 0, Loss: 0.18765045702457428\n",
      "Epoch: 71, Batch: 64, Loss: 0.21213532984256744\n",
      "Epoch: 71, Batch: 128, Loss: 0.17159590125083923\n",
      "Epoch: 71, Batch: 192, Loss: 0.20755702257156372\n",
      "Epoch: 71, Train Loss: 0.17788517490913303, Val Loss: 0.23887221590947297\n",
      "Epoch: 72, Batch: 0, Loss: 0.15231800079345703\n",
      "Epoch: 72, Batch: 64, Loss: 0.1505749672651291\n",
      "Epoch: 72, Batch: 128, Loss: 0.17093457281589508\n",
      "Epoch: 72, Batch: 192, Loss: 0.22106797993183136\n",
      "Epoch: 72, Train Loss: 0.17794033862902955, Val Loss: 0.2402385329290972\n",
      "Epoch: 73, Batch: 0, Loss: 0.15653270483016968\n",
      "Epoch: 73, Batch: 64, Loss: 0.1721429079771042\n",
      "Epoch: 73, Batch: 128, Loss: 0.1799851655960083\n",
      "Epoch: 73, Batch: 192, Loss: 0.18998415768146515\n",
      "Epoch: 73, Train Loss: 0.17715956662165916, Val Loss: 0.23891338680760335\n",
      "Epoch: 74, Batch: 0, Loss: 0.1903844177722931\n",
      "Epoch: 74, Batch: 64, Loss: 0.2099226415157318\n",
      "Epoch: 74, Batch: 128, Loss: 0.19553832709789276\n",
      "Epoch: 74, Batch: 192, Loss: 0.1594049632549286\n",
      "Epoch: 74, Train Loss: 0.17776418262619084, Val Loss: 0.24115762695417567\n",
      "Epoch: 75, Batch: 0, Loss: 0.20905132591724396\n",
      "Epoch: 75, Batch: 64, Loss: 0.16426223516464233\n",
      "Epoch: 75, Batch: 128, Loss: 0.1541280895471573\n",
      "Epoch: 75, Batch: 192, Loss: 0.15213876962661743\n",
      "Epoch: 75, Train Loss: 0.17730367328908483, Val Loss: 0.24113790791923717\n",
      "Epoch: 76, Batch: 0, Loss: 0.20149266719818115\n",
      "Epoch: 76, Batch: 64, Loss: 0.15708038210868835\n",
      "Epoch: 76, Batch: 128, Loss: 0.1792304962873459\n",
      "Epoch: 76, Batch: 192, Loss: 0.16426947712898254\n",
      "Epoch: 76, Train Loss: 0.17620570263115026, Val Loss: 0.2431923747062683\n",
      "Epoch: 77, Batch: 0, Loss: 0.16319502890110016\n",
      "Epoch: 77, Batch: 64, Loss: 0.16422192752361298\n",
      "Epoch: 77, Batch: 128, Loss: 0.19722522795200348\n",
      "Epoch: 77, Batch: 192, Loss: 0.18735262751579285\n",
      "Epoch: 77, Train Loss: 0.1756606358340231, Val Loss: 0.2411241796562227\n",
      "Epoch: 78, Batch: 0, Loss: 0.1822972595691681\n",
      "Epoch: 78, Batch: 64, Loss: 0.16100983321666718\n",
      "Epoch: 78, Batch: 128, Loss: 0.1963784545660019\n",
      "Epoch: 78, Batch: 192, Loss: 0.17367208003997803\n",
      "Epoch: 78, Train Loss: 0.17464722585627587, Val Loss: 0.24507833890995737\n",
      "Epoch: 79, Batch: 0, Loss: 0.16205988824367523\n",
      "Epoch: 79, Batch: 64, Loss: 0.19800789654254913\n",
      "Epoch: 79, Batch: 128, Loss: 0.19533976912498474\n",
      "Epoch: 79, Batch: 192, Loss: 0.1617009937763214\n",
      "Epoch: 79, Train Loss: 0.1748895152645596, Val Loss: 0.24304081070221076\n",
      "Epoch: 80, Batch: 0, Loss: 0.16048724949359894\n",
      "Epoch: 80, Batch: 64, Loss: 0.1645239293575287\n",
      "Epoch: 80, Batch: 128, Loss: 0.18168053030967712\n",
      "Epoch: 80, Batch: 192, Loss: 0.15961451828479767\n",
      "Epoch: 80, Train Loss: 0.1740135828948627, Val Loss: 0.24440504383232634\n",
      "Epoch: 81, Batch: 0, Loss: 0.1847122311592102\n",
      "Epoch: 81, Batch: 64, Loss: 0.1654934585094452\n",
      "Epoch: 81, Batch: 128, Loss: 0.15152758359909058\n",
      "Epoch: 81, Batch: 192, Loss: 0.19233189523220062\n",
      "Epoch: 81, Train Loss: 0.17328026107812333, Val Loss: 0.2416002566026429\n",
      "Epoch: 82, Batch: 0, Loss: 0.16158922016620636\n",
      "Epoch: 82, Batch: 64, Loss: 0.16104793548583984\n",
      "Epoch: 82, Batch: 128, Loss: 0.16000308096408844\n",
      "Epoch: 82, Batch: 192, Loss: 0.18699464201927185\n",
      "Epoch: 82, Train Loss: 0.17384171249123953, Val Loss: 0.24129607869406877\n",
      "Epoch: 83, Batch: 0, Loss: 0.13449233770370483\n",
      "Epoch: 83, Batch: 64, Loss: 0.1764736771583557\n",
      "Epoch: 83, Batch: 128, Loss: 0.15018808841705322\n",
      "Epoch: 83, Batch: 192, Loss: 0.16243667900562286\n",
      "Epoch: 83, Train Loss: 0.17269039832813254, Val Loss: 0.24362486095751745\n",
      "Epoch: 84, Batch: 0, Loss: 0.16026946902275085\n",
      "Epoch: 84, Batch: 64, Loss: 0.1863025426864624\n",
      "Epoch: 84, Batch: 128, Loss: 0.17013131082057953\n",
      "Epoch: 84, Batch: 192, Loss: 0.14400547742843628\n",
      "Epoch: 84, Train Loss: 0.17170556113755298, Val Loss: 0.2440690148179814\n",
      "Epoch: 85, Batch: 0, Loss: 0.20209142565727234\n",
      "Epoch: 85, Batch: 64, Loss: 0.17971521615982056\n",
      "Epoch: 85, Batch: 128, Loss: 0.16001704335212708\n",
      "Epoch: 85, Batch: 192, Loss: 0.17036880552768707\n",
      "Epoch: 85, Train Loss: 0.17073967045772884, Val Loss: 0.24378457185575517\n",
      "Epoch: 86, Batch: 0, Loss: 0.15996097028255463\n",
      "Epoch: 86, Batch: 64, Loss: 0.17984503507614136\n",
      "Epoch: 86, Batch: 128, Loss: 0.22388112545013428\n",
      "Epoch: 86, Batch: 192, Loss: 0.13980984687805176\n",
      "Epoch: 86, Train Loss: 0.17058679815066063, Val Loss: 0.24508643554428877\n",
      "Epoch: 87, Batch: 0, Loss: 0.21590247750282288\n",
      "Epoch: 87, Batch: 64, Loss: 0.15142837166786194\n",
      "Epoch: 87, Batch: 128, Loss: 0.16693662106990814\n",
      "Epoch: 87, Batch: 192, Loss: 0.1598641574382782\n",
      "Epoch: 87, Train Loss: 0.17045325665908345, Val Loss: 0.2446173780550391\n",
      "Epoch: 88, Batch: 0, Loss: 0.1886429339647293\n",
      "Epoch: 88, Batch: 64, Loss: 0.2181054651737213\n",
      "Epoch: 88, Batch: 128, Loss: 0.20821131765842438\n",
      "Epoch: 88, Batch: 192, Loss: 0.17364998161792755\n",
      "Epoch: 88, Train Loss: 0.16943810119340985, Val Loss: 0.24406483845185425\n",
      "Epoch: 89, Batch: 0, Loss: 0.14562541246414185\n",
      "Epoch: 89, Batch: 64, Loss: 0.1668308526277542\n",
      "Epoch: 89, Batch: 128, Loss: 0.16162078082561493\n",
      "Epoch: 89, Batch: 192, Loss: 0.16820599138736725\n",
      "Epoch: 89, Train Loss: 0.16915969503254202, Val Loss: 0.24237871927730106\n",
      "Epoch: 90, Batch: 0, Loss: 0.1924002319574356\n",
      "Epoch: 90, Batch: 64, Loss: 0.1342088282108307\n",
      "Epoch: 90, Batch: 128, Loss: 0.15379878878593445\n",
      "Epoch: 90, Batch: 192, Loss: 0.19290593266487122\n",
      "Epoch: 90, Train Loss: 0.1687615308279203, Val Loss: 0.24211715338593823\n",
      "Epoch: 91, Batch: 0, Loss: 0.1496332883834839\n",
      "Epoch: 91, Batch: 64, Loss: 0.20921240746974945\n",
      "Epoch: 91, Batch: 128, Loss: 0.18060187995433807\n",
      "Epoch: 91, Batch: 192, Loss: 0.1798432618379593\n",
      "Epoch: 91, Train Loss: 0.16842567336635064, Val Loss: 0.24584949521695154\n",
      "Epoch: 92, Batch: 0, Loss: 0.15145814418792725\n",
      "Epoch: 92, Batch: 64, Loss: 0.14607398211956024\n",
      "Epoch: 92, Batch: 128, Loss: 0.16575783491134644\n",
      "Epoch: 92, Batch: 192, Loss: 0.16394230723381042\n",
      "Epoch: 92, Train Loss: 0.16724323737040414, Val Loss: 0.2433549539517548\n",
      "Epoch: 93, Batch: 0, Loss: 0.17092721164226532\n",
      "Epoch: 93, Batch: 64, Loss: 0.19002918899059296\n",
      "Epoch: 93, Batch: 128, Loss: 0.16023313999176025\n",
      "Epoch: 93, Batch: 192, Loss: 0.1458490639925003\n",
      "Epoch: 93, Train Loss: 0.16697050681558706, Val Loss: 0.24673889678413585\n",
      "Epoch: 94, Batch: 0, Loss: 0.14965398609638214\n",
      "Epoch: 94, Batch: 64, Loss: 0.14211952686309814\n",
      "Epoch: 94, Batch: 128, Loss: 0.18445685505867004\n",
      "Epoch: 94, Batch: 192, Loss: 0.16841229796409607\n",
      "Epoch: 94, Train Loss: 0.16498012979656962, Val Loss: 0.24715563124519283\n",
      "Epoch: 95, Batch: 0, Loss: 0.14353670179843903\n",
      "Epoch: 95, Batch: 64, Loss: 0.12463643401861191\n",
      "Epoch: 95, Batch: 128, Loss: 0.18463659286499023\n",
      "Epoch: 95, Batch: 192, Loss: 0.18999342620372772\n",
      "Epoch: 95, Train Loss: 0.16539715018944215, Val Loss: 0.24569870632583812\n",
      "Epoch: 96, Batch: 0, Loss: 0.16412048041820526\n",
      "Epoch: 96, Batch: 64, Loss: 0.1389700174331665\n",
      "Epoch: 96, Batch: 128, Loss: 0.14413075149059296\n",
      "Epoch: 96, Batch: 192, Loss: 0.1394425481557846\n",
      "Epoch: 96, Train Loss: 0.16500734140054654, Val Loss: 0.2462956988710468\n",
      "Epoch: 97, Batch: 0, Loss: 0.15981481969356537\n",
      "Epoch: 97, Batch: 64, Loss: 0.18408609926700592\n",
      "Epoch: 97, Batch: 128, Loss: 0.1522061824798584\n",
      "Epoch: 97, Batch: 192, Loss: 0.18134012818336487\n",
      "Epoch: 97, Train Loss: 0.1642607583711713, Val Loss: 0.24575318774934543\n",
      "Epoch: 98, Batch: 0, Loss: 0.15361452102661133\n",
      "Epoch: 98, Batch: 64, Loss: 0.17534351348876953\n",
      "Epoch: 98, Batch: 128, Loss: 0.1321239322423935\n",
      "Epoch: 98, Batch: 192, Loss: 0.1734769195318222\n",
      "Epoch: 98, Train Loss: 0.16492145830544375, Val Loss: 0.24455031095925023\n",
      "Epoch: 99, Batch: 0, Loss: 0.14721530675888062\n",
      "Epoch: 99, Batch: 64, Loss: 0.1900867521762848\n",
      "Epoch: 99, Batch: 128, Loss: 0.14254042506217957\n",
      "Epoch: 99, Batch: 192, Loss: 0.17331038415431976\n",
      "Epoch: 99, Train Loss: 0.16356045206598305, Val Loss: 0.24510610608731287\n",
      "Epoch: 100, Batch: 0, Loss: 0.1359875202178955\n",
      "Epoch: 100, Batch: 64, Loss: 0.14110305905342102\n",
      "Epoch: 100, Batch: 128, Loss: 0.14833304286003113\n",
      "Epoch: 100, Batch: 192, Loss: 0.1696595549583435\n",
      "Epoch: 100, Train Loss: 0.16276230435755293, Val Loss: 0.24678780795153926\n",
      "Epoch: 101, Batch: 0, Loss: 0.15700151026248932\n",
      "Epoch: 101, Batch: 64, Loss: 0.17032873630523682\n",
      "Epoch: 101, Batch: 128, Loss: 0.16706036031246185\n",
      "Epoch: 101, Batch: 192, Loss: 0.14067552983760834\n",
      "Epoch: 101, Train Loss: 0.1618464944284346, Val Loss: 0.246105763871791\n",
      "Epoch: 102, Batch: 0, Loss: 0.17689171433448792\n",
      "Epoch: 102, Batch: 64, Loss: 0.17915618419647217\n",
      "Epoch: 102, Batch: 128, Loss: 0.1748778223991394\n",
      "Epoch: 102, Batch: 192, Loss: 0.15623672306537628\n",
      "Epoch: 102, Train Loss: 0.16142851641496359, Val Loss: 0.24658602072020708\n",
      "Epoch: 103, Batch: 0, Loss: 0.16586437821388245\n",
      "Epoch: 103, Batch: 64, Loss: 0.1627824604511261\n",
      "Epoch: 103, Batch: 128, Loss: 0.12668295204639435\n",
      "Epoch: 103, Batch: 192, Loss: 0.15943899750709534\n",
      "Epoch: 103, Train Loss: 0.16067145338629263, Val Loss: 0.246674008541188\n",
      "Epoch: 104, Batch: 0, Loss: 0.1667415201663971\n",
      "Epoch: 104, Batch: 64, Loss: 0.15127985179424286\n",
      "Epoch: 104, Batch: 128, Loss: 0.16975735127925873\n",
      "Epoch: 104, Batch: 192, Loss: 0.16483134031295776\n",
      "Epoch: 104, Train Loss: 0.16100781676122697, Val Loss: 0.24478157091948946\n",
      "Epoch: 105, Batch: 0, Loss: 0.15206372737884521\n",
      "Epoch: 105, Batch: 64, Loss: 0.15204735100269318\n",
      "Epoch: 105, Batch: 128, Loss: 0.16693776845932007\n",
      "Epoch: 105, Batch: 192, Loss: 0.1448071002960205\n",
      "Epoch: 105, Train Loss: 0.16142872989303986, Val Loss: 0.24910071290145486\n",
      "Epoch: 106, Batch: 0, Loss: 0.15033048391342163\n",
      "Epoch: 106, Batch: 64, Loss: 0.14862896502017975\n",
      "Epoch: 106, Batch: 128, Loss: 0.16318383812904358\n",
      "Epoch: 106, Batch: 192, Loss: 0.131594717502594\n",
      "Epoch: 106, Train Loss: 0.16102803653200803, Val Loss: 0.2480764240026474\n",
      "Epoch: 107, Batch: 0, Loss: 0.14502327144145966\n",
      "Epoch: 107, Batch: 64, Loss: 0.1556016355752945\n",
      "Epoch: 107, Batch: 128, Loss: 0.14913561940193176\n",
      "Epoch: 107, Batch: 192, Loss: 0.1862383484840393\n",
      "Epoch: 107, Train Loss: 0.16081777991632284, Val Loss: 0.24420323286016107\n",
      "Epoch: 108, Batch: 0, Loss: 0.16626505553722382\n",
      "Epoch: 108, Batch: 64, Loss: 0.16500739753246307\n",
      "Epoch: 108, Batch: 128, Loss: 0.18539246916770935\n",
      "Epoch: 108, Batch: 192, Loss: 0.15138423442840576\n",
      "Epoch: 108, Train Loss: 0.15904862057985897, Val Loss: 0.24868070826692096\n",
      "Epoch: 109, Batch: 0, Loss: 0.15015007555484772\n",
      "Epoch: 109, Batch: 64, Loss: 0.1487571746110916\n",
      "Epoch: 109, Batch: 128, Loss: 0.1376257985830307\n",
      "Epoch: 109, Batch: 192, Loss: 0.1484338939189911\n",
      "Epoch: 109, Train Loss: 0.15885369103970165, Val Loss: 0.24869984767194522\n",
      "Epoch: 110, Batch: 0, Loss: 0.17335055768489838\n",
      "Epoch: 110, Batch: 64, Loss: 0.171407550573349\n",
      "Epoch: 110, Batch: 128, Loss: 0.14962266385555267\n",
      "Epoch: 110, Batch: 192, Loss: 0.14156712591648102\n",
      "Epoch: 110, Train Loss: 0.1573631954774008, Val Loss: 0.24941985637454664\n",
      "Epoch: 111, Batch: 0, Loss: 0.14286485314369202\n",
      "Epoch: 111, Batch: 64, Loss: 0.14475008845329285\n",
      "Epoch: 111, Batch: 128, Loss: 0.16927075386047363\n",
      "Epoch: 111, Batch: 192, Loss: 0.14530237019062042\n",
      "Epoch: 111, Train Loss: 0.15794761469429833, Val Loss: 0.2484227082992004\n",
      "Epoch: 112, Batch: 0, Loss: 0.13123276829719543\n",
      "Epoch: 112, Batch: 64, Loss: 0.163593128323555\n",
      "Epoch: 112, Batch: 128, Loss: 0.1446818858385086\n",
      "Epoch: 112, Batch: 192, Loss: 0.13716581463813782\n",
      "Epoch: 112, Train Loss: 0.15752935586339337, Val Loss: 0.2498109229540421\n",
      "Epoch: 113, Batch: 0, Loss: 0.18888388574123383\n",
      "Epoch: 113, Batch: 64, Loss: 0.1339733898639679\n",
      "Epoch: 113, Batch: 128, Loss: 0.14656944572925568\n",
      "Epoch: 113, Batch: 192, Loss: 0.17153508961200714\n",
      "Epoch: 113, Train Loss: 0.15677833781277728, Val Loss: 0.2489674462605331\n",
      "Epoch: 114, Batch: 0, Loss: 0.13891814649105072\n",
      "Epoch: 114, Batch: 64, Loss: 0.14759448170661926\n",
      "Epoch: 114, Batch: 128, Loss: 0.1311555653810501\n",
      "Epoch: 114, Batch: 192, Loss: 0.1951645314693451\n",
      "Epoch: 114, Train Loss: 0.15609952245475883, Val Loss: 0.250553394020614\n",
      "Epoch: 115, Batch: 0, Loss: 0.1381143182516098\n",
      "Epoch: 115, Batch: 64, Loss: 0.1594783216714859\n",
      "Epoch: 115, Batch: 128, Loss: 0.18263523280620575\n",
      "Epoch: 115, Batch: 192, Loss: 0.1406526416540146\n",
      "Epoch: 115, Train Loss: 0.15587906814101388, Val Loss: 0.2493667226221602\n",
      "Epoch: 116, Batch: 0, Loss: 0.15673111379146576\n",
      "Epoch: 116, Batch: 64, Loss: 0.1390022337436676\n",
      "Epoch: 116, Batch: 128, Loss: 0.1508650779724121\n",
      "Epoch: 116, Batch: 192, Loss: 0.13865458965301514\n",
      "Epoch: 116, Train Loss: 0.15496241045579062, Val Loss: 0.24978417486457502\n",
      "Epoch: 117, Batch: 0, Loss: 0.15472598373889923\n",
      "Epoch: 117, Batch: 64, Loss: 0.16554711759090424\n",
      "Epoch: 117, Batch: 128, Loss: 0.17161895334720612\n",
      "Epoch: 117, Batch: 192, Loss: 0.149339959025383\n",
      "Epoch: 117, Train Loss: 0.15497283912184884, Val Loss: 0.2501932425519167\n",
      "Epoch: 118, Batch: 0, Loss: 0.13482174277305603\n",
      "Epoch: 118, Batch: 64, Loss: 0.14882926642894745\n",
      "Epoch: 118, Batch: 128, Loss: 0.1469610035419464\n",
      "Epoch: 118, Batch: 192, Loss: 0.2001456767320633\n",
      "Epoch: 118, Train Loss: 0.15421233433535544, Val Loss: 0.2526753289719759\n",
      "Epoch: 119, Batch: 0, Loss: 0.15819504857063293\n",
      "Epoch: 119, Batch: 64, Loss: 0.1693594604730606\n",
      "Epoch: 119, Batch: 128, Loss: 0.16505098342895508\n",
      "Epoch: 119, Batch: 192, Loss: 0.153664693236351\n",
      "Epoch: 119, Train Loss: 0.15348509752775652, Val Loss: 0.2509190624548217\n",
      "Epoch: 120, Batch: 0, Loss: 0.12888582050800323\n",
      "Epoch: 120, Batch: 64, Loss: 0.16868731379508972\n",
      "Epoch: 120, Batch: 128, Loss: 0.13803547620773315\n",
      "Epoch: 120, Batch: 192, Loss: 0.16957756876945496\n",
      "Epoch: 120, Train Loss: 0.15409954791983305, Val Loss: 0.24739850628173957\n",
      "Epoch: 121, Batch: 0, Loss: 0.1717095524072647\n",
      "Epoch: 121, Batch: 64, Loss: 0.15204374492168427\n",
      "Epoch: 121, Batch: 128, Loss: 0.12977683544158936\n",
      "Epoch: 121, Batch: 192, Loss: 0.1625637710094452\n",
      "Epoch: 121, Train Loss: 0.1528111856935893, Val Loss: 0.25167556687936943\n",
      "Epoch: 122, Batch: 0, Loss: 0.15854179859161377\n",
      "Epoch: 122, Batch: 64, Loss: 0.15369750559329987\n",
      "Epoch: 122, Batch: 128, Loss: 0.15279251337051392\n",
      "Epoch: 122, Batch: 192, Loss: 0.13178293406963348\n",
      "Epoch: 122, Train Loss: 0.15256889638001636, Val Loss: 0.2500503959797196\n",
      "Epoch: 123, Batch: 0, Loss: 0.13718613982200623\n",
      "Epoch: 123, Batch: 64, Loss: 0.12907780706882477\n",
      "Epoch: 123, Batch: 128, Loss: 0.14170487225055695\n",
      "Epoch: 123, Batch: 192, Loss: 0.14687000215053558\n",
      "Epoch: 123, Train Loss: 0.1519231927281214, Val Loss: 0.2526226402339289\n",
      "Epoch: 124, Batch: 0, Loss: 0.1585434079170227\n",
      "Epoch: 124, Batch: 64, Loss: 0.14521998167037964\n",
      "Epoch: 124, Batch: 128, Loss: 0.14997287094593048\n",
      "Epoch: 124, Batch: 192, Loss: 0.1261611431837082\n",
      "Epoch: 124, Train Loss: 0.1516579009466252, Val Loss: 0.2522466523667513\n",
      "Epoch: 125, Batch: 0, Loss: 0.16113053262233734\n",
      "Epoch: 125, Batch: 64, Loss: 0.15136092901229858\n",
      "Epoch: 125, Batch: 128, Loss: 0.12634281814098358\n",
      "Epoch: 125, Batch: 192, Loss: 0.11369191110134125\n",
      "Epoch: 125, Train Loss: 0.15153047177246062, Val Loss: 0.2506515886318886\n",
      "Epoch: 126, Batch: 0, Loss: 0.15082651376724243\n",
      "Epoch: 126, Batch: 64, Loss: 0.16505371034145355\n",
      "Epoch: 126, Batch: 128, Loss: 0.1026134341955185\n",
      "Epoch: 126, Batch: 192, Loss: 0.14946088194847107\n",
      "Epoch: 126, Train Loss: 0.15106657920878822, Val Loss: 0.25245452331284346\n",
      "Epoch: 127, Batch: 0, Loss: 0.1328810304403305\n",
      "Epoch: 127, Batch: 64, Loss: 0.15529093146324158\n",
      "Epoch: 127, Batch: 128, Loss: 0.15634602308273315\n",
      "Epoch: 127, Batch: 192, Loss: 0.11765609681606293\n",
      "Epoch: 127, Train Loss: 0.15038360712134233, Val Loss: 0.2532912290702432\n",
      "Epoch: 128, Batch: 0, Loss: 0.1480877846479416\n",
      "Epoch: 128, Batch: 64, Loss: 0.15140917897224426\n",
      "Epoch: 128, Batch: 128, Loss: 0.16560091078281403\n",
      "Epoch: 128, Batch: 192, Loss: 0.18598727881908417\n",
      "Epoch: 128, Train Loss: 0.15047180422155534, Val Loss: 0.2516825034961862\n",
      "Epoch: 129, Batch: 0, Loss: 0.1352337747812271\n",
      "Epoch: 129, Batch: 64, Loss: 0.15171334147453308\n",
      "Epoch: 129, Batch: 128, Loss: 0.1614346206188202\n",
      "Epoch: 129, Batch: 192, Loss: 0.15979117155075073\n",
      "Epoch: 129, Train Loss: 0.14946735205160358, Val Loss: 0.25334345637741734\n",
      "Epoch: 130, Batch: 0, Loss: 0.11469683796167374\n",
      "Epoch: 130, Batch: 64, Loss: 0.14210809767246246\n",
      "Epoch: 130, Batch: 128, Loss: 0.1568712741136551\n",
      "Epoch: 130, Batch: 192, Loss: 0.14431262016296387\n",
      "Epoch: 130, Train Loss: 0.1487991296196893, Val Loss: 0.25298937675306354\n",
      "Epoch: 131, Batch: 0, Loss: 0.16056913137435913\n",
      "Epoch: 131, Batch: 64, Loss: 0.1454799473285675\n",
      "Epoch: 131, Batch: 128, Loss: 0.15164336562156677\n",
      "Epoch: 131, Batch: 192, Loss: 0.14703796803951263\n",
      "Epoch: 131, Train Loss: 0.14809606012777757, Val Loss: 0.2551661691928314\n",
      "Epoch: 132, Batch: 0, Loss: 0.13863228261470795\n",
      "Epoch: 132, Batch: 64, Loss: 0.13669396936893463\n",
      "Epoch: 132, Batch: 128, Loss: 0.14457006752490997\n",
      "Epoch: 132, Batch: 192, Loss: 0.1583341509103775\n",
      "Epoch: 132, Train Loss: 0.14833008321159977, Val Loss: 0.25414657340211383\n",
      "Epoch: 133, Batch: 0, Loss: 0.1375434249639511\n",
      "Epoch: 133, Batch: 64, Loss: 0.13478079438209534\n",
      "Epoch: 133, Batch: 128, Loss: 0.1476372629404068\n",
      "Epoch: 133, Batch: 192, Loss: 0.16108542680740356\n",
      "Epoch: 133, Train Loss: 0.1482274328689959, Val Loss: 0.25188508554030276\n",
      "Epoch: 134, Batch: 0, Loss: 0.15600493550300598\n",
      "Epoch: 134, Batch: 64, Loss: 0.1223980113863945\n",
      "Epoch: 134, Batch: 128, Loss: 0.142800971865654\n",
      "Epoch: 134, Batch: 192, Loss: 0.13551707565784454\n",
      "Epoch: 134, Train Loss: 0.1472906765480668, Val Loss: 0.25369198140451465\n",
      "Epoch: 135, Batch: 0, Loss: 0.12489662319421768\n",
      "Epoch: 135, Batch: 64, Loss: 0.128163143992424\n",
      "Epoch: 135, Batch: 128, Loss: 0.1592482179403305\n",
      "Epoch: 135, Batch: 192, Loss: 0.12445486336946487\n",
      "Epoch: 135, Train Loss: 0.14640900331659842, Val Loss: 0.25483474034374043\n",
      "Epoch: 136, Batch: 0, Loss: 0.16781538724899292\n",
      "Epoch: 136, Batch: 64, Loss: 0.13165181875228882\n",
      "Epoch: 136, Batch: 128, Loss: 0.12854193150997162\n",
      "Epoch: 136, Batch: 192, Loss: 0.14184178411960602\n",
      "Epoch: 136, Train Loss: 0.1464037987072084, Val Loss: 0.2545885602801533\n",
      "Epoch: 137, Batch: 0, Loss: 0.1444714069366455\n",
      "Epoch: 137, Batch: 64, Loss: 0.15258187055587769\n",
      "Epoch: 137, Batch: 128, Loss: 0.1701909899711609\n",
      "Epoch: 137, Batch: 192, Loss: 0.1472206711769104\n",
      "Epoch: 137, Train Loss: 0.14634322160381383, Val Loss: 0.25286011170532746\n",
      "Epoch: 138, Batch: 0, Loss: 0.15904860198497772\n",
      "Epoch: 138, Batch: 64, Loss: 0.13347654044628143\n",
      "Epoch: 138, Batch: 128, Loss: 0.15177389979362488\n",
      "Epoch: 138, Batch: 192, Loss: 0.15721066296100616\n",
      "Epoch: 138, Train Loss: 0.14674427458164047, Val Loss: 0.2548767371197878\n",
      "Epoch: 139, Batch: 0, Loss: 0.1147351935505867\n",
      "Epoch: 139, Batch: 64, Loss: 0.12544260919094086\n",
      "Epoch: 139, Batch: 128, Loss: 0.12811315059661865\n",
      "Epoch: 139, Batch: 192, Loss: 0.16364438831806183\n",
      "Epoch: 139, Train Loss: 0.14565932071941384, Val Loss: 0.25429299574787334\n",
      "Epoch: 140, Batch: 0, Loss: 0.17095395922660828\n",
      "Epoch: 140, Batch: 64, Loss: 0.17637208104133606\n",
      "Epoch: 140, Batch: 128, Loss: 0.12418372929096222\n",
      "Epoch: 140, Batch: 192, Loss: 0.15641658008098602\n",
      "Epoch: 140, Train Loss: 0.1447868338959702, Val Loss: 0.25298200749744804\n",
      "Epoch: 141, Batch: 0, Loss: 0.16026967763900757\n",
      "Epoch: 141, Batch: 64, Loss: 0.16105592250823975\n",
      "Epoch: 141, Batch: 128, Loss: 0.1268254518508911\n",
      "Epoch: 141, Batch: 192, Loss: 0.13074856996536255\n",
      "Epoch: 141, Train Loss: 0.1437503816717762, Val Loss: 0.2534662659390498\n",
      "Epoch: 142, Batch: 0, Loss: 0.1499839872121811\n",
      "Epoch: 142, Batch: 64, Loss: 0.12232115119695663\n",
      "Epoch: 142, Batch: 128, Loss: 0.1437426656484604\n",
      "Epoch: 142, Batch: 192, Loss: 0.14121299982070923\n",
      "Epoch: 142, Train Loss: 0.14292378521571725, Val Loss: 0.2538568642684969\n",
      "Epoch: 143, Batch: 0, Loss: 0.14743253588676453\n",
      "Epoch: 143, Batch: 64, Loss: 0.17905744910240173\n",
      "Epoch: 143, Batch: 128, Loss: 0.14087215065956116\n",
      "Epoch: 143, Batch: 192, Loss: 0.14560464024543762\n",
      "Epoch: 143, Train Loss: 0.14324988854133477, Val Loss: 0.25334987120103025\n",
      "Epoch: 144, Batch: 0, Loss: 0.1373656541109085\n",
      "Epoch: 144, Batch: 64, Loss: 0.12134244292974472\n",
      "Epoch: 144, Batch: 128, Loss: 0.13510899245738983\n",
      "Epoch: 144, Batch: 192, Loss: 0.15106646716594696\n",
      "Epoch: 144, Train Loss: 0.14192812611996117, Val Loss: 0.2565441853919272\n",
      "Epoch: 145, Batch: 0, Loss: 0.15545113384723663\n",
      "Epoch: 145, Batch: 64, Loss: 0.1389903426170349\n",
      "Epoch: 145, Batch: 128, Loss: 0.15125508606433868\n",
      "Epoch: 145, Batch: 192, Loss: 0.11985315382480621\n",
      "Epoch: 145, Train Loss: 0.1418718959252208, Val Loss: 0.25543864652261894\n",
      "Epoch: 146, Batch: 0, Loss: 0.1588297188282013\n",
      "Epoch: 146, Batch: 64, Loss: 0.13985677063465118\n",
      "Epoch: 146, Batch: 128, Loss: 0.14149996638298035\n",
      "Epoch: 146, Batch: 192, Loss: 0.1424054056406021\n",
      "Epoch: 146, Train Loss: 0.14119573452083742, Val Loss: 0.2555307306475558\n",
      "Epoch: 147, Batch: 0, Loss: 0.15142209827899933\n",
      "Epoch: 147, Batch: 64, Loss: 0.13364490866661072\n",
      "Epoch: 147, Batch: 128, Loss: 0.108860544860363\n",
      "Epoch: 147, Batch: 192, Loss: 0.14319659769535065\n",
      "Epoch: 147, Train Loss: 0.14112824706707972, Val Loss: 0.2558462617255874\n",
      "Epoch: 148, Batch: 0, Loss: 0.1578870564699173\n",
      "Epoch: 148, Batch: 64, Loss: 0.1625107377767563\n",
      "Epoch: 148, Batch: 128, Loss: 0.1443215012550354\n",
      "Epoch: 148, Batch: 192, Loss: 0.15586313605308533\n",
      "Epoch: 148, Train Loss: 0.14068940556529214, Val Loss: 0.25707423282881914\n",
      "Epoch: 149, Batch: 0, Loss: 0.14988844096660614\n",
      "Epoch: 149, Batch: 64, Loss: 0.1556379795074463\n",
      "Epoch: 149, Batch: 128, Loss: 0.16076409816741943\n",
      "Epoch: 149, Batch: 192, Loss: 0.1454603224992752\n",
      "Epoch: 149, Train Loss: 0.14008751918830104, Val Loss: 0.25891549758991955\n",
      "Epoch: 150, Batch: 0, Loss: 0.1473861038684845\n",
      "Epoch: 150, Batch: 64, Loss: 0.12152452766895294\n",
      "Epoch: 150, Batch: 128, Loss: 0.14720644056797028\n",
      "Epoch: 150, Batch: 192, Loss: 0.15468242764472961\n",
      "Epoch: 150, Train Loss: 0.13921348599054045, Val Loss: 0.25792897277969423\n",
      "Epoch: 151, Batch: 0, Loss: 0.13649846613407135\n",
      "Epoch: 151, Batch: 64, Loss: 0.1473436802625656\n",
      "Epoch: 151, Batch: 128, Loss: 0.15822485089302063\n",
      "Epoch: 151, Batch: 192, Loss: 0.12903855741024017\n",
      "Epoch: 151, Train Loss: 0.1390241458696329, Val Loss: 0.25825591637926587\n",
      "Epoch: 152, Batch: 0, Loss: 0.1469014436006546\n",
      "Epoch: 152, Batch: 64, Loss: 0.15910744667053223\n",
      "Epoch: 152, Batch: 128, Loss: 0.13095980882644653\n",
      "Epoch: 152, Batch: 192, Loss: 0.15834660828113556\n",
      "Epoch: 152, Train Loss: 0.13815392244417787, Val Loss: 0.2564057142047559\n",
      "Epoch: 153, Batch: 0, Loss: 0.13999369740486145\n",
      "Epoch: 153, Batch: 64, Loss: 0.13317471742630005\n",
      "Epoch: 153, Batch: 128, Loss: 0.14695177972316742\n",
      "Epoch: 153, Batch: 192, Loss: 0.1467246562242508\n",
      "Epoch: 153, Train Loss: 0.13914621005750308, Val Loss: 0.25843213927947867\n",
      "Epoch: 154, Batch: 0, Loss: 0.1278935670852661\n",
      "Epoch: 154, Batch: 64, Loss: 0.13088548183441162\n",
      "Epoch: 154, Batch: 128, Loss: 0.1518210768699646\n",
      "Epoch: 154, Batch: 192, Loss: 0.15687978267669678\n",
      "Epoch: 154, Train Loss: 0.13895604838380368, Val Loss: 0.2600995420904483\n",
      "Epoch: 155, Batch: 0, Loss: 0.12247495353221893\n",
      "Epoch: 155, Batch: 64, Loss: 0.10743622481822968\n",
      "Epoch: 155, Batch: 128, Loss: 0.12803921103477478\n",
      "Epoch: 155, Batch: 192, Loss: 0.17077259719371796\n",
      "Epoch: 155, Train Loss: 0.13847940298334016, Val Loss: 0.25712221936654234\n",
      "Epoch: 156, Batch: 0, Loss: 0.15402798354625702\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss,val_loss \u001b[39m=\u001b[39m train(model,train_loader,val_loader,epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,lr\u001b[39m=\u001b[39;49m\u001b[39m3e-4\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[0;32m     21\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mimages)\n\u001b[0;32m     22\u001b[0m loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> 23\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     24\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m train_loss_\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss,val_loss = train(model,train_loader,val_loader,epochs=500,lr=3e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
