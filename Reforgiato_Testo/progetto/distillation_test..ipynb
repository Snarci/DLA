{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# ! pip3 install einops\n",
    "# ! pip3 install vit_pytorch\n",
    "# ! pip3 install pandas\n",
    "# ! pip3 install scikit-learn\n",
    "# ! pip3 install albumentations\n",
    "# ! pip3 install matplotlib\n",
    "# ! pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from vit_pytorch.vit import Transformer\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets_utils\n",
    "train_loader, val_loader, test_loader, classes , img_size = datasets_utils.get_Chaoyang_loaders(BATCH_SIZE=64, SEED=42)\n",
    "# train_loader, val_loader, test_loader, classes , img_size= datasets_utils.get_CUB_loaders(BATCH_SIZE=256, SEED=42, SPLITS=[0.50,0.25,0.25])\n",
    "# train_loader, val_loader, test_loader, classes , img_size = datasets_utils.get_vegetables_dataloader(BATCH_SIZE=32, SEED=42, SPLITS=[0.50,0.25,0.25])\n",
    "N_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "from vit_pytorch.distill import DistillableViT, DistillWrapper\n",
    "\n",
    "\n",
    "teacher = resnet50(pretrained = True)\n",
    "# replace the head of the teacher with a new fc with N_CLASSES as neurons\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, N_CLASSES)\n",
    "teacher = teacher.to(device)\n",
    "\n",
    "student_vit = DistillableViT(\n",
    "    image_size = img_size,\n",
    "    patch_size = 32,\n",
    "    num_classes = N_CLASSES,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "student_vit = student_vit.to(device)\n",
    "\n",
    "distiller = DistillWrapper(\n",
    "    student = student_vit,\n",
    "    teacher = teacher,\n",
    "    temperature = 3,           # temperature of distillation\n",
    "    alpha = 0.5,               # trade between main loss and distillation loss\n",
    "    hard = False               # whether to use soft or hard distillation\n",
    ")\n",
    "distiller = distiller.to(device)\n",
    "\n",
    "# img = torch.randn(124, 3, 256, 256)\n",
    "# img = img.to(device)\n",
    "# labels = torch.randint(0, 16, (124,))\n",
    "# labels = labels.to(device)\n",
    "\n",
    "# loss = distiller(img, labels)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "def train(model,train_loader,val_loader,epochs=10,lr=1e-2):\n",
    "    current_valid_loss , best_valid_loss = float('inf'), float('inf')\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_ = []\n",
    "        val_loss_ = []\n",
    "        for i,(images,classes) in enumerate(train_loader):\n",
    "\n",
    "            if type(images)==transformers.image_processing_utils.BatchFeature:\n",
    "                images = images['pixel_values']\n",
    "                images=images.to(torch.float32)\n",
    "                s = images.shape\n",
    "                images=images.view(s[0],s[-3],s[-2],s[-1])\n",
    "                images = F.interpolate(images, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "            images = images.to(device)\n",
    "            classes = classes.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(images, classes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_.append(loss.item())\n",
    "            # #every 100 batches, print the loss\n",
    "        train_loss.append(np.mean(train_loss_))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i,(images,classes) in enumerate(val_loader):\n",
    "                if type(images)==transformers.image_processing_utils.BatchFeature:\n",
    "                    images = images['pixel_values']\n",
    "                    images=images.to(torch.float32)\n",
    "                    s = images.shape\n",
    "                    images=images.view(s[0],s[-3],s[-2],s[-1])\n",
    "                    images = F.interpolate(images, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                images = images.to(device)\n",
    "                classes = classes.to(device)\n",
    "                loss = model(images, classes)\n",
    "\n",
    "                current_valid_loss = loss.item()\n",
    "                val_loss_.append(loss.item())\n",
    "               \n",
    "            mean_loss = np.mean(val_loss_)\n",
    "            current_valid_loss = mean_loss\n",
    "            if current_valid_loss < best_valid_loss:\n",
    "                best_valid_loss = current_valid_loss\n",
    "                print(f\"\\nBest validation loss: {best_valid_loss}\")\n",
    "                print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "                torch.save(optimizer.state_dict(), 'jacoExperiments/best_distilled_model.pth')\n",
    "            val_loss.append(mean_loss)\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}')\n",
    "\n",
    "        # print(f'Epoch: {epoch+1}, Train Loss: {train_loss[-1]}')\n",
    "        #save the last output image on the disk\n",
    "        #save the model\n",
    "        torch.save(model.state_dict(),f'jacoExperiments/last_distilled.pth')\n",
    "        \n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation loss: 0.6310295131471422\n",
      "\n",
      "Saving best model for epoch: 1\n",
      "\n",
      "Epoch: 1, Train Loss: 0.775751488118232, Val Loss: 0.6310295131471422\n",
      "\n",
      "Best validation loss: 0.5922340154647827\n",
      "\n",
      "Saving best model for epoch: 2\n",
      "\n",
      "Epoch: 2, Train Loss: 0.6369126152388657, Val Loss: 0.5922340154647827\n",
      "\n",
      "Best validation loss: 0.5671591560045878\n",
      "\n",
      "Saving best model for epoch: 3\n",
      "\n",
      "Epoch: 3, Train Loss: 0.6165906198417084, Val Loss: 0.5671591560045878\n",
      "\n",
      "Best validation loss: 0.5616585844092898\n",
      "\n",
      "Saving best model for epoch: 4\n",
      "\n",
      "Epoch: 4, Train Loss: 0.5905899850628044, Val Loss: 0.5616585844092898\n",
      "\n",
      "Best validation loss: 0.5013054410616556\n",
      "\n",
      "Saving best model for epoch: 5\n",
      "\n",
      "Epoch: 5, Train Loss: 0.5696962113621868, Val Loss: 0.5013054410616556\n",
      "\n",
      "Best validation loss: 0.4945526553524865\n",
      "\n",
      "Saving best model for epoch: 6\n",
      "\n",
      "Epoch: 6, Train Loss: 0.5593898194500163, Val Loss: 0.4945526553524865\n",
      "\n",
      "Best validation loss: 0.49395152264171177\n",
      "\n",
      "Saving best model for epoch: 7\n",
      "\n",
      "Epoch: 7, Train Loss: 0.5336030306695383, Val Loss: 0.49395152264171177\n",
      "\n",
      "Best validation loss: 0.4708704915311601\n",
      "\n",
      "Saving best model for epoch: 8\n",
      "\n",
      "Epoch: 8, Train Loss: 0.5094845272317717, Val Loss: 0.4708704915311601\n",
      "\n",
      "Best validation loss: 0.4649674395720164\n",
      "\n",
      "Saving best model for epoch: 9\n",
      "\n",
      "Epoch: 9, Train Loss: 0.5055230335344242, Val Loss: 0.4649674395720164\n",
      "\n",
      "Best validation loss: 0.43442876140276593\n",
      "\n",
      "Saving best model for epoch: 10\n",
      "\n",
      "Epoch: 10, Train Loss: 0.48358152372927604, Val Loss: 0.43442876140276593\n",
      "Epoch: 11, Train Loss: 0.4773904960366744, Val Loss: 0.452672067615721\n",
      "Epoch: 12, Train Loss: 0.4687536332426192, Val Loss: 0.44857532117101884\n",
      "Epoch: 13, Train Loss: 0.46209761387185205, Val Loss: 0.4504366351498498\n",
      "\n",
      "Best validation loss: 0.4333224594593048\n",
      "\n",
      "Saving best model for epoch: 14\n",
      "\n",
      "Epoch: 14, Train Loss: 0.469581445183935, Val Loss: 0.4333224594593048\n",
      "\n",
      "Best validation loss: 0.4158039953973558\n",
      "\n",
      "Saving best model for epoch: 15\n",
      "\n",
      "Epoch: 15, Train Loss: 0.4448918402194977, Val Loss: 0.4158039953973558\n",
      "Epoch: 16, Train Loss: 0.4536104077779794, Val Loss: 0.41701894336276585\n",
      "\n",
      "Best validation loss: 0.40966157780753243\n",
      "\n",
      "Saving best model for epoch: 17\n",
      "\n",
      "Epoch: 17, Train Loss: 0.44241918981829775, Val Loss: 0.40966157780753243\n",
      "Epoch: 18, Train Loss: 0.42912753699701045, Val Loss: 0.41614240739080643\n",
      "\n",
      "Best validation loss: 0.4041067792309655\n",
      "\n",
      "Saving best model for epoch: 19\n",
      "\n",
      "Epoch: 19, Train Loss: 0.4261469791961622, Val Loss: 0.4041067792309655\n",
      "Epoch: 20, Train Loss: 0.42777360541911064, Val Loss: 0.40871937407387626\n",
      "\n",
      "Best validation loss: 0.3874298764599694\n",
      "\n",
      "Saving best model for epoch: 21\n",
      "\n",
      "Epoch: 21, Train Loss: 0.4246766348428364, Val Loss: 0.3874298764599694\n",
      "Epoch: 22, Train Loss: 0.417639839121058, Val Loss: 0.43336983521779376\n",
      "\n",
      "Best validation loss: 0.38620440496338737\n",
      "\n",
      "Saving best model for epoch: 23\n",
      "\n",
      "Epoch: 23, Train Loss: 0.4204227237761775, Val Loss: 0.38620440496338737\n",
      "Epoch: 24, Train Loss: 0.4035009526753727, Val Loss: 0.42108501659499276\n",
      "Epoch: 25, Train Loss: 0.41221885326542435, Val Loss: 0.4123401972982619\n",
      "Epoch: 26, Train Loss: 0.40966869117338445, Val Loss: 0.3990351955095927\n",
      "Epoch: 27, Train Loss: 0.4032831742793699, Val Loss: 0.38795941405826145\n",
      "\n",
      "Best validation loss: 0.3825887242952983\n",
      "\n",
      "Saving best model for epoch: 28\n",
      "\n",
      "Epoch: 28, Train Loss: 0.40032847885844075, Val Loss: 0.3825887242952983\n",
      "Epoch: 29, Train Loss: 0.40008159264733517, Val Loss: 0.46100857853889465\n",
      "\n",
      "Best validation loss: 0.37308136290974087\n",
      "\n",
      "Saving best model for epoch: 30\n",
      "\n",
      "Epoch: 30, Train Loss: 0.4109600363652917, Val Loss: 0.37308136290974087\n",
      "Epoch: 31, Train Loss: 0.392169698506971, Val Loss: 0.3754204942120446\n",
      "Epoch: 32, Train Loss: 0.40239287969432297, Val Loss: 0.3788727952374352\n",
      "Epoch: 33, Train Loss: 0.3895652343200732, Val Loss: 0.38245147466659546\n",
      "\n",
      "Best validation loss: 0.3707968095938365\n",
      "\n",
      "Saving best model for epoch: 34\n",
      "\n",
      "Epoch: 34, Train Loss: 0.3836441794528237, Val Loss: 0.3707968095938365\n",
      "Epoch: 35, Train Loss: 0.37635847812966455, Val Loss: 0.37471138106452095\n",
      "Epoch: 36, Train Loss: 0.38095895738541324, Val Loss: 0.38635188341140747\n",
      "Epoch: 37, Train Loss: 0.3793177540543713, Val Loss: 0.3966781364546882\n",
      "\n",
      "Best validation loss: 0.35627642936176723\n",
      "\n",
      "Saving best model for epoch: 38\n",
      "\n",
      "Epoch: 38, Train Loss: 0.3775486416077312, Val Loss: 0.35627642936176723\n",
      "Epoch: 39, Train Loss: 0.3655985574933547, Val Loss: 0.3582596249050564\n",
      "Epoch: 40, Train Loss: 0.3734882381520694, Val Loss: 0.3650703728199005\n",
      "Epoch: 41, Train Loss: 0.3714395380472835, Val Loss: 0.37956118914816117\n",
      "Epoch: 42, Train Loss: 0.3680604334119, Val Loss: 0.38498607609007096\n",
      "Epoch: 43, Train Loss: 0.3665811234637152, Val Loss: 0.3608768151866065\n",
      "Epoch: 44, Train Loss: 0.36422206140771696, Val Loss: 0.3657256033685472\n",
      "Epoch: 45, Train Loss: 0.35615707990489426, Val Loss: 0.37880317701233757\n",
      "\n",
      "Best validation loss: 0.3554139965110355\n",
      "\n",
      "Saving best model for epoch: 46\n",
      "\n",
      "Epoch: 46, Train Loss: 0.3565250416345234, Val Loss: 0.3554139965110355\n",
      "Epoch: 47, Train Loss: 0.3600554760498337, Val Loss: 0.386321034696367\n",
      "Epoch: 48, Train Loss: 0.3597362948746621, Val Loss: 0.3685785432656606\n",
      "Epoch: 49, Train Loss: 0.36002458745165716, Val Loss: 0.36695491274197894\n",
      "\n",
      "Best validation loss: 0.34615372286902535\n",
      "\n",
      "Saving best model for epoch: 50\n",
      "\n",
      "Epoch: 50, Train Loss: 0.3553713182859783, Val Loss: 0.34615372286902535\n",
      "Epoch: 51, Train Loss: 0.3410790428707871, Val Loss: 0.38335147500038147\n",
      "Epoch: 52, Train Loss: 0.3519941730589806, Val Loss: 0.36595048838191563\n",
      "\n",
      "Best validation loss: 0.34472912218835616\n",
      "\n",
      "Saving best model for epoch: 53\n",
      "\n",
      "Epoch: 53, Train Loss: 0.3483496532787251, Val Loss: 0.34472912218835616\n",
      "Epoch: 54, Train Loss: 0.33575295119345944, Val Loss: 0.3628160423702664\n",
      "Epoch: 55, Train Loss: 0.3323698615348792, Val Loss: 0.36663544840282863\n",
      "Epoch: 56, Train Loss: 0.34396701277811315, Val Loss: 0.3653123875459035\n",
      "Epoch: 57, Train Loss: 0.3375139668395248, Val Loss: 0.35524486170874703\n",
      "Epoch: 58, Train Loss: 0.33103607387482364, Val Loss: 0.3784196145004696\n",
      "Epoch: 59, Train Loss: 0.3432412488928324, Val Loss: 0.34806955191824174\n",
      "Epoch: 60, Train Loss: 0.3361291230856618, Val Loss: 0.3470052546925015\n",
      "Epoch: 61, Train Loss: 0.3332911846758444, Val Loss: 0.37709055344263714\n",
      "Epoch: 62, Train Loss: 0.3206717783137213, Val Loss: 0.37015340063307023\n",
      "Epoch: 63, Train Loss: 0.3271596309127687, Val Loss: 0.3546626071135203\n",
      "Epoch: 64, Train Loss: 0.3185428662013404, Val Loss: 0.35798047648535836\n",
      "\n",
      "Best validation loss: 0.3402613765663571\n",
      "\n",
      "Saving best model for epoch: 65\n",
      "\n",
      "Epoch: 65, Train Loss: 0.3235640256087991, Val Loss: 0.3402613765663571\n",
      "Epoch: 66, Train Loss: 0.32148655758628364, Val Loss: 0.3456044892470042\n",
      "Epoch: 67, Train Loss: 0.3256445734561244, Val Loss: 0.3498641186290317\n",
      "\n",
      "Best validation loss: 0.3313075602054596\n",
      "\n",
      "Saving best model for epoch: 68\n",
      "\n",
      "Epoch: 68, Train Loss: 0.31255059717576716, Val Loss: 0.3313075602054596\n",
      "Epoch: 69, Train Loss: 0.3228078793875779, Val Loss: 0.34433889720175004\n",
      "\n",
      "Best validation loss: 0.3294704622692532\n",
      "\n",
      "Saving best model for epoch: 70\n",
      "\n",
      "Epoch: 70, Train Loss: 0.31101456473145306, Val Loss: 0.3294704622692532\n",
      "Epoch: 71, Train Loss: 0.31419265854962264, Val Loss: 0.3464156885941823\n",
      "\n",
      "Best validation loss: 0.31266023384200203\n",
      "\n",
      "Saving best model for epoch: 72\n",
      "\n",
      "Epoch: 72, Train Loss: 0.3121909814167626, Val Loss: 0.31266023384200203\n",
      "Epoch: 73, Train Loss: 0.31758160832562027, Val Loss: 0.33721121152242023\n",
      "Epoch: 74, Train Loss: 0.3109755806530578, Val Loss: 0.3740635911623637\n",
      "Epoch: 75, Train Loss: 0.30719210320635687, Val Loss: 0.33581530716684127\n",
      "Epoch: 76, Train Loss: 0.30598129237754434, Val Loss: 0.3425389660729302\n",
      "Epoch: 77, Train Loss: 0.3102086267516583, Val Loss: 0.3402378757794698\n",
      "Epoch: 78, Train Loss: 0.308770224640641, Val Loss: 0.3342212968402439\n",
      "Epoch: 79, Train Loss: 0.29627574357805375, Val Loss: 0.3589646518230438\n",
      "\n",
      "Best validation loss: 0.312081903219223\n",
      "\n",
      "Saving best model for epoch: 80\n",
      "\n",
      "Epoch: 80, Train Loss: 0.307278326795071, Val Loss: 0.312081903219223\n",
      "Epoch: 81, Train Loss: 0.2981366349171989, Val Loss: 0.3398875461684333\n",
      "Epoch: 82, Train Loss: 0.29877416864980627, Val Loss: 0.3259879681799147\n",
      "Epoch: 83, Train Loss: 0.29529657337484483, Val Loss: 0.3320075141059028\n",
      "Epoch: 84, Train Loss: 0.29995725351043895, Val Loss: 0.3676798509226905\n",
      "Epoch: 85, Train Loss: 0.29358567750152154, Val Loss: 0.31690261761347455\n",
      "Epoch: 86, Train Loss: 0.28237024798423427, Val Loss: 0.3265542884667714\n",
      "Epoch: 87, Train Loss: 0.2859482231396663, Val Loss: 0.319373670551512\n",
      "Epoch: 88, Train Loss: 0.28834096729000913, Val Loss: 0.3361559576458401\n",
      "Epoch: 89, Train Loss: 0.27901674071444743, Val Loss: 0.3164020942317115\n",
      "Epoch: 90, Train Loss: 0.28139317318608487, Val Loss: 0.33304210835032994\n",
      "Epoch: 91, Train Loss: 0.28291006914422484, Val Loss: 0.332238366206487\n",
      "Epoch: 92, Train Loss: 0.2806960595182226, Val Loss: 0.3735903799533844\n",
      "Epoch: 93, Train Loss: 0.2781983627171456, Val Loss: 0.34184796611468\n",
      "Epoch: 94, Train Loss: 0.28822081847281394, Val Loss: 0.3228837963607576\n",
      "Epoch: 95, Train Loss: 0.2731211698507961, Val Loss: 0.3241814970970154\n",
      "\n",
      "Best validation loss: 0.30104393594794804\n",
      "\n",
      "Saving best model for epoch: 96\n",
      "\n",
      "Epoch: 96, Train Loss: 0.2815436228543897, Val Loss: 0.30104393594794804\n",
      "Epoch: 97, Train Loss: 0.2872668504714966, Val Loss: 0.3115484615166982\n",
      "Epoch: 98, Train Loss: 0.27531570667707467, Val Loss: 0.31314218375417924\n",
      "Epoch: 99, Train Loss: 0.2684306453300428, Val Loss: 0.34197792079713607\n",
      "Epoch: 100, Train Loss: 0.2764332422727271, Val Loss: 0.3353735672103034\n",
      "\n",
      "Best validation loss: 0.29147670169671375\n",
      "\n",
      "Saving best model for epoch: 101\n",
      "\n",
      "Epoch: 101, Train Loss: 0.26916826620132106, Val Loss: 0.29147670169671375\n",
      "Epoch: 102, Train Loss: 0.2750416447090197, Val Loss: 0.3168479667769538\n",
      "Epoch: 103, Train Loss: 0.26234053735491597, Val Loss: 0.3662272095680237\n",
      "Epoch: 104, Train Loss: 0.2801414586697953, Val Loss: 0.34879931807518005\n",
      "Epoch: 105, Train Loss: 0.26359511515762235, Val Loss: 0.33494973844952053\n",
      "Epoch: 106, Train Loss: 0.2590085521717615, Val Loss: 0.32278941406144035\n",
      "Epoch: 107, Train Loss: 0.26421720053576214, Val Loss: 0.3194914493295882\n",
      "Epoch: 108, Train Loss: 0.2603346126366265, Val Loss: 0.3164741165108151\n",
      "Epoch: 109, Train Loss: 0.2596927075823651, Val Loss: 0.3354411522547404\n",
      "Epoch: 110, Train Loss: 0.26123525447483303, Val Loss: 0.30836996767256\n",
      "Epoch: 111, Train Loss: 0.2549679160495348, Val Loss: 0.32083870636092293\n",
      "Epoch: 112, Train Loss: 0.2568848861169212, Val Loss: 0.3077244162559509\n",
      "Epoch: 113, Train Loss: 0.27376843404166307, Val Loss: 0.31266415781444973\n",
      "Epoch: 114, Train Loss: 0.25791342122645317, Val Loss: 0.33298178513844806\n",
      "Epoch: 115, Train Loss: 0.25172091548955894, Val Loss: 0.31400492787361145\n",
      "\n",
      "Best validation loss: 0.27749523686038124\n",
      "\n",
      "Saving best model for epoch: 116\n",
      "\n",
      "Epoch: 116, Train Loss: 0.2515368797356569, Val Loss: 0.27749523686038124\n",
      "Epoch: 117, Train Loss: 0.2534920731677285, Val Loss: 0.3187055107620027\n",
      "Epoch: 118, Train Loss: 0.25068156609806835, Val Loss: 0.3206156525346968\n",
      "Epoch: 119, Train Loss: 0.24997906386852264, Val Loss: 0.2949044555425644\n",
      "Epoch: 120, Train Loss: 0.2413268872076952, Val Loss: 0.31803270677725476\n",
      "Epoch: 121, Train Loss: 0.24625906574575207, Val Loss: 0.3150793645117018\n",
      "Epoch: 122, Train Loss: 0.2544874773372578, Val Loss: 0.2985101143519084\n",
      "Epoch: 123, Train Loss: 0.2345391773158991, Val Loss: 0.3278311789035797\n",
      "Epoch: 124, Train Loss: 0.24621436773221703, Val Loss: 0.29801847371790147\n",
      "Epoch: 125, Train Loss: 0.24044530527501168, Val Loss: 0.3018726160128911\n",
      "Epoch: 126, Train Loss: 0.23909584913827195, Val Loss: 0.312869605090883\n",
      "Epoch: 127, Train Loss: 0.2344018895226189, Val Loss: 0.30112310085031724\n",
      "Epoch: 128, Train Loss: 0.22825273041483723, Val Loss: 0.30845339265134597\n",
      "Epoch: 129, Train Loss: 0.24261099533944191, Val Loss: 0.29481742944982314\n",
      "Epoch: 130, Train Loss: 0.24778756470997124, Val Loss: 0.29682763417561847\n",
      "Epoch: 131, Train Loss: 0.24768932974791225, Val Loss: 0.29689670105775195\n",
      "Epoch: 132, Train Loss: 0.2387725286468675, Val Loss: 0.30729072127077317\n",
      "Epoch: 133, Train Loss: 0.236612201680111, Val Loss: 0.2792479892571767\n",
      "Epoch: 134, Train Loss: 0.2373723468825787, Val Loss: 0.29826830989784664\n",
      "Epoch: 135, Train Loss: 0.23081852090132388, Val Loss: 0.2910650223493576\n",
      "Epoch: 136, Train Loss: 0.22599904805044585, Val Loss: 0.3053147710031933\n",
      "Epoch: 137, Train Loss: 0.24062896491606026, Val Loss: 0.3178558349609375\n",
      "Epoch: 138, Train Loss: 0.22884017757222622, Val Loss: 0.2820340411530601\n",
      "\n",
      "Best validation loss: 0.2666335552930832\n",
      "\n",
      "Saving best model for epoch: 139\n",
      "\n",
      "Epoch: 139, Train Loss: 0.22670567620404158, Val Loss: 0.2666335552930832\n",
      "Epoch: 140, Train Loss: 0.22492732654643965, Val Loss: 0.3164141641722785\n",
      "Epoch: 141, Train Loss: 0.22883559216426896, Val Loss: 0.31454524397850037\n",
      "Epoch: 142, Train Loss: 0.22823268099676206, Val Loss: 0.2791287981801563\n",
      "Epoch: 143, Train Loss: 0.22297594187003147, Val Loss: 0.31168873773680794\n",
      "Epoch: 144, Train Loss: 0.22276786214943173, Val Loss: 0.29950370060073006\n",
      "Epoch: 145, Train Loss: 0.22294641701103765, Val Loss: 0.27874315943982864\n",
      "Epoch: 146, Train Loss: 0.22125808479665202, Val Loss: 0.31678741839196944\n",
      "Epoch: 147, Train Loss: 0.22912911740662176, Val Loss: 0.2814982881148656\n",
      "Epoch: 148, Train Loss: 0.2127959520756444, Val Loss: 0.2687247263060676\n",
      "Epoch: 149, Train Loss: 0.22013755123826523, Val Loss: 0.3021003653605779\n",
      "Epoch: 150, Train Loss: 0.21234536878293073, Val Loss: 0.29088082081741756\n",
      "Epoch: 151, Train Loss: 0.21759768556567688, Val Loss: 0.3244255814287398\n",
      "Epoch: 152, Train Loss: 0.21791475615169428, Val Loss: 0.2852328254116906\n",
      "Epoch: 153, Train Loss: 0.2111335938112645, Val Loss: 0.28076426188151044\n",
      "Epoch: 154, Train Loss: 0.217889399279522, Val Loss: 0.2814028263092041\n",
      "\n",
      "Best validation loss: 0.24451534946759543\n",
      "\n",
      "Saving best model for epoch: 155\n",
      "\n",
      "Epoch: 155, Train Loss: 0.21722899301897122, Val Loss: 0.24451534946759543\n",
      "Epoch: 156, Train Loss: 0.21542435724146758, Val Loss: 0.3029453224605984\n",
      "Epoch: 157, Train Loss: 0.21215013659830334, Val Loss: 0.28790969484382206\n",
      "Epoch: 158, Train Loss: 0.2010454122401491, Val Loss: 0.298588333858384\n",
      "Epoch: 159, Train Loss: 0.21327144397964962, Val Loss: 0.29236039022604626\n",
      "Epoch: 160, Train Loss: 0.2087572491433047, Val Loss: 0.29350461231337654\n",
      "Epoch: 161, Train Loss: 0.21187451321490203, Val Loss: 0.27751730548010933\n",
      "Epoch: 162, Train Loss: 0.20684275936476793, Val Loss: 0.29164418909284806\n",
      "Epoch: 163, Train Loss: 0.2017695200216921, Val Loss: 0.30014461113346946\n",
      "Epoch: 164, Train Loss: 0.19994629730906668, Val Loss: 0.29773277706570095\n",
      "Epoch: 165, Train Loss: 0.20527433952953242, Val Loss: 0.28409580886363983\n",
      "Epoch: 166, Train Loss: 0.19772317977268486, Val Loss: 0.27847421500417924\n",
      "Epoch: 167, Train Loss: 0.21201389537581913, Val Loss: 0.29058121310340035\n",
      "Epoch: 168, Train Loss: 0.20265023231129103, Val Loss: 0.29451024697886574\n",
      "Epoch: 169, Train Loss: 0.2116317147318321, Val Loss: 0.278762118683921\n",
      "Epoch: 170, Train Loss: 0.19866302053007898, Val Loss: 0.3169710371229384\n",
      "Epoch: 171, Train Loss: 0.2075997920164579, Val Loss: 0.2985127717256546\n",
      "Epoch: 172, Train Loss: 0.19978316769569734, Val Loss: 0.2727874136633343\n",
      "Epoch: 173, Train Loss: 0.18824558016620105, Val Loss: 0.30442608230643803\n",
      "Epoch: 174, Train Loss: 0.19793395254808135, Val Loss: 0.30132727490531075\n",
      "Epoch: 175, Train Loss: 0.1959112093229837, Val Loss: 0.29021584656503463\n",
      "Epoch: 176, Train Loss: 0.20001795303217973, Val Loss: 0.2790403548214171\n",
      "Epoch: 177, Train Loss: 0.2025080761766132, Val Loss: 0.2510749300320943\n",
      "Epoch: 178, Train Loss: 0.20039624966020825, Val Loss: 0.29301252133316463\n",
      "Epoch: 179, Train Loss: 0.18559245812364772, Val Loss: 0.2883684386809667\n",
      "Epoch: 180, Train Loss: 0.19625930412660672, Val Loss: 0.28978565169705284\n",
      "Epoch: 181, Train Loss: 0.19454469739259045, Val Loss: 0.275317014919387\n",
      "Epoch: 182, Train Loss: 0.18387582775535463, Val Loss: 0.2997936424281862\n",
      "Epoch: 183, Train Loss: 0.19850142234111134, Val Loss: 0.31936520006921554\n",
      "Epoch: 184, Train Loss: 0.18559348856723762, Val Loss: 0.2978082050879796\n",
      "Epoch: 185, Train Loss: 0.19087914547211007, Val Loss: 0.31664223472277325\n",
      "Epoch: 186, Train Loss: 0.18948130922604212, Val Loss: 0.29413462844159866\n",
      "Epoch: 187, Train Loss: 0.19116004999679856, Val Loss: 0.30530739492840236\n",
      "Epoch: 188, Train Loss: 0.18361700392222102, Val Loss: 0.301097830136617\n",
      "Epoch: 189, Train Loss: 0.18760528094783613, Val Loss: 0.2901947564548916\n",
      "Epoch: 190, Train Loss: 0.17653696663394758, Val Loss: 0.2924753576517105\n",
      "Epoch: 191, Train Loss: 0.19167306076122237, Val Loss: 0.30285795860820347\n",
      "Epoch: 192, Train Loss: 0.19332101969402046, Val Loss: 0.3047136283583111\n",
      "Epoch: 193, Train Loss: 0.18636421378277526, Val Loss: 0.27925064663092297\n",
      "Epoch: 194, Train Loss: 0.18900288207621513, Val Loss: 0.2859923773341709\n",
      "Epoch: 195, Train Loss: 0.17265828878064698, Val Loss: 0.2570720795128081\n",
      "Epoch: 196, Train Loss: 0.1836514336210263, Val Loss: 0.2587648547357983\n",
      "Epoch: 197, Train Loss: 0.17492585459464713, Val Loss: 0.2528448949257533\n",
      "Epoch: 198, Train Loss: 0.17750781633054155, Val Loss: 0.2741881079143948\n",
      "Epoch: 199, Train Loss: 0.17237195259408106, Val Loss: 0.31162334647443557\n",
      "Epoch: 200, Train Loss: 0.17119668509009517, Val Loss: 0.2758386466238234\n",
      "Epoch: 201, Train Loss: 0.17450456443843962, Val Loss: 0.2811763667398029\n",
      "Epoch: 202, Train Loss: 0.17608690450463113, Val Loss: 0.28053174581792617\n",
      "Epoch: 203, Train Loss: 0.179762236302412, Val Loss: 0.2654457787672679\n",
      "Epoch: 204, Train Loss: 0.1774609931096246, Val Loss: 0.2569821940528022\n",
      "Epoch: 205, Train Loss: 0.1691482973438275, Val Loss: 0.2978391779793633\n",
      "Epoch: 206, Train Loss: 0.17326430948097496, Val Loss: 0.28277605606449974\n",
      "Epoch: 207, Train Loss: 0.17150122502559348, Val Loss: 0.26465819941626656\n",
      "Epoch: 208, Train Loss: 0.1807611671429646, Val Loss: 0.2999512470430798\n",
      "Epoch: 209, Train Loss: 0.16368215955510923, Val Loss: 0.295334044429991\n",
      "Epoch: 210, Train Loss: 0.17682752711108968, Val Loss: 0.26874567402733696\n",
      "Epoch: 211, Train Loss: 0.1743474905438061, Val Loss: 0.26344606114758384\n",
      "Epoch: 212, Train Loss: 0.1690366181391704, Val Loss: 0.2724109109905031\n",
      "Epoch: 213, Train Loss: 0.1727661837505389, Val Loss: 0.2977128591802385\n",
      "\n",
      "Best validation loss: 0.23260058959325156\n",
      "\n",
      "Saving best model for epoch: 214\n",
      "\n",
      "Epoch: 214, Train Loss: 0.176757716774186, Val Loss: 0.23260058959325156\n",
      "Epoch: 215, Train Loss: 0.1605767862329, Val Loss: 0.2768396619293425\n",
      "Epoch: 216, Train Loss: 0.1684978631666944, Val Loss: 0.2919024344947603\n",
      "Epoch: 217, Train Loss: 0.17704965683478344, Val Loss: 0.28105485604868996\n",
      "Epoch: 218, Train Loss: 0.16470568438496772, Val Loss: 0.2627580099635654\n",
      "Epoch: 219, Train Loss: 0.16214441819281517, Val Loss: 0.2904075351026323\n",
      "Epoch: 220, Train Loss: 0.1616369041649601, Val Loss: 0.25928043988015914\n",
      "Epoch: 221, Train Loss: 0.1629907299257532, Val Loss: 0.2863725142346488\n",
      "Epoch: 222, Train Loss: 0.16310457111913948, Val Loss: 0.2885151025321748\n",
      "Epoch: 223, Train Loss: 0.16732382906388632, Val Loss: 0.28558286858929527\n",
      "Epoch: 224, Train Loss: 0.16278637521251849, Val Loss: 0.2733544343047672\n",
      "Epoch: 225, Train Loss: 0.16240811126330232, Val Loss: 0.29197975662019515\n",
      "Epoch: 226, Train Loss: 0.1595062499370756, Val Loss: 0.27557362450493705\n",
      "Epoch: 227, Train Loss: 0.1549824546032314, Val Loss: 0.2972232285473082\n",
      "Epoch: 228, Train Loss: 0.16960823649092566, Val Loss: 0.258465975522995\n",
      "Epoch: 229, Train Loss: 0.15566461674774748, Val Loss: 0.2664075593153636\n",
      "Epoch: 230, Train Loss: 0.15475389568866055, Val Loss: 0.26875026524066925\n",
      "Epoch: 231, Train Loss: 0.1509825085537343, Val Loss: 0.2651684797472424\n",
      "Epoch: 232, Train Loss: 0.164458363116542, Val Loss: 0.24021902349260119\n",
      "Epoch: 233, Train Loss: 0.15499659212706965, Val Loss: 0.25014714565542007\n",
      "Epoch: 234, Train Loss: 0.15887467600876773, Val Loss: 0.28469060692522263\n",
      "Epoch: 235, Train Loss: 0.1513966718052007, Val Loss: 0.29184794425964355\n",
      "Epoch: 236, Train Loss: 0.1586727689527258, Val Loss: 0.25301365554332733\n",
      "Epoch: 237, Train Loss: 0.15818951241200482, Val Loss: 0.2754182716210683\n",
      "Epoch: 238, Train Loss: 0.15768942241615888, Val Loss: 0.2648206634653939\n",
      "Epoch: 239, Train Loss: 0.15225995845998389, Val Loss: 0.27359097533755833\n",
      "Epoch: 240, Train Loss: 0.15066896770385246, Val Loss: 0.25010327166981167\n",
      "Epoch: 241, Train Loss: 0.15043167486975464, Val Loss: 0.3146376262108485\n",
      "Epoch: 242, Train Loss: 0.15800292129758037, Val Loss: 0.2514891144302156\n",
      "Epoch: 243, Train Loss: 0.14882915198237082, Val Loss: 0.26533031463623047\n",
      "Epoch: 244, Train Loss: 0.1517881968919235, Val Loss: 0.28796429104275173\n",
      "Epoch: 245, Train Loss: 0.1637802642734745, Val Loss: 0.2668411566151513\n",
      "Epoch: 246, Train Loss: 0.15347991610252404, Val Loss: 0.2941994087563621\n",
      "Epoch: 247, Train Loss: 0.14287097638920893, Val Loss: 0.27374395231405896\n",
      "Epoch: 248, Train Loss: 0.15279164580227453, Val Loss: 0.2570605112446679\n",
      "Epoch: 249, Train Loss: 0.1447280454861967, Val Loss: 0.26982199483447605\n",
      "Epoch: 250, Train Loss: 0.1494543160064311, Val Loss: 0.2870100140571594\n",
      "Epoch: 251, Train Loss: 0.1523229449610167, Val Loss: 0.289296943280432\n",
      "Epoch: 252, Train Loss: 0.14931959272185458, Val Loss: 0.26588547395335305\n",
      "Epoch: 253, Train Loss: 0.1410427186402339, Val Loss: 0.2948998510837555\n",
      "Epoch: 254, Train Loss: 0.13450903068237666, Val Loss: 0.297828088204066\n",
      "Epoch: 255, Train Loss: 0.1462305477714237, Val Loss: 0.2679738832844628\n",
      "Epoch: 256, Train Loss: 0.15161005633918545, Val Loss: 0.2785661303334766\n",
      "Epoch: 257, Train Loss: 0.13160417488292803, Val Loss: 0.2457336982091268\n",
      "Epoch: 258, Train Loss: 0.14061490851867048, Val Loss: 0.29021532668007743\n",
      "Epoch: 259, Train Loss: 0.1382518597418749, Val Loss: 0.27681391603416866\n",
      "Epoch: 260, Train Loss: 0.13715705827255792, Val Loss: 0.28634826176696354\n",
      "Epoch: 261, Train Loss: 0.13440212541365926, Val Loss: 0.2780826124880049\n",
      "Epoch: 262, Train Loss: 0.14009414452917968, Val Loss: 0.28907612959543866\n",
      "Epoch: 263, Train Loss: 0.13742549656124053, Val Loss: 0.27369897067546844\n",
      "Epoch: 264, Train Loss: 0.1415607795307908, Val Loss: 0.2788640641503864\n",
      "Epoch: 265, Train Loss: 0.13658973725536203, Val Loss: 0.29455234441492295\n",
      "Epoch: 266, Train Loss: 0.145839250823365, Val Loss: 0.2722676893075307\n",
      "Epoch: 267, Train Loss: 0.13669283202365984, Val Loss: 0.2836651752392451\n",
      "Epoch: 268, Train Loss: 0.1399958808965321, Val Loss: 0.2606050819158554\n",
      "Epoch: 269, Train Loss: 0.12530071038422705, Val Loss: 0.31415920290682053\n",
      "Epoch: 270, Train Loss: 0.14679493632497667, Val Loss: 0.2570510374175178\n",
      "Epoch: 271, Train Loss: 0.14495445278626454, Val Loss: 0.26158982349766624\n",
      "Epoch: 272, Train Loss: 0.13276222810337815, Val Loss: 0.2856028626362483\n",
      "Epoch: 273, Train Loss: 0.13022552722994285, Val Loss: 0.27261799739466774\n",
      "Epoch: 274, Train Loss: 0.13702646718372272, Val Loss: 0.2709205117490556\n",
      "Epoch: 275, Train Loss: 0.1330256697875035, Val Loss: 0.2536561538775762\n",
      "Epoch: 276, Train Loss: 0.13216008203504961, Val Loss: 0.2969373944732878\n",
      "Epoch: 277, Train Loss: 0.14681401839361916, Val Loss: 0.2474216106865141\n",
      "Epoch: 278, Train Loss: 0.1314274756402909, Val Loss: 0.28694750865300495\n",
      "\n",
      "Best validation loss: 0.23024976253509521\n",
      "\n",
      "Saving best model for epoch: 279\n",
      "\n",
      "Epoch: 279, Train Loss: 0.13323139370996742, Val Loss: 0.23024976253509521\n",
      "Epoch: 280, Train Loss: 0.13254303131488304, Val Loss: 0.2611597379048665\n",
      "Epoch: 281, Train Loss: 0.13316891342401505, Val Loss: 0.28257260388798183\n",
      "Epoch: 282, Train Loss: 0.1329017273987396, Val Loss: 0.2569914162158966\n",
      "Epoch: 283, Train Loss: 0.12997115012022514, Val Loss: 0.25164488620228237\n",
      "Epoch: 284, Train Loss: 0.1337131003884575, Val Loss: 0.26998799873722923\n",
      "Epoch: 285, Train Loss: 0.12820455554542662, Val Loss: 0.25822387470139396\n",
      "Epoch: 286, Train Loss: 0.1232532587232469, Val Loss: 0.27345169087251026\n",
      "Epoch: 287, Train Loss: 0.13428794856690154, Val Loss: 0.2560612741443846\n",
      "Epoch: 288, Train Loss: 0.1271219026721731, Val Loss: 0.2540086309115092\n",
      "Epoch: 289, Train Loss: 0.11706512539258486, Val Loss: 0.2535131639904446\n",
      "Epoch: 290, Train Loss: 0.12523685560762127, Val Loss: 0.26557079123126137\n",
      "Epoch: 291, Train Loss: 0.127900017967707, Val Loss: 0.27378440896670025\n",
      "Epoch: 292, Train Loss: 0.13299208257017256, Val Loss: 0.25888589521249133\n",
      "Epoch: 293, Train Loss: 0.12206603379189214, Val Loss: 0.25938500463962555\n",
      "Epoch: 294, Train Loss: 0.12125665006003802, Val Loss: 0.2493610315852695\n",
      "Epoch: 295, Train Loss: 0.1199569554079937, Val Loss: 0.2762848867310418\n",
      "Epoch: 296, Train Loss: 0.11648126790606522, Val Loss: 0.289010700252321\n",
      "Epoch: 297, Train Loss: 0.13263559841279743, Val Loss: 0.28890710572401684\n",
      "Epoch: 298, Train Loss: 0.12394156632355496, Val Loss: 0.23065467013253105\n",
      "Epoch: 299, Train Loss: 0.11876018420804905, Val Loss: 0.29164618253707886\n",
      "Epoch: 300, Train Loss: 0.12363583467240576, Val Loss: 0.2754916565285789\n",
      "Epoch: 301, Train Loss: 0.12472848282961906, Val Loss: 0.28375614020559525\n",
      "Epoch: 302, Train Loss: 0.12210640424414526, Val Loss: 0.2607797450489468\n",
      "Epoch: 303, Train Loss: 0.11075044593101815, Val Loss: 0.26532988746960956\n",
      "Epoch: 304, Train Loss: 0.12148360842013661, Val Loss: 0.29733562303913963\n",
      "Epoch: 305, Train Loss: 0.1136580027073999, Val Loss: 0.2977508521742291\n",
      "Epoch: 306, Train Loss: 0.1191640498611746, Val Loss: 0.270753660135799\n",
      "Epoch: 307, Train Loss: 0.11865377581760853, Val Loss: 0.27722806566291386\n",
      "Epoch: 308, Train Loss: 0.12316872269103799, Val Loss: 0.23917154636647966\n",
      "Epoch: 309, Train Loss: 0.11870328629318672, Val Loss: 0.285516631272104\n",
      "Epoch: 310, Train Loss: 0.11879953742027283, Val Loss: 0.28416213062074447\n",
      "Epoch: 311, Train Loss: 0.11488439208721812, Val Loss: 0.28727801226907307\n",
      "Epoch: 312, Train Loss: 0.11315403718359862, Val Loss: 0.2975238578187095\n",
      "\n",
      "Best validation loss: 0.229283783170912\n",
      "\n",
      "Saving best model for epoch: 313\n",
      "\n",
      "Epoch: 313, Train Loss: 0.12304945139190819, Val Loss: 0.229283783170912\n",
      "Epoch: 314, Train Loss: 0.1137123195147967, Val Loss: 0.28452006147967446\n",
      "Epoch: 315, Train Loss: 0.10587000611084926, Val Loss: 0.29255729417006177\n",
      "Epoch: 316, Train Loss: 0.11920233311343796, Val Loss: 0.28135310113430023\n",
      "Epoch: 317, Train Loss: 0.1109072241130509, Val Loss: 0.2936610099342134\n",
      "Epoch: 318, Train Loss: 0.11814469565884976, Val Loss: 0.29139260947704315\n",
      "Epoch: 319, Train Loss: 0.11262079441471945, Val Loss: 0.24448215630319384\n",
      "Epoch: 320, Train Loss: 0.10230545954236502, Val Loss: 0.2715063856707679\n",
      "Epoch: 321, Train Loss: 0.10954847257537177, Val Loss: 0.26106365356180405\n",
      "Epoch: 322, Train Loss: 0.11682700101710573, Val Loss: 0.24548898306157854\n",
      "Epoch: 323, Train Loss: 0.10698004255566416, Val Loss: 0.2834996200270123\n",
      "Epoch: 324, Train Loss: 0.10819654808014253, Val Loss: 0.2907026343875461\n",
      "Epoch: 325, Train Loss: 0.1091980076854742, Val Loss: 0.24963240987724727\n",
      "Epoch: 326, Train Loss: 0.11981678339122218, Val Loss: 0.26240652137332493\n",
      "Epoch: 327, Train Loss: 0.10613674609155595, Val Loss: 0.2613169650236766\n",
      "Epoch: 328, Train Loss: 0.0984335502114477, Val Loss: 0.2751236706972122\n",
      "Epoch: 329, Train Loss: 0.11313571997835667, Val Loss: 0.2839711573388841\n",
      "Epoch: 330, Train Loss: 0.11346646841568282, Val Loss: 0.2705530375242233\n",
      "Epoch: 331, Train Loss: 0.10368102633311779, Val Loss: 0.28140609959761304\n",
      "Epoch: 332, Train Loss: 0.09836546963528742, Val Loss: 0.2749204734961192\n",
      "Epoch: 333, Train Loss: 0.10228602743790119, Val Loss: 0.3113829377624724\n",
      "Epoch: 334, Train Loss: 0.1113820261310173, Val Loss: 0.2792516615655687\n",
      "Epoch: 335, Train Loss: 0.0990380928203275, Val Loss: 0.30521859725316364\n",
      "Epoch: 336, Train Loss: 0.10358088133455831, Val Loss: 0.28454994327492183\n",
      "Epoch: 337, Train Loss: 0.11087153550190261, Val Loss: 0.28267350792884827\n",
      "Epoch: 338, Train Loss: 0.09660998545587063, Val Loss: 0.3249716791841719\n",
      "Epoch: 339, Train Loss: 0.10688705416985705, Val Loss: 0.26802508367432487\n",
      "Epoch: 340, Train Loss: 0.10939505568976644, Val Loss: 0.24354086816310883\n",
      "Epoch: 341, Train Loss: 0.09561712431568134, Val Loss: 0.28809718125396305\n",
      "Epoch: 342, Train Loss: 0.09835973162722739, Val Loss: 0.3054458068476783\n",
      "Epoch: 343, Train Loss: 0.09890848586830912, Val Loss: 0.2882123738527298\n",
      "Epoch: 344, Train Loss: 0.10521415236724328, Val Loss: 0.2671114355325699\n",
      "Epoch: 345, Train Loss: 0.09520841512498976, Val Loss: 0.29043736391597325\n",
      "Epoch: 346, Train Loss: 0.0871387685212908, Val Loss: 0.25915396875805324\n",
      "Epoch: 347, Train Loss: 0.0999460784317572, Val Loss: 0.29396236108409035\n",
      "Epoch: 348, Train Loss: 0.09622051698874824, Val Loss: 0.2884038786093394\n",
      "Epoch: 349, Train Loss: 0.10547663422324989, Val Loss: 0.2850804527600606\n",
      "Epoch: 350, Train Loss: 0.09176778746179387, Val Loss: 0.2858942531877094\n",
      "Epoch: 351, Train Loss: 0.0998890512163126, Val Loss: 0.30422939856847125\n",
      "Epoch: 352, Train Loss: 0.09529079162055933, Val Loss: 0.27370724081993103\n",
      "Epoch: 353, Train Loss: 0.09187298286941988, Val Loss: 0.28719978365633225\n",
      "Epoch: 354, Train Loss: 0.10097639786103103, Val Loss: 0.27119575440883636\n",
      "Epoch: 355, Train Loss: 0.09905412576243847, Val Loss: 0.2423714945713679\n",
      "Epoch: 356, Train Loss: 0.10348236178861389, Val Loss: 0.2634415378173192\n",
      "Epoch: 357, Train Loss: 0.08981626650577859, Val Loss: 0.29412761992878383\n",
      "Epoch: 358, Train Loss: 0.09898474114605144, Val Loss: 0.25169069485531914\n",
      "Epoch: 359, Train Loss: 0.09599673191580592, Val Loss: 0.2793326990471946\n",
      "Epoch: 360, Train Loss: 0.09031284105362772, Val Loss: 0.2778966873884201\n",
      "Epoch: 361, Train Loss: 0.09343112510027765, Val Loss: 0.25717486772272324\n",
      "Epoch: 362, Train Loss: 0.09826290515499024, Val Loss: 0.2559776140583886\n",
      "Epoch: 363, Train Loss: 0.09812063704940337, Val Loss: 0.27102916936079663\n",
      "Epoch: 364, Train Loss: 0.10052826750693442, Val Loss: 0.2641356603966819\n",
      "Epoch: 365, Train Loss: 0.09132439692657959, Val Loss: 0.3047949257824156\n",
      "Epoch: 366, Train Loss: 0.09052517616390428, Val Loss: 0.3144398911131753\n",
      "Epoch: 367, Train Loss: 0.09217562505244455, Val Loss: 0.2618681291739146\n",
      "Epoch: 368, Train Loss: 0.09114128315844867, Val Loss: 0.2867844303448995\n",
      "Epoch: 369, Train Loss: 0.09412509938584099, Val Loss: 0.25643883479966056\n",
      "Epoch: 370, Train Loss: 0.0825176295364582, Val Loss: 0.2766336268848843\n",
      "Epoch: 371, Train Loss: 0.09238920054292377, Val Loss: 0.27702392141024273\n",
      "Epoch: 372, Train Loss: 0.08573549651081049, Val Loss: 0.2947503543562359\n",
      "Epoch: 373, Train Loss: 0.08929110351431219, Val Loss: 0.24123600125312805\n",
      "Epoch: 374, Train Loss: 0.0825217598223988, Val Loss: 0.29093265864584184\n",
      "Epoch: 375, Train Loss: 0.08282978870446168, Val Loss: 0.2607284039258957\n",
      "Epoch: 376, Train Loss: 0.08833346624351755, Val Loss: 0.2834752880864673\n",
      "Epoch: 377, Train Loss: 0.08694821025563192, Val Loss: 0.2696091913514667\n",
      "Epoch: 378, Train Loss: 0.08399708585554286, Val Loss: 0.2768806964159012\n",
      "Epoch: 379, Train Loss: 0.08978903541176379, Val Loss: 0.26975620951917434\n",
      "Epoch: 380, Train Loss: 0.08279585270093212, Val Loss: 0.287804987695482\n",
      "Epoch: 381, Train Loss: 0.09014634127858319, Val Loss: 0.2790479792488946\n",
      "Epoch: 382, Train Loss: 0.08687374019358732, Val Loss: 0.2763191792700026\n",
      "Epoch: 383, Train Loss: 0.0903835580978967, Val Loss: 0.29050979846053654\n",
      "Epoch: 384, Train Loss: 0.08124528858291952, Val Loss: 0.31340014437834424\n",
      "Epoch: 385, Train Loss: 0.08666326157465766, Val Loss: 0.2898612568775813\n",
      "Epoch: 386, Train Loss: 0.09236134249198286, Val Loss: 0.29801299836900497\n",
      "Epoch: 387, Train Loss: 0.08025446020161049, Val Loss: 0.2695855249961217\n",
      "Epoch: 388, Train Loss: 0.09071885601063318, Val Loss: 0.2768099142445458\n",
      "Epoch: 389, Train Loss: 0.08774065603561039, Val Loss: 0.2607152462005615\n",
      "Epoch: 390, Train Loss: 0.09048372511810894, Val Loss: 0.27445421285099453\n",
      "Epoch: 391, Train Loss: 0.07797603481272354, Val Loss: 0.286260606514083\n",
      "Epoch: 392, Train Loss: 0.07575931515591809, Val Loss: 0.27693232148885727\n",
      "Epoch: 393, Train Loss: 0.0899195016797962, Val Loss: 0.2485654205083847\n",
      "Epoch: 394, Train Loss: 0.08448561766668211, Val Loss: 0.2557777563730876\n",
      "Epoch: 395, Train Loss: 0.08763370514387571, Val Loss: 0.2767612338066101\n",
      "Epoch: 396, Train Loss: 0.07941225274831434, Val Loss: 0.2786308940913942\n",
      "\n",
      "Best validation loss: 0.22747647927867043\n",
      "\n",
      "Saving best model for epoch: 397\n",
      "\n",
      "Epoch: 397, Train Loss: 0.08295073657284809, Val Loss: 0.22747647927867043\n",
      "Epoch: 398, Train Loss: 0.07309494129841841, Val Loss: 0.25824107229709625\n",
      "Epoch: 399, Train Loss: 0.08382638428313068, Val Loss: 0.2530255864063899\n",
      "Epoch: 400, Train Loss: 0.07882908552507811, Val Loss: 0.2852596359120475\n",
      "Epoch: 401, Train Loss: 0.078443471365903, Val Loss: 0.24786946177482605\n",
      "Epoch: 402, Train Loss: 0.0835588489057897, Val Loss: 0.29371169706185657\n",
      "Epoch: 403, Train Loss: 0.0762605337025244, Val Loss: 0.24982835435205036\n",
      "Epoch: 404, Train Loss: 0.08285242620902726, Val Loss: 0.3026413487063514\n",
      "Epoch: 405, Train Loss: 0.07614606279361097, Val Loss: 0.2623716973596149\n",
      "Epoch: 406, Train Loss: 0.07855958866450606, Val Loss: 0.2801527910762363\n",
      "Epoch: 407, Train Loss: 0.07186709077958064, Val Loss: 0.33587978614701164\n",
      "Epoch: 408, Train Loss: 0.07880635215323183, Val Loss: 0.28352664079931045\n",
      "Epoch: 409, Train Loss: 0.07041198559859886, Val Loss: 0.2909735788901647\n",
      "Epoch: 410, Train Loss: 0.08201839449473575, Val Loss: 0.281819889942805\n",
      "Epoch: 411, Train Loss: 0.07350588960077943, Val Loss: 0.2406635516219669\n",
      "Epoch: 412, Train Loss: 0.07302425679149507, Val Loss: 0.24544403619236416\n",
      "Epoch: 413, Train Loss: 0.0752147355977493, Val Loss: 0.2542044272025426\n",
      "Epoch: 414, Train Loss: 0.07655733435921654, Val Loss: 0.3167190303405126\n",
      "Epoch: 415, Train Loss: 0.08837111567771888, Val Loss: 0.28450053764714134\n",
      "Epoch: 416, Train Loss: 0.0823625734288104, Val Loss: 0.28270714150534737\n",
      "Epoch: 417, Train Loss: 0.07166755609685861, Val Loss: 0.2587042368120617\n",
      "Epoch: 418, Train Loss: 0.07409109326103065, Val Loss: 0.2973459197415246\n",
      "Epoch: 419, Train Loss: 0.0751111437389745, Val Loss: 0.30801695585250854\n",
      "Epoch: 420, Train Loss: 0.07485763834718662, Val Loss: 0.2647319518857532\n",
      "Epoch: 421, Train Loss: 0.07306263829239562, Val Loss: 0.24395804272757637\n",
      "Epoch: 422, Train Loss: 0.07619963003959082, Val Loss: 0.27751971284548443\n",
      "\n",
      "Best validation loss: 0.2260221110449897\n",
      "\n",
      "Saving best model for epoch: 423\n",
      "\n",
      "Epoch: 423, Train Loss: 0.07899455103693129, Val Loss: 0.2260221110449897\n",
      "Epoch: 424, Train Loss: 0.07273897035872634, Val Loss: 0.26587696704599595\n",
      "Epoch: 425, Train Loss: 0.06503057482216178, Val Loss: 0.2704969296852748\n",
      "Epoch: 426, Train Loss: 0.06870895823393064, Val Loss: 0.29270364344120026\n",
      "Epoch: 427, Train Loss: 0.07091909920490241, Val Loss: 0.3008470982313156\n",
      "Epoch: 428, Train Loss: 0.07384460388765304, Val Loss: 0.28017592761251664\n",
      "Epoch: 429, Train Loss: 0.07742960583654386, Val Loss: 0.26862308382987976\n",
      "Epoch: 430, Train Loss: 0.06950744998323012, Val Loss: 0.2739426576428943\n",
      "Epoch: 431, Train Loss: 0.07213584409107136, Val Loss: 0.28276315331459045\n",
      "Epoch: 432, Train Loss: 0.0685149570809135, Val Loss: 0.2805890705850389\n",
      "Epoch: 433, Train Loss: 0.06962419746891607, Val Loss: 0.2622102068530189\n",
      "Epoch: 434, Train Loss: 0.07393590806500067, Val Loss: 0.3021927972634633\n",
      "Epoch: 435, Train Loss: 0.06897590396619296, Val Loss: 0.2626875109142727\n",
      "Epoch: 436, Train Loss: 0.07690122207225877, Val Loss: 0.28419264654318493\n",
      "Epoch: 437, Train Loss: 0.06987770439326009, Val Loss: 0.2581694788402981\n",
      "Epoch: 438, Train Loss: 0.06555860048702246, Val Loss: 0.2625908719168769\n",
      "Epoch: 439, Train Loss: 0.06232033561490759, Val Loss: 0.27417096495628357\n",
      "Epoch: 440, Train Loss: 0.07472834198535243, Val Loss: 0.2721889118353526\n",
      "Epoch: 441, Train Loss: 0.0628588393660663, Val Loss: 0.33650769458876717\n",
      "Epoch: 442, Train Loss: 0.06170944077304647, Val Loss: 0.26212293406327564\n",
      "Epoch: 443, Train Loss: 0.0736392893227218, Val Loss: 0.27304985953701866\n",
      "Epoch: 444, Train Loss: 0.06833316345663765, Val Loss: 0.2637225207355287\n",
      "Epoch: 445, Train Loss: 0.07184244250242092, Val Loss: 0.29383108185397255\n",
      "Epoch: 446, Train Loss: 0.07061517604072637, Val Loss: 0.2900039570199119\n",
      "Epoch: 447, Train Loss: 0.06728347680896898, Val Loss: 0.32292023963398403\n",
      "Epoch: 448, Train Loss: 0.06709983089019227, Val Loss: 0.29650307032797074\n",
      "Epoch: 449, Train Loss: 0.06417273503692844, Val Loss: 0.2738907453086641\n",
      "Epoch: 450, Train Loss: 0.06815080792654919, Val Loss: 0.26775573525163865\n",
      "Epoch: 451, Train Loss: 0.06820186029506635, Val Loss: 0.2748538429538409\n",
      "Epoch: 452, Train Loss: 0.06646826573282102, Val Loss: 0.31162670420275795\n",
      "Epoch: 453, Train Loss: 0.06050080611500182, Val Loss: 0.27815059158537125\n",
      "Epoch: 454, Train Loss: 0.06714678937687149, Val Loss: 0.322960505882899\n",
      "Epoch: 455, Train Loss: 0.06553104338295097, Val Loss: 0.3264832744995753\n",
      "Epoch: 456, Train Loss: 0.07046066989674221, Val Loss: 0.2505960580375459\n",
      "Epoch: 457, Train Loss: 0.06003298822648918, Val Loss: 0.3195735646618737\n",
      "Epoch: 458, Train Loss: 0.06481257590311992, Val Loss: 0.31900586353407967\n",
      "Epoch: 459, Train Loss: 0.05314180919829803, Val Loss: 0.29477839999728733\n",
      "Epoch: 460, Train Loss: 0.06538708555170253, Val Loss: 0.30352753897507984\n",
      "Epoch: 461, Train Loss: 0.06512818204922767, Val Loss: 0.30577873024675584\n",
      "Epoch: 462, Train Loss: 0.06861475069971779, Val Loss: 0.29472604890664417\n",
      "Epoch: 463, Train Loss: 0.06658246576833197, Val Loss: 0.2553057289785809\n",
      "Epoch: 464, Train Loss: 0.05884228601957424, Val Loss: 0.3075785454776552\n",
      "Epoch: 465, Train Loss: 0.05811614637510686, Val Loss: 0.30823145972357857\n",
      "Epoch: 466, Train Loss: 0.06149225446242321, Val Loss: 0.3199928171104855\n",
      "Epoch: 467, Train Loss: 0.06602718624510343, Val Loss: 0.2707114617029826\n",
      "Epoch: 468, Train Loss: 0.06553026614121243, Val Loss: 0.2813107329938147\n",
      "Epoch: 469, Train Loss: 0.05689407116438769, Val Loss: 0.25155025720596313\n",
      "Epoch: 470, Train Loss: 0.061513549471391905, Val Loss: 0.27835797932412887\n",
      "Epoch: 471, Train Loss: 0.050775204195723504, Val Loss: 0.27648253904448616\n",
      "Epoch: 472, Train Loss: 0.06037467333806466, Val Loss: 0.32619018521573806\n",
      "Epoch: 473, Train Loss: 0.06803200733435305, Val Loss: 0.2820448709858788\n",
      "Epoch: 474, Train Loss: 0.05889697350514463, Val Loss: 0.3234031895796458\n",
      "Epoch: 475, Train Loss: 0.06203713191413804, Val Loss: 0.2670379761192534\n",
      "Epoch: 476, Train Loss: 0.05672314647537998, Val Loss: 0.23938013778792488\n",
      "Epoch: 477, Train Loss: 0.057867797669259056, Val Loss: 0.29652928229835296\n",
      "Epoch: 478, Train Loss: 0.05507894419133663, Val Loss: 0.28595926198694444\n",
      "Epoch: 479, Train Loss: 0.056220447915735876, Val Loss: 0.26535731057326\n",
      "Epoch: 480, Train Loss: 0.058361621019489406, Val Loss: 0.32443426218297744\n",
      "Epoch: 481, Train Loss: 0.05417041278951153, Val Loss: 0.3017874343527688\n",
      "Epoch: 482, Train Loss: 0.05700142272524064, Val Loss: 0.2670558426115248\n",
      "Epoch: 483, Train Loss: 0.061068019794323775, Val Loss: 0.2572770267724991\n",
      "Epoch: 484, Train Loss: 0.05669686989269302, Val Loss: 0.23858785463704002\n",
      "Epoch: 485, Train Loss: 0.05339103807471221, Val Loss: 0.2959492521153556\n",
      "Epoch: 486, Train Loss: 0.05595720238700698, Val Loss: 0.29195132354895276\n",
      "Epoch: 487, Train Loss: 0.05718384852892236, Val Loss: 0.29368797938028973\n",
      "Epoch: 488, Train Loss: 0.05591576058370403, Val Loss: 0.2753574500481288\n",
      "Epoch: 489, Train Loss: 0.05328429095399908, Val Loss: 0.2760874695248074\n",
      "Epoch: 490, Train Loss: 0.05407294272621976, Val Loss: 0.2736333906650543\n",
      "Epoch: 491, Train Loss: 0.060901778408243686, Val Loss: 0.29007943471272785\n",
      "Epoch: 492, Train Loss: 0.052729228747230544, Val Loss: 0.2560219284560945\n",
      "Epoch: 493, Train Loss: 0.059239585023326206, Val Loss: 0.2812594970067342\n",
      "Epoch: 494, Train Loss: 0.05836119544043948, Val Loss: 0.2785912834935718\n",
      "Epoch: 495, Train Loss: 0.05375840342780457, Val Loss: 0.2766404267814424\n",
      "Epoch: 496, Train Loss: 0.054227706870134874, Val Loss: 0.2737371623516083\n",
      "Epoch: 497, Train Loss: 0.05504696061716804, Val Loss: 0.3082665205001831\n",
      "Epoch: 498, Train Loss: 0.059655574079650116, Val Loss: 0.29852454364299774\n",
      "Epoch: 499, Train Loss: 0.05921216311428366, Val Loss: 0.2830000768105189\n",
      "Epoch: 500, Train Loss: 0.0516135557024162, Val Loss: 0.3006061679787106\n"
     ]
    }
   ],
   "source": [
    "train_loss,val_loss = train(distiller, train_loader,val_loader,epochs=500,lr=1e-4)\n",
    "# images shape:  <class 'transformers.image_processing_utils.BatchFeature'>\n",
    "# classes shape:  torch.Size([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\transformers\\feature_extraction_utils.py:90\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[0;32m     91\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m classes \u001b[39m=\u001b[39m classes\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m pred \u001b[39m=\u001b[39m student_vit(images)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Get the maximum predicted class\u001b[39;00m\n\u001b[0;32m     11\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\vit_pytorch\\distill.py:20\u001b[0m, in \u001b[0;36mDistillMixin.forward\u001b[1;34m(self, img, distill_token)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img, distill_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m     distilling \u001b[39m=\u001b[39m exists(distill_token)\n\u001b[1;32m---> 20\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_patch_embedding(img)\n\u001b[0;32m     21\u001b[0m     b, n, _ \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m     23\u001b[0m     cls_tokens \u001b[39m=\u001b[39m repeat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token, \u001b[39m'\u001b[39m\u001b[39m() n d -> b n d\u001b[39m\u001b[39m'\u001b[39m, b \u001b[39m=\u001b[39m b)\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\einops\\layers\\torch.py:14\u001b[0m, in \u001b[0;36mRearrange.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_for_scriptable_torch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recipe, \u001b[39minput\u001b[39;49m, reduction_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrearrange\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\einops\\_torch_specific.py:77\u001b[0m, in \u001b[0;36mapply_for_scriptable_torch\u001b[1;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_for_scriptable_torch\u001b[39m(recipe: TransformRecipe, tensor: torch\u001b[39m.\u001b[39mTensor, reduction_type: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m     75\u001b[0m     backend \u001b[39m=\u001b[39m TorchJitBackend\n\u001b[0;32m     76\u001b[0m     init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> 77\u001b[0m         _reconstruct_from_shape_uncached(recipe, backend\u001b[39m.\u001b[39;49mshape(tensor))\n\u001b[0;32m     78\u001b[0m     tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mreshape(tensor, init_shapes)\n\u001b[0;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(reduced_axes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\einops\\_torch_specific.py:66\u001b[0m, in \u001b[0;36mTorchJitBackend.shape\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshape\u001b[39m(x):\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mshape\n",
      "File \u001b[1;32mc:\\Users\\Jacopo\\miniconda3\\envs\\dl_refo_project\\lib\\site-packages\\transformers\\feature_extraction_utils.py:92\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[0;32m     91\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pred = student_vit(test_loader) # (2, 1000)\n",
    "student_vit.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss_=[]\n",
    "    for i,(images,classes) in enumerate(val_loader):\n",
    "        images = images.to(device)\n",
    "        classes = classes.to(device)\n",
    "        pred = student_vit(images)\n",
    "        \n",
    "        # Get the maximum predicted class\n",
    "        pred = pred.argmax(dim=1, keepdim=True)\n",
    "        print(pred)\n",
    "        \n",
    "    val_loss.append(np.mean(val_loss_))\n",
    "print(f'Test Loss: {val_loss[-1]}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DistillableViT class is identical to ViT except for how the forward pass is handled, \n",
    "# so you should be able to load the parameters back to ViT after you have completed distillation training.\n",
    "\n",
    "# TODO: It might be uselful if we want to use a custom vit\n",
    "student_vit = student_vit.to_vit()\n",
    "type(student_vit) # <class 'vit_pytorch.vit_pytorch.ViT'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save student vit\n",
    "torch.save(student_vit.state_dict(),f'jacoExperiments/distilled_student_vit.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
