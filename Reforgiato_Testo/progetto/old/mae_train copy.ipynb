{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.Black_footed_Albatross', '002.Laysan_Albatross', '003.Sooty_Albatross', '004.Groove_billed_Ani', '005.Crested_Auklet', '006.Least_Auklet', '007.Parakeet_Auklet', '008.Rhinoceros_Auklet', '009.Brewer_Blackbird', '010.Red_winged_Blackbird', '011.Rusty_Blackbird', '012.Yellow_headed_Blackbird', '013.Bobolink', '014.Indigo_Bunting', '015.Lazuli_Bunting', '016.Painted_Bunting', '017.Cardinal', '018.Spotted_Catbird', '019.Gray_Catbird', '020.Yellow_breasted_Chat', '021.Eastern_Towhee', '022.Chuck_will_Widow', '023.Brandt_Cormorant', '024.Red_faced_Cormorant', '025.Pelagic_Cormorant', '026.Bronzed_Cowbird', '027.Shiny_Cowbird', '028.Brown_Creeper', '029.American_Crow', '030.Fish_Crow', '031.Black_billed_Cuckoo', '032.Mangrove_Cuckoo', '033.Yellow_billed_Cuckoo', '034.Gray_crowned_Rosy_Finch', '035.Purple_Finch', '036.Northern_Flicker', '037.Acadian_Flycatcher', '038.Great_Crested_Flycatcher', '039.Least_Flycatcher', '040.Olive_sided_Flycatcher', '041.Scissor_tailed_Flycatcher', '042.Vermilion_Flycatcher', '043.Yellow_bellied_Flycatcher', '044.Frigatebird', '045.Northern_Fulmar', '046.Gadwall', '047.American_Goldfinch', '048.European_Goldfinch', '049.Boat_tailed_Grackle', '050.Eared_Grebe', '051.Horned_Grebe', '052.Pied_billed_Grebe', '053.Western_Grebe', '054.Blue_Grosbeak', '055.Evening_Grosbeak', '056.Pine_Grosbeak', '057.Rose_breasted_Grosbeak', '058.Pigeon_Guillemot', '059.California_Gull', '060.Glaucous_winged_Gull', '061.Heermann_Gull', '062.Herring_Gull', '063.Ivory_Gull', '064.Ring_billed_Gull', '065.Slaty_backed_Gull', '066.Western_Gull', '067.Anna_Hummingbird', '068.Ruby_throated_Hummingbird', '069.Rufous_Hummingbird', '070.Green_Violetear', '071.Long_tailed_Jaeger', '072.Pomarine_Jaeger', '073.Blue_Jay', '074.Florida_Jay', '075.Green_Jay', '076.Dark_eyed_Junco', '077.Tropical_Kingbird', '078.Gray_Kingbird', '079.Belted_Kingfisher', '080.Green_Kingfisher', '081.Pied_Kingfisher', '082.Ringed_Kingfisher', '083.White_breasted_Kingfisher', '084.Red_legged_Kittiwake', '085.Horned_Lark', '086.Pacific_Loon', '087.Mallard', '088.Western_Meadowlark', '089.Hooded_Merganser', '090.Red_breasted_Merganser', '091.Mockingbird', '092.Nighthawk', '093.Clark_Nutcracker', '094.White_breasted_Nuthatch', '095.Baltimore_Oriole', '096.Hooded_Oriole', '097.Orchard_Oriole', '098.Scott_Oriole', '099.Ovenbird', '100.Brown_Pelican', '101.White_Pelican', '102.Western_Wood_Pewee', '103.Sayornis', '104.American_Pipit', '105.Whip_poor_Will', '106.Horned_Puffin', '107.Common_Raven', '108.White_necked_Raven', '109.American_Redstart', '110.Geococcyx', '111.Loggerhead_Shrike', '112.Great_Grey_Shrike', '113.Baird_Sparrow', '114.Black_throated_Sparrow', '115.Brewer_Sparrow', '116.Chipping_Sparrow', '117.Clay_colored_Sparrow', '118.House_Sparrow', '119.Field_Sparrow', '120.Fox_Sparrow', '121.Grasshopper_Sparrow', '122.Harris_Sparrow', '123.Henslow_Sparrow', '124.Le_Conte_Sparrow', '125.Lincoln_Sparrow', '126.Nelson_Sharp_tailed_Sparrow', '127.Savannah_Sparrow', '128.Seaside_Sparrow', '129.Song_Sparrow', '130.Tree_Sparrow', '131.Vesper_Sparrow', '132.White_crowned_Sparrow', '133.White_throated_Sparrow', '134.Cape_Glossy_Starling', '135.Bank_Swallow', '136.Barn_Swallow', '137.Cliff_Swallow', '138.Tree_Swallow', '139.Scarlet_Tanager', '140.Summer_Tanager', '141.Artic_Tern', '142.Black_Tern', '143.Caspian_Tern', '144.Common_Tern', '145.Elegant_Tern', '146.Forsters_Tern', '147.Least_Tern', '148.Green_tailed_Towhee', '149.Brown_Thrasher', '150.Sage_Thrasher', '151.Black_capped_Vireo', '152.Blue_headed_Vireo', '153.Philadelphia_Vireo', '154.Red_eyed_Vireo', '155.Warbling_Vireo', '156.White_eyed_Vireo', '157.Yellow_throated_Vireo', '158.Bay_breasted_Warbler', '159.Black_and_white_Warbler', '160.Black_throated_Blue_Warbler', '161.Blue_winged_Warbler', '162.Canada_Warbler', '163.Cape_May_Warbler', '164.Cerulean_Warbler', '165.Chestnut_sided_Warbler', '166.Golden_winged_Warbler', '167.Hooded_Warbler', '168.Kentucky_Warbler', '169.Magnolia_Warbler', '170.Mourning_Warbler', '171.Myrtle_Warbler', '172.Nashville_Warbler', '173.Orange_crowned_Warbler', '174.Palm_Warbler', '175.Pine_Warbler', '176.Prairie_Warbler', '177.Prothonotary_Warbler', '178.Swainson_Warbler', '179.Tennessee_Warbler', '180.Wilson_Warbler', '181.Worm_eating_Warbler', '182.Yellow_Warbler', '183.Northern_Waterthrush', '184.Louisiana_Waterthrush', '185.Bohemian_Waxwing', '186.Cedar_Waxwing', '187.American_Three_toed_Woodpecker', '188.Pileated_Woodpecker', '189.Red_bellied_Woodpecker', '190.Red_cockaded_Woodpecker', '191.Red_headed_Woodpecker', '192.Downy_Woodpecker', '193.Bewick_Wren', '194.Cactus_Wren', '195.Carolina_Wren', '196.House_Wren', '197.Marsh_Wren', '198.Rock_Wren', '199.Winter_Wren', '200.Common_Yellowthroat']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>./CUB_200_2011/images/001.Black_footed_Albatro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        class  \\\n",
       "0  001.Black_footed_Albatross   \n",
       "1  001.Black_footed_Albatross   \n",
       "2  001.Black_footed_Albatross   \n",
       "3  001.Black_footed_Albatross   \n",
       "4  001.Black_footed_Albatross   \n",
       "\n",
       "                                                path  \n",
       "0  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "1  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "2  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "3  ./CUB_200_2011/images/001.Black_footed_Albatro...  \n",
       "4  ./CUB_200_2011/images/001.Black_footed_Albatro...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all files from the folder CUB_200_2011 and assign the subfolder as a class\n",
    "#the subfolder name is the class name\n",
    "\n",
    "path = './CUB_200_2011/images/'\n",
    "classes = os.listdir(path)\n",
    "classes.sort()\n",
    "print(classes)\n",
    "#read all files from the subfolders\n",
    "\n",
    "data = []\n",
    "for i in range(len(classes)):\n",
    "    folder = os.path.join(path,classes[i])\n",
    "    files = os.listdir(folder)\n",
    "    for j in range(len(files)):\n",
    "        data.append([classes[i],os.path.join(folder,files[j])])\n",
    "\n",
    "#convert the list to a dataframe\n",
    "df = pd.DataFrame(data,columns=['class','path'])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumenations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#transformations for train and validation\n",
    "transform = A.Compose([\n",
    "    A.Resize(256,256),\n",
    "    ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the Dataset class\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.images = self.df['path'].values\n",
    "        self.classes = self.df['class'].values\n",
    "        self.classes = np.array([classes.index(i) for i in self.classes])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = self.images[idx]\n",
    "        image = plt.imread(image)\n",
    "        #image = np.transpose(image,(2,0,1))\n",
    "        #if is grayscale, convert to rgb\n",
    "        if image.shape[0] == 1:\n",
    "            image = np.repeat(image,3,0)\n",
    "        #cast to float and normalize\n",
    "        image = image.astype(np.float32)\n",
    "        image = image/255.0 \n",
    "        image =  transform(image=image)['image']\n",
    "        class_ = self.classes[idx]\n",
    "        class_ = torch.tensor(class_,dtype=torch.long)\n",
    "        image = image.type(torch.FloatTensor)\n",
    "        if image.shape[0] == 1:\n",
    "            image = torch.repeat_interleave(image,3,0)\n",
    "        return image,class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataloaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(BirdDataset(train_df),batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(BirdDataset(val_df),batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(BirdDataset(test_df),batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT, MAE\n",
    "from mae_bellino import MAE as MAEBELLINO\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "v = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048\n",
    ")\n",
    "\n",
    "v = v.to(device)\n",
    "#import pretrained weights from the paper\n",
    "\n",
    "mae_bellino = MAEBELLINO(\n",
    "    encoder = v,\n",
    "    masking_ratio = 0.75,   # the paper recommended 75% masked patches\n",
    "    decoder_dim = 512,      # paper showed good results with just 512\n",
    "    decoder_depth = 6       # anywhere from 1 to 8\n",
    ")\n",
    "\n",
    "mae_bellino = mae_bellino.to(device)\n",
    "mae = MAE(\n",
    "    encoder = v,\n",
    "    masking_ratio = 0.75,   # the paper recommended 75% masked patches\n",
    "    decoder_dim = 512,      # paper showed good results with just 512\n",
    "    decoder_depth = 6       # anywhere from 1 to 8\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "mae = mae.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "def train(model,train_loader,val_loader,epochs=10,lr=1e-2):\n",
    "\n",
    "    mae_bellino = MAEBELLINO(\n",
    "        encoder = v,\n",
    "        masking_ratio = 0.75,   # the paper recommended 75% masked patches\n",
    "        decoder_dim = 512,      # paper showed good results with just 512\n",
    "        decoder_depth = 6       # anywhere from 1 to 8\n",
    "    )\n",
    "    mae_bellino = mae_bellino.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_ = []\n",
    "        val_loss_ = []\n",
    "        for i,(images,classes) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            classes = classes.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_.append(loss.item())\n",
    "            #every 100 batches, print the loss\n",
    "            if i%80 == 0:\n",
    "                #transfer weights from model to maebellino model\n",
    "                mae_bellino.load_state_dict(model.state_dict())\n",
    "                #get the output image\n",
    "                output = mae_bellino(images) \n",
    "                o = output[0].cpu().detach().numpy().transpose(1,2,0)\n",
    "                #apply relu\n",
    "                o = np.maximum(o,0)\n",
    "                o = np.minimum(o,1)\n",
    "                plt.imsave(f'outputs/epoch_{epoch+1}_batch_{i}.png',o)\n",
    "        train_loss.append(np.mean(train_loss_))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i,(images,classes) in enumerate(val_loader):\n",
    "                \n",
    "                images = images.to(device)\n",
    "                classes = classes.to(device)\n",
    "                loss = model(images)\n",
    "                val_loss_.append(loss.item())\n",
    "                \n",
    "            val_loss.append(np.mean(val_loss_))\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}')\n",
    "        #save the last output image on the disk\n",
    "        #save the model\n",
    "        torch.save(model.state_dict(),f'last.pth')\n",
    "        \n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.0767658321676239, Val Loss: 0.0518376429454755\n",
      "Epoch: 2, Train Loss: 0.05049062111430754, Val Loss: 0.049360179421255146\n",
      "Epoch: 3, Train Loss: 0.048815903995754355, Val Loss: 0.04760433742934364\n",
      "Epoch: 4, Train Loss: 0.04752320779282284, Val Loss: 0.04716150566809258\n",
      "Epoch: 5, Train Loss: 0.046860323989984846, Val Loss: 0.04711972052326142\n",
      "Epoch: 6, Train Loss: 0.04668159530324451, Val Loss: 0.047814717390022035\n",
      "Epoch: 7, Train Loss: 0.046062051551416516, Val Loss: 0.04687287080717289\n",
      "Epoch: 8, Train Loss: 0.04562902234699893, Val Loss: 0.0456416071787224\n",
      "Epoch: 9, Train Loss: 0.04537453663030292, Val Loss: 0.04624925236517595\n",
      "Epoch: 10, Train Loss: 0.045330212120029884, Val Loss: 0.04715892221084086\n",
      "Epoch: 11, Train Loss: 0.04504454727047833, Val Loss: 0.046525340337874525\n",
      "Epoch: 12, Train Loss: 0.044771281678734696, Val Loss: 0.04506942544574455\n",
      "Epoch: 13, Train Loss: 0.04418137640755434, Val Loss: 0.04506376256250729\n",
      "Epoch: 14, Train Loss: 0.0439798513514195, Val Loss: 0.045049584596970324\n",
      "Epoch: 15, Train Loss: 0.04353612606508373, Val Loss: 0.04415962229467044\n",
      "Epoch: 16, Train Loss: 0.043159157905917046, Val Loss: 0.04389803618255813\n",
      "Epoch: 17, Train Loss: 0.0427254399292613, Val Loss: 0.0441224474017903\n",
      "Epoch: 18, Train Loss: 0.04225712210744998, Val Loss: 0.04307269947445494\n",
      "Epoch: 19, Train Loss: 0.041773826018038945, Val Loss: 0.04342407793185468\n",
      "Epoch: 20, Train Loss: 0.04103624674302163, Val Loss: 0.04258734685510902\n",
      "Epoch: 21, Train Loss: 0.040712125524404945, Val Loss: 0.04169698632559029\n",
      "Epoch: 22, Train Loss: 0.039774084830719786, Val Loss: 0.04071849537224083\n",
      "Epoch: 23, Train Loss: 0.03942975637866026, Val Loss: 0.04030242849583343\n",
      "Epoch: 24, Train Loss: 0.03894338593423619, Val Loss: 0.04054070305142362\n",
      "Epoch: 25, Train Loss: 0.038175165404612986, Val Loss: 0.03975667648206828\n",
      "Epoch: 26, Train Loss: 0.03794129225188645, Val Loss: 0.039454863233081366\n",
      "Epoch: 27, Train Loss: 0.03760036531741084, Val Loss: 0.039249235827286365\n",
      "Epoch: 28, Train Loss: 0.03712941840748792, Val Loss: 0.038385097600393374\n",
      "Epoch: 29, Train Loss: 0.036664030285773135, Val Loss: 0.038465710609393605\n",
      "Epoch: 30, Train Loss: 0.0365362364277875, Val Loss: 0.038666565050134216\n",
      "Epoch: 31, Train Loss: 0.036124107494981864, Val Loss: 0.03779643851379722\n",
      "Epoch: 32, Train Loss: 0.03574631226217469, Val Loss: 0.038172628556899096\n",
      "Epoch: 33, Train Loss: 0.03570448611152627, Val Loss: 0.037491242480227505\n",
      "Epoch: 34, Train Loss: 0.03534598562519176, Val Loss: 0.03772191572302984\n",
      "Epoch: 35, Train Loss: 0.035170305295358015, Val Loss: 0.03768353705625918\n",
      "Epoch: 36, Train Loss: 0.03499354788307416, Val Loss: 0.03690356125076443\n",
      "Epoch: 37, Train Loss: 0.03477509009999113, Val Loss: 0.03710056681943647\n",
      "Epoch: 38, Train Loss: 0.03454141142004627, Val Loss: 0.03692872519210234\n",
      "Epoch: 39, Train Loss: 0.034431114891480846, Val Loss: 0.03742855100624137\n",
      "Epoch: 40, Train Loss: 0.034327800405385384, Val Loss: 0.036376741762130946\n",
      "Epoch: 41, Train Loss: 0.03409070149458692, Val Loss: 0.038054730080194395\n",
      "Epoch: 42, Train Loss: 0.034089784508855164, Val Loss: 0.03649098845988007\n",
      "Epoch: 43, Train Loss: 0.03406114750778524, Val Loss: 0.03647512980436875\n",
      "Epoch: 44, Train Loss: 0.033721151094788966, Val Loss: 0.0367136737331748\n",
      "Epoch: 45, Train Loss: 0.033456764069496325, Val Loss: 0.03655323302531141\n",
      "Epoch: 46, Train Loss: 0.033250471995354206, Val Loss: 0.03682926426625858\n",
      "Epoch: 47, Train Loss: 0.03332191053383305, Val Loss: 0.0360001812894213\n",
      "Epoch: 48, Train Loss: 0.03320856076681008, Val Loss: 0.035963185289401116\n",
      "Epoch: 49, Train Loss: 0.03280101233888102, Val Loss: 0.03893914564623166\n",
      "Epoch: 50, Train Loss: 0.03305232418716838, Val Loss: 0.03667111900019444\n",
      "Epoch: 51, Train Loss: 0.0329556147709994, Val Loss: 0.03609367191665253\n",
      "Epoch: 52, Train Loss: 0.03279089572902579, Val Loss: 0.036943511579627705\n",
      "Epoch: 53, Train Loss: 0.032418852135286495, Val Loss: 0.036054601103572524\n",
      "Epoch: 54, Train Loss: 0.03262041906542066, Val Loss: 0.035811448453972904\n",
      "Epoch: 55, Train Loss: 0.03252392623699823, Val Loss: 0.03619932195455846\n",
      "Epoch: 56, Train Loss: 0.032390047097578645, Val Loss: 0.03552277574031535\n",
      "Epoch: 57, Train Loss: 0.0321425961693591, Val Loss: 0.0370416939574278\n",
      "Epoch: 58, Train Loss: 0.03193572600829905, Val Loss: 0.03569256978393611\n",
      "Epoch: 59, Train Loss: 0.03199708176050651, Val Loss: 0.0350233683583595\n",
      "Epoch: 60, Train Loss: 0.03166926355827284, Val Loss: 0.035733544425565304\n",
      "Epoch: 61, Train Loss: 0.03166904770848104, Val Loss: 0.03556772113098937\n",
      "Epoch: 62, Train Loss: 0.03175548093109313, Val Loss: 0.03496469285781101\n",
      "Epoch: 63, Train Loss: 0.03131793305214684, Val Loss: 0.035581724292014616\n",
      "Epoch: 64, Train Loss: 0.031230162114883633, Val Loss: 0.03548096710721315\n",
      "Epoch: 65, Train Loss: 0.031080021574240873, Val Loss: 0.035438023886438144\n",
      "Epoch: 66, Train Loss: 0.031146759881590633, Val Loss: 0.03519895095062458\n",
      "Epoch: 67, Train Loss: 0.030781224756738394, Val Loss: 0.03548002465612302\n",
      "Epoch: 68, Train Loss: 0.031012847151417854, Val Loss: 0.03648205082547867\n",
      "Epoch: 69, Train Loss: 0.03087821990346252, Val Loss: 0.03543009223826861\n",
      "Epoch: 70, Train Loss: 0.030851129671337747, Val Loss: 0.03550645821112192\n",
      "Epoch: 71, Train Loss: 0.030778623541217236, Val Loss: 0.03508905619698561\n",
      "Epoch: 72, Train Loss: 0.030409380272662236, Val Loss: 0.03489462647713342\n",
      "Epoch: 73, Train Loss: 0.030491748185417916, Val Loss: 0.03567511725665654\n",
      "Epoch: 74, Train Loss: 0.03041682337900087, Val Loss: 0.034891601793973125\n",
      "Epoch: 75, Train Loss: 0.030228332695314438, Val Loss: 0.03465681510456538\n",
      "Epoch: 76, Train Loss: 0.030268778345721253, Val Loss: 0.03503757377423472\n",
      "Epoch: 77, Train Loss: 0.030250150074992897, Val Loss: 0.035001956826170626\n",
      "Epoch: 78, Train Loss: 0.029972491675388006, Val Loss: 0.03442892099146621\n",
      "Epoch: 79, Train Loss: 0.030167535764275718, Val Loss: 0.034931005689047154\n",
      "Epoch: 80, Train Loss: 0.029856413409461158, Val Loss: 0.034724234271857696\n",
      "Epoch: 81, Train Loss: 0.029701179426195004, Val Loss: 0.03463109181720322\n",
      "Epoch: 82, Train Loss: 0.029441383844872906, Val Loss: 0.03483408545854233\n",
      "Epoch: 83, Train Loss: 0.02954045442027687, Val Loss: 0.0346370687139994\n",
      "Epoch: 84, Train Loss: 0.02951362503739863, Val Loss: 0.03490853506945452\n",
      "Epoch: 85, Train Loss: 0.029454804207119396, Val Loss: 0.03558815035463895\n",
      "Epoch: 86, Train Loss: 0.029446018303318295, Val Loss: 0.03578120324823816\n",
      "Epoch: 87, Train Loss: 0.029466204203160134, Val Loss: 0.03447613249517093\n",
      "Epoch: 88, Train Loss: 0.02913505707520171, Val Loss: 0.03427925677496498\n",
      "Epoch: 89, Train Loss: 0.02901626877897119, Val Loss: 0.03489708766278069\n",
      "Epoch: 90, Train Loss: 0.028845507075410275, Val Loss: 0.0347200711652384\n",
      "Epoch: 91, Train Loss: 0.028930533677339554, Val Loss: 0.0346884898588819\n",
      "Epoch: 92, Train Loss: 0.028778716369326843, Val Loss: 0.03497922010088371\n",
      "Epoch: 93, Train Loss: 0.028734952724396676, Val Loss: 0.03472289679792978\n",
      "Epoch: 94, Train Loss: 0.02858195031630033, Val Loss: 0.03488421168620304\n",
      "Epoch: 95, Train Loss: 0.02859763887916076, Val Loss: 0.035057066466217326\n",
      "Epoch: 96, Train Loss: 0.028457746276695093, Val Loss: 0.03435870686198695\n",
      "Epoch: 97, Train Loss: 0.028433606387668495, Val Loss: 0.03447823706319777\n",
      "Epoch: 98, Train Loss: 0.02837012645835846, Val Loss: 0.03435442991302175\n",
      "Epoch: 99, Train Loss: 0.02810497490405026, Val Loss: 0.034230263856381685\n",
      "Epoch: 100, Train Loss: 0.028289905457236503, Val Loss: 0.03460019351756674\n",
      "Epoch: 101, Train Loss: 0.02827354981879688, Val Loss: 0.034484180921720245\n",
      "Epoch: 102, Train Loss: 0.028063732814990867, Val Loss: 0.03511084534101567\n",
      "Epoch: 103, Train Loss: 0.027916905462299867, Val Loss: 0.0341497439164984\n",
      "Epoch: 104, Train Loss: 0.02786735190331178, Val Loss: 0.03402260725654788\n",
      "Epoch: 105, Train Loss: 0.027773302948197066, Val Loss: 0.03437624055639667\n",
      "Epoch: 106, Train Loss: 0.027725420217424378, Val Loss: 0.034520298889759236\n",
      "Epoch: 107, Train Loss: 0.027607748691403006, Val Loss: 0.03409500101233943\n",
      "Epoch: 108, Train Loss: 0.02753377869188533, Val Loss: 0.03456815254827172\n",
      "Epoch: 109, Train Loss: 0.02759055215549671, Val Loss: 0.03404314268254122\n",
      "Epoch: 110, Train Loss: 0.027445531114778023, Val Loss: 0.03413253190722001\n",
      "Epoch: 111, Train Loss: 0.027391855336599432, Val Loss: 0.033758231122993816\n",
      "Epoch: 112, Train Loss: 0.027290017055978968, Val Loss: 0.03442514575702154\n",
      "Epoch: 113, Train Loss: 0.027207514961739464, Val Loss: 0.03449025567052728\n",
      "Epoch: 114, Train Loss: 0.027015847019788066, Val Loss: 0.03397927079665459\n",
      "Epoch: 115, Train Loss: 0.02699903738527101, Val Loss: 0.03420625148751473\n",
      "Epoch: 116, Train Loss: 0.026849893119059883, Val Loss: 0.03399497437906467\n",
      "Epoch: 117, Train Loss: 0.026879903951154676, Val Loss: 0.03427456136224633\n",
      "Epoch: 118, Train Loss: 0.026674356603584554, Val Loss: 0.03382798832036176\n",
      "Epoch: 119, Train Loss: 0.02694698927087425, Val Loss: 0.03432517082004224\n",
      "Epoch: 120, Train Loss: 0.026732749132029082, Val Loss: 0.03383352771653968\n",
      "Epoch: 121, Train Loss: 0.026783284277386837, Val Loss: 0.033949300189013196\n",
      "Epoch: 122, Train Loss: 0.026493125417569683, Val Loss: 0.033595852901117274\n",
      "Epoch: 123, Train Loss: 0.026457014825131932, Val Loss: 0.03398752496656725\n",
      "Epoch: 124, Train Loss: 0.026483975303501398, Val Loss: 0.03483431105959719\n",
      "Epoch: 125, Train Loss: 0.026321181206632467, Val Loss: 0.03414699158994323\n",
      "Epoch: 126, Train Loss: 0.026205975696507653, Val Loss: 0.03432147992553852\n",
      "Epoch: 127, Train Loss: 0.026137142663158603, Val Loss: 0.03412586564229707\n",
      "Epoch: 128, Train Loss: 0.026215760739741183, Val Loss: 0.033954220634522074\n",
      "Epoch: 129, Train Loss: 0.02612228173431072, Val Loss: 0.03367939684540033\n",
      "Epoch: 130, Train Loss: 0.02604794103289181, Val Loss: 0.033649568903749275\n",
      "Epoch: 131, Train Loss: 0.02600700720173891, Val Loss: 0.03390746416871326\n",
      "Epoch: 132, Train Loss: 0.02588004345800412, Val Loss: 0.033525564313187435\n",
      "Epoch: 133, Train Loss: 0.025752232958546888, Val Loss: 0.03542152905868272\n",
      "Epoch: 134, Train Loss: 0.02600933611392975, Val Loss: 0.03408129249652058\n",
      "Epoch: 135, Train Loss: 0.025685730559940813, Val Loss: 0.033888802200682085\n",
      "Epoch: 136, Train Loss: 0.02565577308947252, Val Loss: 0.033586842844546855\n",
      "Epoch: 137, Train Loss: 0.025683143207708658, Val Loss: 0.03392938395046582\n",
      "Epoch: 138, Train Loss: 0.02543376894841381, Val Loss: 0.03371649177082009\n",
      "Epoch: 139, Train Loss: 0.025643258765971257, Val Loss: 0.03351639293260494\n",
      "Epoch: 140, Train Loss: 0.025541652988453033, Val Loss: 0.034055065047942985\n",
      "Epoch: 141, Train Loss: 0.025524387810915962, Val Loss: 0.03373715919205698\n",
      "Epoch: 142, Train Loss: 0.02529208598484806, Val Loss: 0.0333742919551619\n",
      "Epoch: 143, Train Loss: 0.025458869508514972, Val Loss: 0.03423082678563009\n",
      "Epoch: 144, Train Loss: 0.025197330964887042, Val Loss: 0.03419552339335619\n",
      "Epoch: 145, Train Loss: 0.025340870516059006, Val Loss: 0.033843714759637746\n",
      "Epoch: 146, Train Loss: 0.025224882252987916, Val Loss: 0.033896997381569975\n",
      "Epoch: 147, Train Loss: 0.02511983189210927, Val Loss: 0.0340780306260212\n",
      "Epoch: 148, Train Loss: 0.025034169252109478, Val Loss: 0.03428292156086635\n",
      "Epoch: 149, Train Loss: 0.02479609386537666, Val Loss: 0.033898934027401066\n",
      "Epoch: 150, Train Loss: 0.02482860391343928, Val Loss: 0.033730669183877564\n",
      "Epoch: 151, Train Loss: 0.024884068904349865, Val Loss: 0.03374519203868458\n",
      "Epoch: 152, Train Loss: 0.024867106560554544, Val Loss: 0.03365317451120433\n",
      "Epoch: 153, Train Loss: 0.024721937190930722, Val Loss: 0.033322471604382585\n",
      "Epoch: 154, Train Loss: 0.02485637804019754, Val Loss: 0.03382603915692386\n",
      "Epoch: 155, Train Loss: 0.02462552429050586, Val Loss: 0.03355699654479148\n",
      "Epoch: 156, Train Loss: 0.024527406655409832, Val Loss: 0.03357195724748959\n",
      "Epoch: 157, Train Loss: 0.02464612971170474, Val Loss: 0.033067154208734885\n",
      "Epoch: 158, Train Loss: 0.024539873368990748, Val Loss: 0.033898268447462784\n",
      "Epoch: 159, Train Loss: 0.024534427762126266, Val Loss: 0.03347991621595318\n",
      "Epoch: 160, Train Loss: 0.024542712732770686, Val Loss: 0.033669793473209364\n",
      "Epoch: 161, Train Loss: 0.024281280233635235, Val Loss: 0.033780334880417685\n",
      "Epoch: 162, Train Loss: 0.0243100017759065, Val Loss: 0.03342309747270103\n",
      "Epoch: 163, Train Loss: 0.024294852326482028, Val Loss: 0.033583448189547505\n",
      "Epoch: 164, Train Loss: 0.02414493325316363, Val Loss: 0.03324445013461982\n",
      "Epoch: 165, Train Loss: 0.02410879002694609, Val Loss: 0.03348078100421166\n",
      "Epoch: 166, Train Loss: 0.02402383712775437, Val Loss: 0.03384718025829327\n",
      "Epoch: 167, Train Loss: 0.024049728982529398, Val Loss: 0.03383495363454193\n",
      "Epoch: 168, Train Loss: 0.023990266516378495, Val Loss: 0.03339022432722277\n",
      "Epoch: 169, Train Loss: 0.023950312160334344, Val Loss: 0.03364404993352749\n",
      "Epoch: 170, Train Loss: 0.023891592157562657, Val Loss: 0.033797913294096114\n",
      "Epoch: 171, Train Loss: 0.023853502117918205, Val Loss: 0.03399588189766568\n",
      "Epoch: 172, Train Loss: 0.023813157002994065, Val Loss: 0.03326251761893095\n",
      "Epoch: 173, Train Loss: 0.02374387251102697, Val Loss: 0.03394193726323419\n",
      "Epoch: 174, Train Loss: 0.023708775396837647, Val Loss: 0.033344589236934306\n",
      "Epoch: 175, Train Loss: 0.023603008320097322, Val Loss: 0.03347555350638547\n",
      "Epoch: 176, Train Loss: 0.023623603751283077, Val Loss: 0.03374507671253661\n",
      "Epoch: 177, Train Loss: 0.02353863376271674, Val Loss: 0.0333384914260547\n",
      "Epoch: 178, Train Loss: 0.023465940859010916, Val Loss: 0.03376506065349963\n",
      "Epoch: 179, Train Loss: 0.02347122417305107, Val Loss: 0.03417601920980013\n",
      "Epoch: 180, Train Loss: 0.023378125281404642, Val Loss: 0.03369711023771157\n",
      "Epoch: 181, Train Loss: 0.0234253070656589, Val Loss: 0.03325730495912544\n",
      "Epoch: 182, Train Loss: 0.023406784267212124, Val Loss: 0.03342592956150992\n",
      "Epoch: 183, Train Loss: 0.023271345000682495, Val Loss: 0.03339856409988666\n",
      "Epoch: 184, Train Loss: 0.023196541207005917, Val Loss: 0.0334234062771676\n",
      "Epoch: 185, Train Loss: 0.023134930442103137, Val Loss: 0.0332397492286765\n",
      "Epoch: 186, Train Loss: 0.023115337027189462, Val Loss: 0.03341267632036391\n",
      "Epoch: 187, Train Loss: 0.02322932168194172, Val Loss: 0.03361395849072832\n",
      "Epoch: 188, Train Loss: 0.02305155861096741, Val Loss: 0.03348671318173914\n",
      "Epoch: 189, Train Loss: 0.023045584638366253, Val Loss: 0.03343463002435737\n",
      "Epoch: 190, Train Loss: 0.02288780658174369, Val Loss: 0.03352365017694942\n",
      "Epoch: 191, Train Loss: 0.022869618702285245, Val Loss: 0.03355599006922063\n",
      "Epoch: 192, Train Loss: 0.02285935343006405, Val Loss: 0.0333273679302153\n",
      "Epoch: 193, Train Loss: 0.022821448644686302, Val Loss: 0.033722180888940724\n",
      "Epoch: 194, Train Loss: 0.022778606839414874, Val Loss: 0.03393082420136464\n",
      "Epoch: 195, Train Loss: 0.022782007924880894, Val Loss: 0.03353863556758832\n",
      "Epoch: 196, Train Loss: 0.022745160977908616, Val Loss: 0.033766356044275275\n",
      "Epoch: 197, Train Loss: 0.02264767691642993, Val Loss: 0.03352312058589216\n",
      "Epoch: 198, Train Loss: 0.022617563325122506, Val Loss: 0.03314453657021967\n",
      "Epoch: 199, Train Loss: 0.022490452769828046, Val Loss: 0.03392460484499649\n",
      "Epoch: 200, Train Loss: 0.022511485076146358, Val Loss: 0.03347765310209686\n",
      "Epoch: 201, Train Loss: 0.022530033450447403, Val Loss: 0.033264333824232474\n",
      "Epoch: 202, Train Loss: 0.022480070435094757, Val Loss: 0.0329618328172019\n",
      "Epoch: 203, Train Loss: 0.022462995981371376, Val Loss: 0.03337404232914165\n",
      "Epoch: 204, Train Loss: 0.022355712539816306, Val Loss: 0.03372624336536658\n",
      "Epoch: 205, Train Loss: 0.02230920066851807, Val Loss: 0.03379820284070605\n",
      "Epoch: 206, Train Loss: 0.022289210160941642, Val Loss: 0.03451307878782183\n",
      "Epoch: 207, Train Loss: 0.022230528166785963, Val Loss: 0.03376474348291502\n",
      "Epoch: 208, Train Loss: 0.02224124643667522, Val Loss: 0.032958205000071204\n",
      "Epoch: 209, Train Loss: 0.02224670384095003, Val Loss: 0.03334122743078713\n",
      "Epoch: 210, Train Loss: 0.022159429269021976, Val Loss: 0.03322259172544641\n",
      "Epoch: 211, Train Loss: 0.02202864824123365, Val Loss: 0.033673095990414334\n",
      "Epoch: 212, Train Loss: 0.021892836335178276, Val Loss: 0.03373609208580801\n",
      "Epoch: 213, Train Loss: 0.021944187832128068, Val Loss: 0.033408085215773625\n",
      "Epoch: 214, Train Loss: 0.0218928089657388, Val Loss: 0.0333158380146754\n",
      "Epoch: 215, Train Loss: 0.02187717470372001, Val Loss: 0.03415000206648798\n",
      "Epoch: 216, Train Loss: 0.021813037611891406, Val Loss: 0.033897613301494364\n",
      "Epoch: 217, Train Loss: 0.02182645406731862, Val Loss: 0.03349269410374306\n",
      "Epoch: 218, Train Loss: 0.021862106001146644, Val Loss: 0.03369061096366179\n",
      "Epoch: 219, Train Loss: 0.021734166771994304, Val Loss: 0.033818348468739094\n",
      "Epoch: 220, Train Loss: 0.02163131550049125, Val Loss: 0.0335653084724889\n",
      "Epoch: 221, Train Loss: 0.021658960690374596, Val Loss: 0.03379958695968834\n",
      "Epoch: 222, Train Loss: 0.021718430530153595, Val Loss: 0.033509084276097306\n",
      "Epoch: 223, Train Loss: 0.021606002792210904, Val Loss: 0.03349260785380157\n",
      "Epoch: 224, Train Loss: 0.021673178896150093, Val Loss: 0.033483283531110165\n",
      "Epoch: 225, Train Loss: 0.021495016404206597, Val Loss: 0.0337737238160887\n",
      "Epoch: 226, Train Loss: 0.021356939132940973, Val Loss: 0.03328919413862592\n",
      "Epoch: 227, Train Loss: 0.02131795022297272, Val Loss: 0.03302428989781667\n",
      "Epoch: 228, Train Loss: 0.021285291889414065, Val Loss: 0.03346102384818812\n",
      "Epoch: 229, Train Loss: 0.021316306251180122, Val Loss: 0.03327057622688807\n",
      "Epoch: 230, Train Loss: 0.021287071058858124, Val Loss: 0.03346551048679877\n",
      "Epoch: 231, Train Loss: 0.021258611930534244, Val Loss: 0.03344395204242003\n",
      "Epoch: 232, Train Loss: 0.021173358231909193, Val Loss: 0.033902479682938524\n",
      "Epoch: 233, Train Loss: 0.02122335158035917, Val Loss: 0.03344232025477341\n",
      "Epoch: 234, Train Loss: 0.021237811855024706, Val Loss: 0.03408574398165032\n",
      "Epoch: 235, Train Loss: 0.021007762562830823, Val Loss: 0.03357072775962494\n",
      "Epoch: 236, Train Loss: 0.021115856904189212, Val Loss: 0.03364602244317026\n",
      "Epoch: 237, Train Loss: 0.021117109748124446, Val Loss: 0.03358164574887793\n",
      "Epoch: 238, Train Loss: 0.021132129976541687, Val Loss: 0.033992978128588806\n",
      "Epoch: 239, Train Loss: 0.020984855914583146, Val Loss: 0.03377021059899007\n",
      "Epoch: 240, Train Loss: 0.02091099354793649, Val Loss: 0.03340276480668177\n",
      "Epoch: 241, Train Loss: 0.020900449157155797, Val Loss: 0.03340667656670183\n",
      "Epoch: 242, Train Loss: 0.020777159404025365, Val Loss: 0.03449060019673938\n",
      "Epoch: 243, Train Loss: 0.020886536148945027, Val Loss: 0.033856079424336806\n",
      "Epoch: 244, Train Loss: 0.02074267016329124, Val Loss: 0.033785144151267356\n",
      "Epoch: 245, Train Loss: 0.02073352294974029, Val Loss: 0.03387513321082471\n",
      "Epoch: 246, Train Loss: 0.020670830599679533, Val Loss: 0.033397550079024445\n",
      "Epoch: 247, Train Loss: 0.02080746005896984, Val Loss: 0.033743089317517766\n",
      "Epoch: 248, Train Loss: 0.020700160417926765, Val Loss: 0.033864677189138985\n",
      "Epoch: 249, Train Loss: 0.020682955474466466, Val Loss: 0.033629863507161704\n",
      "Epoch: 250, Train Loss: 0.020456208425210946, Val Loss: 0.0337878831709593\n",
      "Epoch: 251, Train Loss: 0.02057188356658286, Val Loss: 0.033724478698509225\n",
      "Epoch: 252, Train Loss: 0.020394005013582437, Val Loss: 0.033615171388423036\n",
      "Epoch: 253, Train Loss: 0.020529933548058873, Val Loss: 0.034271062083416064\n",
      "Epoch: 254, Train Loss: 0.02036857065523706, Val Loss: 0.033745841865064734\n",
      "Epoch: 255, Train Loss: 0.02040249226670081, Val Loss: 0.03358389798678079\n",
      "Epoch: 256, Train Loss: 0.020360900333814196, Val Loss: 0.03363874433088606\n",
      "Epoch: 257, Train Loss: 0.020284798972987395, Val Loss: 0.033618741305702825\n",
      "Epoch: 258, Train Loss: 0.020301107901258993, Val Loss: 0.034162252604708836\n",
      "Epoch: 259, Train Loss: 0.020167210806255876, Val Loss: 0.03382809706411119\n",
      "Epoch: 260, Train Loss: 0.020105923091898025, Val Loss: 0.034216449406566256\n",
      "Epoch: 261, Train Loss: 0.020200149199562305, Val Loss: 0.03356297145266149\n",
      "Epoch: 262, Train Loss: 0.020129859627035084, Val Loss: 0.034010733509341536\n",
      "Epoch: 263, Train Loss: 0.020088905200235926, Val Loss: 0.03386735363658202\n",
      "Epoch: 264, Train Loss: 0.020107055536739655, Val Loss: 0.03372272168743914\n",
      "Epoch: 265, Train Loss: 0.01994423139682513, Val Loss: 0.03381962485422017\n",
      "Epoch: 266, Train Loss: 0.019982570644183937, Val Loss: 0.03337721930722059\n",
      "Epoch: 267, Train Loss: 0.019916101520628494, Val Loss: 0.033477911536218756\n",
      "Epoch: 268, Train Loss: 0.01986418513904752, Val Loss: 0.03354327325409247\n",
      "Epoch: 269, Train Loss: 0.01995572601287168, Val Loss: 0.033967097258289995\n",
      "Epoch: 270, Train Loss: 0.019885045198044914, Val Loss: 0.033638561270752196\n",
      "Epoch: 271, Train Loss: 0.01979305731700891, Val Loss: 0.033685928257971495\n",
      "Epoch: 272, Train Loss: 0.019805826075075163, Val Loss: 0.03388082573853307\n",
      "Epoch: 273, Train Loss: 0.019751127408738484, Val Loss: 0.033853152893104796\n",
      "Epoch: 274, Train Loss: 0.01974211763432723, Val Loss: 0.03414309466794386\n",
      "Epoch: 275, Train Loss: 0.01966485703537651, Val Loss: 0.033754781283185646\n",
      "Epoch: 276, Train Loss: 0.019647272043245827, Val Loss: 0.03386708543146566\n",
      "Epoch: 277, Train Loss: 0.019523847844230675, Val Loss: 0.033805080263291376\n",
      "Epoch: 278, Train Loss: 0.019502508473756203, Val Loss: 0.03418149267149679\n",
      "Epoch: 279, Train Loss: 0.019563262761270597, Val Loss: 0.034140223510942216\n",
      "Epoch: 280, Train Loss: 0.019491072511300445, Val Loss: 0.03417344546040236\n",
      "Epoch: 281, Train Loss: 0.019404243070127097, Val Loss: 0.033887943963251885\n",
      "Epoch: 282, Train Loss: 0.01945272746685324, Val Loss: 0.03402271036500648\n",
      "Epoch: 283, Train Loss: 0.019470553124970678, Val Loss: 0.03374058047656791\n",
      "Epoch: 284, Train Loss: 0.019286819936912822, Val Loss: 0.03379611063243474\n",
      "Epoch: 285, Train Loss: 0.01932104306208696, Val Loss: 0.03429973575333923\n",
      "Epoch: 286, Train Loss: 0.019324285238665546, Val Loss: 0.034125588976351894\n",
      "Epoch: 287, Train Loss: 0.019291850566564097, Val Loss: 0.03433588775411501\n",
      "Epoch: 288, Train Loss: 0.019321530451603487, Val Loss: 0.03442034313171092\n",
      "Epoch: 289, Train Loss: 0.019230982916177076, Val Loss: 0.03398264008495262\n",
      "Epoch: 290, Train Loss: 0.019140338657376497, Val Loss: 0.034505039690283394\n",
      "Epoch: 291, Train Loss: 0.019128338260165717, Val Loss: 0.033995095166867065\n",
      "Epoch: 292, Train Loss: 0.019030559150959855, Val Loss: 0.03379869030125565\n",
      "Epoch: 293, Train Loss: 0.01911533143671261, Val Loss: 0.0347334272307107\n",
      "Epoch: 294, Train Loss: 0.019030063190118615, Val Loss: 0.03417513827335531\n",
      "Epoch: 295, Train Loss: 0.01905782308658364, Val Loss: 0.034385193202455165\n",
      "Epoch: 296, Train Loss: 0.01893441560401944, Val Loss: 0.03417181283614393\n",
      "Epoch: 297, Train Loss: 0.018870069580231557, Val Loss: 0.03396690595831912\n",
      "Epoch: 298, Train Loss: 0.018914024759936383, Val Loss: 0.03388070908643431\n",
      "Epoch: 299, Train Loss: 0.01879140814584759, Val Loss: 0.03427607986002655\n",
      "Epoch: 300, Train Loss: 0.01875814165667457, Val Loss: 0.03440950160561982\n",
      "Epoch: 301, Train Loss: 0.018870199262953788, Val Loss: 0.03426328542942213\n",
      "Epoch: 302, Train Loss: 0.01874409469783748, Val Loss: 0.03477857473416854\n",
      "Epoch: 303, Train Loss: 0.01859559730026808, Val Loss: 0.03408286354299319\n",
      "Epoch: 304, Train Loss: 0.018741385898773825, Val Loss: 0.03428382369673858\n",
      "Epoch: 305, Train Loss: 0.01859752225523846, Val Loss: 0.03414415647038969\n",
      "Epoch: 306, Train Loss: 0.01857034718594119, Val Loss: 0.03417770583498276\n",
      "Epoch: 307, Train Loss: 0.018610883044410435, Val Loss: 0.03456422446642892\n",
      "Epoch: 308, Train Loss: 0.018555007298808483, Val Loss: 0.03437408457620669\n",
      "Epoch: 309, Train Loss: 0.018577992476096723, Val Loss: 0.034239038335696116\n",
      "Epoch: 310, Train Loss: 0.018460962272217593, Val Loss: 0.034484585415635066\n",
      "Epoch: 311, Train Loss: 0.018404547515783017, Val Loss: 0.03429817439893545\n",
      "Epoch: 312, Train Loss: 0.01840470999605575, Val Loss: 0.03453819128542633\n",
      "Epoch: 313, Train Loss: 0.01844342695193965, Val Loss: 0.03444310721262532\n",
      "Epoch: 314, Train Loss: 0.018403013136190503, Val Loss: 0.034579521502857494\n",
      "Epoch: 315, Train Loss: 0.01836940923595201, Val Loss: 0.033887300198361024\n",
      "Epoch: 316, Train Loss: 0.018293998636289548, Val Loss: 0.03382603210097147\n",
      "Epoch: 317, Train Loss: 0.018210086809723813, Val Loss: 0.0344687306274802\n",
      "Epoch: 318, Train Loss: 0.018183194134400178, Val Loss: 0.03387906705423937\n",
      "Epoch: 319, Train Loss: 0.018160905955375124, Val Loss: 0.033990117200350355\n",
      "Epoch: 320, Train Loss: 0.018154885535033704, Val Loss: 0.03413329091933319\n",
      "Epoch: 321, Train Loss: 0.01814156630218534, Val Loss: 0.034110882351080236\n",
      "Epoch: 322, Train Loss: 0.018137318774726303, Val Loss: 0.03426799105511884\n",
      "Epoch: 323, Train Loss: 0.01809702789750329, Val Loss: 0.03455970584714817\n",
      "Epoch: 324, Train Loss: 0.01807707884364714, Val Loss: 0.034006638120923\n",
      "Epoch: 325, Train Loss: 0.018029602118215318, Val Loss: 0.03431311024005635\n",
      "Epoch: 326, Train Loss: 0.018002034934490162, Val Loss: 0.03442222722884962\n",
      "Epoch: 327, Train Loss: 0.01797423759767375, Val Loss: 0.034718354690378\n",
      "Epoch: 328, Train Loss: 0.017863168578468643, Val Loss: 0.034678529316591\n",
      "Epoch: 329, Train Loss: 0.017884790607317652, Val Loss: 0.034591848488455104\n",
      "Epoch: 330, Train Loss: 0.017862428711743046, Val Loss: 0.03448745638321517\n",
      "Epoch: 331, Train Loss: 0.017782315258647048, Val Loss: 0.034433726426529684\n",
      "Epoch: 332, Train Loss: 0.01776083816490057, Val Loss: 0.03475711504155296\n",
      "Epoch: 333, Train Loss: 0.017705821872183707, Val Loss: 0.03463556808498451\n",
      "Epoch: 334, Train Loss: 0.017731430504265858, Val Loss: 0.03463076765380674\n",
      "Epoch: 335, Train Loss: 0.017696182928205927, Val Loss: 0.03438380758388568\n",
      "Epoch: 336, Train Loss: 0.01769151684228268, Val Loss: 0.03502178842485961\n",
      "Epoch: 337, Train Loss: 0.0176612536908301, Val Loss: 0.034507165994432015\n",
      "Epoch: 338, Train Loss: 0.017571236772494295, Val Loss: 0.03453197314451307\n",
      "Epoch: 339, Train Loss: 0.017563534411207095, Val Loss: 0.03417165413246316\n",
      "Epoch: 340, Train Loss: 0.017529562973985606, Val Loss: 0.034589922450230286\n",
      "Epoch: 341, Train Loss: 0.01753160115370874, Val Loss: 0.0345470485893094\n",
      "Epoch: 342, Train Loss: 0.01744536347134765, Val Loss: 0.034693417281417525\n",
      "Epoch: 343, Train Loss: 0.01739350016726086, Val Loss: 0.0345133874817925\n",
      "Epoch: 344, Train Loss: 0.01736752106413498, Val Loss: 0.03464559917101415\n",
      "Epoch: 345, Train Loss: 0.01748042045776748, Val Loss: 0.03435243175254535\n",
      "Epoch: 346, Train Loss: 0.017329944408167215, Val Loss: 0.03424258706933361\n",
      "Epoch: 347, Train Loss: 0.017333958292048486, Val Loss: 0.0348805958026294\n",
      "Epoch: 348, Train Loss: 0.017242894509917724, Val Loss: 0.03429904409636885\n",
      "Epoch: 349, Train Loss: 0.01726687740005742, Val Loss: 0.0344082422680774\n",
      "Epoch: 350, Train Loss: 0.017251000621245575, Val Loss: 0.03508465225666256\n",
      "Epoch: 351, Train Loss: 0.017273971615595966, Val Loss: 0.034623383433889536\n",
      "Epoch: 352, Train Loss: 0.017216059545023463, Val Loss: 0.03543321507335719\n",
      "Epoch: 353, Train Loss: 0.01717340522958755, Val Loss: 0.03475017523614027\n",
      "Epoch: 354, Train Loss: 0.017121530135983002, Val Loss: 0.03473348953461243\n",
      "Epoch: 355, Train Loss: 0.01706449799949966, Val Loss: 0.03496183914337623\n",
      "Epoch: 356, Train Loss: 0.017032057283667183, Val Loss: 0.035133951580372906\n",
      "Epoch: 357, Train Loss: 0.0169834294514257, Val Loss: 0.0348545754075808\n",
      "Epoch: 358, Train Loss: 0.01700975234675521, Val Loss: 0.03435778895677146\n",
      "Epoch: 359, Train Loss: 0.01698094280042007, Val Loss: 0.03508260563584203\n",
      "Epoch: 360, Train Loss: 0.016981794498860836, Val Loss: 0.034823518562114845\n",
      "Epoch: 361, Train Loss: 0.016879770229965195, Val Loss: 0.034721629015343675\n",
      "Epoch: 362, Train Loss: 0.016895125513517503, Val Loss: 0.034736921553010655\n",
      "Epoch: 363, Train Loss: 0.01686162414163414, Val Loss: 0.034757719312052605\n",
      "Epoch: 364, Train Loss: 0.016900548516078143, Val Loss: 0.034618599214038606\n",
      "Epoch: 365, Train Loss: 0.01696349542303863, Val Loss: 0.035131458035212455\n",
      "Epoch: 366, Train Loss: 0.016771251933601825, Val Loss: 0.03480597181340395\n",
      "Epoch: 367, Train Loss: 0.016715372200281818, Val Loss: 0.03505957625427489\n",
      "Epoch: 368, Train Loss: 0.016644114971760722, Val Loss: 0.034945530185507515\n",
      "Epoch: 369, Train Loss: 0.016651778126735302, Val Loss: 0.03494518804284981\n",
      "Epoch: 370, Train Loss: 0.0166477891458672, Val Loss: 0.03504609269232063\n",
      "Epoch: 371, Train Loss: 0.016692441177822776, Val Loss: 0.0345619283142989\n",
      "Epoch: 372, Train Loss: 0.01655179710458901, Val Loss: 0.0352769611024503\n",
      "Epoch: 373, Train Loss: 0.016513491298024683, Val Loss: 0.03478981641326415\n",
      "Epoch: 374, Train Loss: 0.01659821221305026, Val Loss: 0.03480294236313489\n",
      "Epoch: 375, Train Loss: 0.016520384091378775, Val Loss: 0.03493430882187213\n",
      "Epoch: 376, Train Loss: 0.016535617482169706, Val Loss: 0.03470051402257661\n",
      "Epoch: 377, Train Loss: 0.016511225800613982, Val Loss: 0.03514717505076679\n",
      "Epoch: 378, Train Loss: 0.016487106023814087, Val Loss: 0.03518204735118454\n",
      "Epoch: 379, Train Loss: 0.016446647514447063, Val Loss: 0.03520379711131928\n",
      "Epoch: 380, Train Loss: 0.016331386391341813, Val Loss: 0.03528309869185343\n",
      "Epoch: 381, Train Loss: 0.016304043293299184, Val Loss: 0.0343444879182567\n",
      "Epoch: 382, Train Loss: 0.01631041534118733, Val Loss: 0.034688290824198116\n",
      "Epoch: 383, Train Loss: 0.016318114805903474, Val Loss: 0.03518233959705143\n",
      "Epoch: 384, Train Loss: 0.0162586339737557, Val Loss: 0.0352827014275274\n",
      "Epoch: 385, Train Loss: 0.016347354858071875, Val Loss: 0.03505752491370096\n",
      "Epoch: 386, Train Loss: 0.01626350756505725, Val Loss: 0.03573594545408831\n",
      "Epoch: 387, Train Loss: 0.016195999537357838, Val Loss: 0.035021528617431553\n",
      "Epoch: 388, Train Loss: 0.01615179108132182, Val Loss: 0.034722580227180055\n",
      "Epoch: 389, Train Loss: 0.016152800865850206, Val Loss: 0.03551843670844022\n",
      "Epoch: 390, Train Loss: 0.01623005232323845, Val Loss: 0.03565989265818212\n",
      "Epoch: 391, Train Loss: 0.016093382396798393, Val Loss: 0.03538545464181294\n",
      "Epoch: 392, Train Loss: 0.01604901304097562, Val Loss: 0.035087578156489434\n",
      "Epoch: 393, Train Loss: 0.015980342101163655, Val Loss: 0.03513972357041755\n",
      "Epoch: 394, Train Loss: 0.015978485993030717, Val Loss: 0.035389307365452836\n",
      "Epoch: 395, Train Loss: 0.01597685339244998, Val Loss: 0.03515598578852112\n",
      "Epoch: 396, Train Loss: 0.01595126354861689, Val Loss: 0.03516014046588187\n",
      "Epoch: 397, Train Loss: 0.015876496737522974, Val Loss: 0.035383615279728074\n",
      "Epoch: 398, Train Loss: 0.015912797191511778, Val Loss: 0.03490720370437129\n",
      "Epoch: 399, Train Loss: 0.01588501814373974, Val Loss: 0.03535450238860765\n",
      "Epoch: 400, Train Loss: 0.015792859359091874, Val Loss: 0.035771533736359266\n",
      "Epoch: 401, Train Loss: 0.015879892667171436, Val Loss: 0.035062016714048586\n",
      "Epoch: 402, Train Loss: 0.01577565889268861, Val Loss: 0.03553423099219799\n",
      "Epoch: 403, Train Loss: 0.015716300736639207, Val Loss: 0.035620428495487924\n",
      "Epoch: 404, Train Loss: 0.015775777316668023, Val Loss: 0.035288351461670156\n",
      "Epoch: 405, Train Loss: 0.015744143595792733, Val Loss: 0.03523283734349376\n",
      "Epoch: 406, Train Loss: 0.015631725259867134, Val Loss: 0.03532601435179428\n",
      "Epoch: 407, Train Loss: 0.015684195348235258, Val Loss: 0.035298997425805716\n",
      "Epoch: 408, Train Loss: 0.015738515183329582, Val Loss: 0.035272546964934315\n",
      "Epoch: 409, Train Loss: 0.015623971097708002, Val Loss: 0.035357614852866884\n",
      "Epoch: 410, Train Loss: 0.01559149703184553, Val Loss: 0.035585601640454795\n",
      "Epoch: 411, Train Loss: 0.015535838777688727, Val Loss: 0.03518818820810924\n",
      "Epoch: 412, Train Loss: 0.015543624060228467, Val Loss: 0.034990956343836706\n",
      "Epoch: 413, Train Loss: 0.01547620376824575, Val Loss: 0.035290048315615975\n",
      "Epoch: 414, Train Loss: 0.015566291765041523, Val Loss: 0.03479168290998471\n",
      "Epoch: 415, Train Loss: 0.015521915335508094, Val Loss: 0.035048451416699565\n",
      "Epoch: 416, Train Loss: 0.015433119627710242, Val Loss: 0.03535786258467173\n",
      "Epoch: 417, Train Loss: 0.015480224577852099, Val Loss: 0.035591202661773916\n",
      "Epoch: 418, Train Loss: 0.015416879664664552, Val Loss: 0.0353825288209117\n",
      "Epoch: 419, Train Loss: 0.015333076285877092, Val Loss: 0.03574798990167299\n",
      "Epoch: 420, Train Loss: 0.01530902823867371, Val Loss: 0.0352312501961902\n",
      "Epoch: 421, Train Loss: 0.015381514643303167, Val Loss: 0.03582727175079665\n",
      "Epoch: 422, Train Loss: 0.01528031399307804, Val Loss: 0.035208939811435795\n",
      "Epoch: 423, Train Loss: 0.015525889024533078, Val Loss: 0.035259571510477594\n",
      "Epoch: 424, Train Loss: 0.01529921071753853, Val Loss: 0.03540074004460189\n",
      "Epoch: 425, Train Loss: 0.01528058253560152, Val Loss: 0.03578473966947551\n",
      "Epoch: 426, Train Loss: 0.01521679931083473, Val Loss: 0.034843731908348656\n",
      "Epoch: 427, Train Loss: 0.01522753038640939, Val Loss: 0.035811245567717795\n",
      "Epoch: 428, Train Loss: 0.015268232061732875, Val Loss: 0.035122814556678476\n",
      "Epoch: 429, Train Loss: 0.0151843873042045, Val Loss: 0.03543600462124509\n",
      "Epoch: 430, Train Loss: 0.015046277454407033, Val Loss: 0.03576447082273031\n",
      "Epoch: 431, Train Loss: 0.015135182967504202, Val Loss: 0.035543726709813386\n",
      "Epoch: 432, Train Loss: 0.015082368275971484, Val Loss: 0.035999010743226034\n",
      "Epoch: 433, Train Loss: 0.014972542108715339, Val Loss: 0.03555300613959967\n",
      "Epoch: 434, Train Loss: 0.015034956628677704, Val Loss: 0.035189523866747396\n",
      "Epoch: 435, Train Loss: 0.014938341605603317, Val Loss: 0.03565952169187998\n",
      "Epoch: 436, Train Loss: 0.01496366437887615, Val Loss: 0.03556715840694763\n",
      "Epoch: 437, Train Loss: 0.014910601560910374, Val Loss: 0.03548646782982653\n",
      "Epoch: 438, Train Loss: 0.014944055080177042, Val Loss: 0.03546771760714256\n",
      "Epoch: 439, Train Loss: 0.014936804242621539, Val Loss: 0.036084703453895396\n",
      "Epoch: 440, Train Loss: 0.014869357394133458, Val Loss: 0.035696750269981765\n",
      "Epoch: 441, Train Loss: 0.014790156548369234, Val Loss: 0.03548539999746165\n",
      "Epoch: 442, Train Loss: 0.014785825042054057, Val Loss: 0.03596166279798342\n",
      "Epoch: 443, Train Loss: 0.014839393499117913, Val Loss: 0.03599828795798249\n",
      "Epoch: 444, Train Loss: 0.014777177431762724, Val Loss: 0.03557297563717022\n",
      "Epoch: 445, Train Loss: 0.014746065004886585, Val Loss: 0.03583378005393986\n",
      "Epoch: 446, Train Loss: 0.014665374998003244, Val Loss: 0.0357986886243699\n",
      "Epoch: 447, Train Loss: 0.014815815071322782, Val Loss: 0.035817271524692995\n",
      "Epoch: 448, Train Loss: 0.014679274813792968, Val Loss: 0.03600798992408534\n",
      "Epoch: 449, Train Loss: 0.014641690352079222, Val Loss: 0.035598090613039875\n",
      "Epoch: 450, Train Loss: 0.014610487409330651, Val Loss: 0.035707269353255376\n",
      "Epoch: 451, Train Loss: 0.014578722302912402, Val Loss: 0.03583043518523544\n",
      "Epoch: 452, Train Loss: 0.014608681673515525, Val Loss: 0.03539791329116639\n",
      "Epoch: 453, Train Loss: 0.014565360584390997, Val Loss: 0.035244111161110765\n",
      "Epoch: 454, Train Loss: 0.014558185614108786, Val Loss: 0.035938249522094\n",
      "Epoch: 455, Train Loss: 0.014506963415118724, Val Loss: 0.03580669151082382\n",
      "Epoch: 456, Train Loss: 0.014526558086923245, Val Loss: 0.035679673555038746\n",
      "Epoch: 457, Train Loss: 0.014633467647846852, Val Loss: 0.03487993464252706\n",
      "Epoch: 458, Train Loss: 0.014473519960844542, Val Loss: 0.035463873912596096\n",
      "Epoch: 459, Train Loss: 0.014391605821201357, Val Loss: 0.03575410752288871\n",
      "Epoch: 460, Train Loss: 0.01446301814650138, Val Loss: 0.036132257857943995\n",
      "Epoch: 461, Train Loss: 0.014440120562863678, Val Loss: 0.03557042000909983\n",
      "Epoch: 462, Train Loss: 0.01437878621331716, Val Loss: 0.03563587474873511\n",
      "Epoch: 463, Train Loss: 0.01438582084986176, Val Loss: 0.03581194585915339\n",
      "Epoch: 464, Train Loss: 0.014325529555947977, Val Loss: 0.036667758267435985\n",
      "Epoch: 465, Train Loss: 0.014345408125259614, Val Loss: 0.03584805582413229\n",
      "Epoch: 466, Train Loss: 0.014265680000959438, Val Loss: 0.03538768640640429\n",
      "Epoch: 467, Train Loss: 0.014218957056843881, Val Loss: 0.036025711698299746\n",
      "Epoch: 468, Train Loss: 0.01420176885153909, Val Loss: 0.03588564305613607\n",
      "Epoch: 469, Train Loss: 0.0142177712286712, Val Loss: 0.03537821656061431\n",
      "Epoch: 470, Train Loss: 0.014229756683284828, Val Loss: 0.03559000989011789\n",
      "Epoch: 471, Train Loss: 0.014157513286760551, Val Loss: 0.03568855513644926\n",
      "Epoch: 472, Train Loss: 0.014166854183836761, Val Loss: 0.03585981397684348\n",
      "Epoch: 473, Train Loss: 0.014162694547972563, Val Loss: 0.035841084953586934\n",
      "Epoch: 474, Train Loss: 0.014075831518019154, Val Loss: 0.035955634520594346\n",
      "Epoch: 475, Train Loss: 0.01404821929734958, Val Loss: 0.03613362110898656\n",
      "Epoch: 476, Train Loss: 0.014052901138804095, Val Loss: 0.03565808974395869\n",
      "Epoch: 477, Train Loss: 0.014073411095492795, Val Loss: 0.035879489776315325\n",
      "Epoch: 478, Train Loss: 0.014058571170731369, Val Loss: 0.03570261000955509\n",
      "Epoch: 479, Train Loss: 0.014024930003777905, Val Loss: 0.03593663999147839\n",
      "Epoch: 480, Train Loss: 0.013909279538507936, Val Loss: 0.03585889790270288\n",
      "Epoch: 481, Train Loss: 0.013899100530040214, Val Loss: 0.03607432784165366\n",
      "Epoch: 482, Train Loss: 0.01391886380872042, Val Loss: 0.03614181665293241\n",
      "Epoch: 483, Train Loss: 0.01389346296022946, Val Loss: 0.03639254307027085\n",
      "Epoch: 484, Train Loss: 0.013835173595491482, Val Loss: 0.03552623031566204\n",
      "Epoch: 485, Train Loss: 0.013879809761419892, Val Loss: 0.03631384182973939\n",
      "Epoch: 486, Train Loss: 0.013818603133293405, Val Loss: 0.03574533143349118\n",
      "Epoch: 487, Train Loss: 0.013809908362957885, Val Loss: 0.03572849899340989\n",
      "Epoch: 488, Train Loss: 0.013806726773648317, Val Loss: 0.03589220015111111\n",
      "Epoch: 489, Train Loss: 0.013763055262566245, Val Loss: 0.03607371842520217\n",
      "Epoch: 490, Train Loss: 0.013718737360342579, Val Loss: 0.036063052603375105\n",
      "Epoch: 491, Train Loss: 0.013706414508475465, Val Loss: 0.03595664358492625\n",
      "Epoch: 492, Train Loss: 0.01370853241296264, Val Loss: 0.03621053354719938\n",
      "Epoch: 493, Train Loss: 0.013774046471137238, Val Loss: 0.03598849877904532\n",
      "Epoch: 494, Train Loss: 0.013674335846267009, Val Loss: 0.03620062645335319\n",
      "Epoch: 495, Train Loss: 0.013721562434635046, Val Loss: 0.03614050096248166\n",
      "Epoch: 496, Train Loss: 0.013617362850740299, Val Loss: 0.035670450557086425\n",
      "Epoch: 497, Train Loss: 0.0136613236677091, Val Loss: 0.036308998715574456\n",
      "Epoch: 498, Train Loss: 0.013628892677488848, Val Loss: 0.03666732608640598\n",
      "Epoch: 499, Train Loss: 0.013552417084811476, Val Loss: 0.03635525859702947\n",
      "Epoch: 500, Train Loss: 0.013507051154639635, Val Loss: 0.036051014785544344\n"
     ]
    }
   ],
   "source": [
    "train_loss,val_loss = train(mae,train_loader,val_loader,epochs=500,lr=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
